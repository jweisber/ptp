# The Human Angle

Three hundred years after Pascal and & co., another French mathematician devised a famous challenge for the idea of expected utility.

In the years following World War II, the idea that human decisions could be boiled down to a single equation was taking hold in the United States. I mentioned the classic 1944 book by John von Neumann and Oskar Morgenstern back in Chapter 9.

Well, ten years later another classic was born in *The Foundations of Statistics*, written by von Neumann's former assistant, Leonard Savage. Both books derived the expected utility rule from first principles, establishing it as the gold standard in decision making.

But some French mathematicians disliked this approach. As Ian Hacking writes, they "thought that there was something mechanical, unthinking, and terribly 'American' about having a blind rule to compute your free choices." And in 1953, Maurice Allais developed a famous puzzle to demonstrate.


## The Allais Paradox

Suppose I offer you a choice: you can have a million dollars with no strings attached, or you can gamble. The gamble has an 89% chance of delivering a million dollars, a 10% chance of delivering five million, and a 1% chance of delivering nothing.

Which will you choose, the guaranteed million or the gamble with a shot at five?

Most people favour the sure million. A million dollars would reshape their whole life. And even though five million dollars would make an even bigger difference, the difference between one million and five million isn't enough for them to risk walking away from this opportunity empty handed. Even though the risk would only be a 1% chance, they'd rather just take the safe million.

Now imagine a different choice. This time both options are gambles, with very similar odds. The main difference is in the potential payoffs. The first gamble has an 89% chance of paying nothing and an 11% chance of winning a million. The second gamble has a 90% chance of paying nothing and a 10% chance of paying five million.

Which will you choose, the slightly safer shot at a million or the slightly riskier shot at five million?

Most people favour the second option, the slightly riskier shot at five million. There is no sure thing now, in fact whichever way you go you'll probably walk away empty handed. And if you're just willing to accept a small, 1% increase in the chance of leaving empty handed, you can have a shot at five million instead of one million. For most people, that exchange seems worth it.

So what? Well, in fact most people have both these preferences: they'd rather have the safe million in the first choice, but take the risk at five million in the second. But this violates the law of expected utility.


## Allais, EU, and You

How can I say people are violating the expected utility rule here if I don't know their personal utilities? Isn't it possible that, for these people, the difference in value between getting nothing and getting a million dollars makes sense of these preferences?

Nope. And that's part of the beauty and ingenuity of Allais' paradox. No matter what a person's utilities are for $0, $1M, and $5M, the popular choices violate the expected utility rule. In fact it only takes a few lines of arithmetic to see why.

First let's labels the options here:

+-------+------------+------------+-------------+
| 1A    | 1B         | 2A         | 2B          |
+=======+============+============+=============+
| - $1M | - 89%: $1M | - 89%: $0  | - 90%: $0   |
|       | - 10%: $5M | - 11%: $1M | - 10%: $5M  |
|       | - 1%: $0   |            |             |
+-------+------------+------------+-------------+

Most people choose 1A over 1B and 2B over 2A, so we're considering the possibility that $EU(1A) > EU(1B)$ and $EU(2A) > EU(2B)$.

Let's write out the expected utility formula for each option:
$$
  \begin{aligned}
     EU(1A) &= U(\$1M),\\
     EU(1B) &= .89 \cdot U(\$1M) + .1 \cdot U(\$5M) + .01 \cdot U(\$0M),\\
     EU(2A) &= .89 \cdot U(\$0M) + .11 \cdot U(\$1M),\\
     EU(2B) &= .9 \cdot U(\$0M) + .1 \cdot U(\$5M).
  \end{aligned}
$$
A little arithmetic will show that $EU(1A) - EU(1B) = EU(2A) - EU(2B)$:
$$
  \begin{aligned}
    EU(1A) - EU(1B) &= -.01 \cdot U(\$0M) + .11 \cdot U(\$1M) - .1 \cdot U(\$5M),\\
    EU(2A) - EU(2B) &= -.01 \cdot U(\$0M) + .11 \cdot U(\$1M) - .1 \cdot U(\$5M).\\
  \end{aligned}
$$
So if 1A has higher expected utility than 1B, the same has to be true for 2A over 2B!

And this holds no matter what we plug in for the utility values. Because we left those untouched, as placeholders. So however you value money, there's just no way to have $EU(1A) > EU(1B)$ but $EU(2A) < EU(2B)$.


## Savage's Response

In a way, this actually makes sense. After all, both choices involve the same tradeoff.

Imagine you're holding a million dollars, and contemplating trading it for gamble 1B. What you're contemplating is taking on a 1% risk of nothing in exchange for a 10% shot at five million dollars. And that's the same tradeoff as in the choice between 2A and 2B: are you willing to take on an extra 1% risk of nothing in exchange for a 10% chance at five million?

The only difference between the two choices is the context in which this tradeoff is contemplated. In the first context there is a safe option, you can keep your million and walk away. But in the second context, there is no safe option. You must face an 89% chance of leaving empty handed. The only question is whether you're willing to accept an added 1% risk in exchange for the shot at five million.

Should this difference in context make a difference to whether you accept the tradeoff?

Leonard Savage, the champion of expected utility, famously felt the allure of context here. As he wrote in his landmark 1954 book:

> When the two situations were first presented, I immediately expressed preference for Gamble [1A] as opposed to Gamble [1B] and for Gamble [2B] as opposed to Gamble [2A], and I still feel an intuitive attraction to these preferences. But I have accepted the following way of looking at things...

Savage's way of looking at things was to imagine the gambles being determined by a lottery with a hundred tickets labelled #1 to #100:

+------+-------+-------+-----------+
|      | #1    | #2–11 |  #12–100  |
+:====:+:=====:+:=====:+:=========:+
|  1A  | $1M   | $1M   | $1M       |
+------+-------+-------+-----------+
|  1B  | $0    | $5M   | $1M       |
+------+-------+-------+-----------+
|  2A  | $1M   | $1M   | $0        |
+------+-------+-------+-----------+
|  2B  | $0    | $5M   | $0        |
+------+-------+-------+-----------+

Now, if you're choosing between 1A and 1B, you wouldn't care if you knew the ticket would be one of #12 to #100. Either way you get $1M.

So all that matters is which option you would prefer if you know the ticket would be one of #1 through #11. And if you would choose 1A then, you should make the same choice between 2A and 2B. Because it's the exact same choice when the ticket drawn will be one of #1 through #11. And it doesn't matter which you choose if it's one of the others.

Savage encoded this idea in a famous principle he dubbed "the Sure-thing Principle":

Sure-Thing Principle
:    If you would choose A over B if you knew $X$ was true, and also if you knew $X$ was false, then you should choose A over B when you don't know whether $X$ is true or not.

Allais' paradox shows that people don't always follow this principle. Although it may sound obvious, it has implications that aren't so obvious. It enforces a kind of "context free" decision making that humans don't always find intuitive or comfortable. For many of us, it matters whether there's a safe option---whether we could have the million dollars regardless, whether the  ticket drawn will be one of #1–#11 or not.


## Dr. Savage's Prescription

Savage's answer to Allais illustrates a core conceptual distinction, one that arises again and again in philosophy.

In Savage's view, the expected utility rule is *prescriptive* rule, not *descriptive*. It says what people *should*, not what they will do. Just as people don't always take their medicine as prescribed, people don't always choose the option with the highest expected utility.

We've already seen many ways in which humans are error-prone when it comes to reasoning with probabilities: base-rate neglect, the gambler's fallacy, etc. Savage viewed Allais' paradox similarly. It's just another way humans fail to follow core principles of good reasoning, like the Sure-thing Principle.

In fact Savage made the Sure-thing Principle a foundational axiom of his theory. It's one of the eponymous *Foundations of Statistics* his book examines, and one central achievement of the book was to derive the law of expected utility from a small list of such assumptions.

More than sixty years later, the debate over the Sure-thing Principle still hasn't been entirely settled.


## The Ellsberg Paradox

When Chelsea Manning and Edward Snowden leaked classified material to the public, they found an advocate in Daniel Ellsberg.

Ellsberg had leaked the famous [Pentagon Papers](https://en.wikipedia.org/wiki/Pentagon_Papers) to newspapers in 1971, revealing the U.S. government lies to the American public about the Vietnam war. Ellsberg was charged with espionage and conspiracy. Luckily for him, the government bungled the case.

At Ellsberg's trial all sorts of shenanigans emerged. White House officials had even broken into his psychiatrist's office to find embarrassing information. The judge dismissed the charges. And the same White House burglars later became notorious for the most famous break-in in American politics, the Watergate scandal.

But before lighting the fuse that ultimately destroyed Richard Nixon's presidency, Ellsberg was known for his work in decision theory. In 1963 he challenged Savage's Sure-thing Principle with puzzles like this one.

> Imagine an urn with 90 balls. 30 are red, and 60 are either black or white, but in unknown proportion. There might be anywhere from 0 to 60 black balls.
>
> A ball will be drawn at random, and you must choose between the following:
>
> - 1A: win $100 if the ball is red,
> - 1B: win $100 if the ball is black.
>
> You also face a second choice:
>
> - 2A: win $100 if the ball is either red or white,
> - 2B: win $100 if the ball is either black or white.

Most people choose 1A over 1B, since you know what you're getting with 1A: a 1/3 chance at the $100. Whereas 1B might give worse odds, maybe even no chance at all if there are no black balls.

At the same time, most people choose 2B over 2A, and for a similar reason. With 2B, you know you're getting a 2/3 chance at the $100. While 2A might give much worse odds, maybe even as low as 1/3 if there are no white balls in the urn.

Like in the Allais paradox, this popular combination of choices violates the expected utility rule. The calculation that shows this is pretty similar to the one we did with Allais though, so let's not rehearse it here.

Instead let's think about what Ellsberg is showing us here.


## Ellsberg & Allais

Ellsberg's paradox is strongly reminiscent of Allais'. More than the two-choice structure they share, both also exploit a human preference for the known. In the Allais paradox we prefer the sure million, and in the Ellsberg paradox we prefer to know our chances.

The kind of risk at play in each paradox has a different character, notice. In Allais' paradox all the probabilities are known, and in one case we can even know the outcome. If you choose the safe million, you know what your fate will be.

But in the Ellsberg paradox, you never know the outcome. The most you can know is the chance of each outcome. And yet, our preference for the known still takes hold. We still prefer to go with what we know, even if all we can know is the chance of each outcome.

Is this preference for known risks rational, or irrational? Well, it violates Savage's Sure-thing Principle. Consider Ellsberg's dilemma as a decision table:

+------+-------+-------+---------+
|      | Red   | Black |  White  |
+:====:+:=====:+:=====:+:=======:+
|  1A  | $100  | $0    | $0      |
+------+-------+-------+---------+
|  1B  | $0    | $100  | $0      |
+------+-------+-------+---------+
|  2A  | $100  | $0    | $100    |
+------+-------+-------+---------+
|  2B  | $0    | $100  | $100    |
+------+-------+-------+---------+

If you knew a white ball was going to be drawn, you wouldn't care which option you chose. And if you knew a white ball wouldn't be drawn, then options 1A and 2A would be equivalent. So consistency would seem to demand selecting 2A if you selected 1A.

Many decision theorists find this reasoning compelling. But more than a few  turn it on its head and say so much the worse for the Sure-thing Principle.


## Conclusion

Psychologists have tested the Allais and Ellsberg paradoxes on real human subjects many times, with pretty consistent results. These results have provided much insight into the psychology of human choice. They've even shaped [a Nobel prize winning theory](https://en.wikipedia.org/wiki/Prospect_theory) about how human decisions differ from the expected utility rule.

But in philosophy, the study of decision theory tends to focus on how we *should* reason, not how we actually do reason. And as a prescriptive theory, the expected utility rule still looms large. It has its rivals, and is by no means the only game in town. But 350 years after Pascal and Fermat first dreamt it up, it remains the most popular game in town, by a wide margin.
