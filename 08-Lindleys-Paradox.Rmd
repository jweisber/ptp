# Lindley's Paradox


So how do scientists test their theories? How do they decide when an experiment supports a hypothesis, and when it refutes it?

The short answer: they ask whether the results would be too much of a
coincidence if the hypothesis were true.


## Coincidence

Suppose the government starts minting a new, three dollar coin. We take one and flip it ten times. It lands heads every time.

That would be a *big* coincidence if the coin were fair. *Too* much of a coincidence, in fact. The chances of getting ten out of ten heads with a fair coin are less than one in a thousand.

So the hypothesis that the coin is fair has been tested, and it has failed that test. From our little experiment, we conclude that the coin is not fair, but  biased.

Another example: in a test of a new cancer drug, 90% of patients who take the drug are cured, compared to only 10% in the placebo group. That, again, would be a big coincidence if the drug were ineffective. Too much of a coincidence.  

So the hypothesis that the drug is ineffective has been tested, and it has failed the test. We conclude instead that the drug has beneficial effects.


## The General Formula

Here's the general formula for this method:

1. State the hypothesis you want to test---that the coin is fair, that the drug is ineffective, etc.
    - This is called the *null hypothesis*. (Because, traditionally, the default assumption is that the data are random, with no pattern.)
2. Gather your data---flip the coin ten times, administer the drug to a group of patients, etc.
3. Imagine the hypothesis is true, and consider how likely the data you've observed would be if it were true.
4. If the answer is "very unlikely", then reject the hypothesis.

How unlikely is "very unlikely", exactly? How much of a coincidence is too much of a coincidence? Traditionally the cutoff is 5% or less. (We'll come back to the reason behind this tradition soon.)

When the results fall in the range below the cutoff, they're called *statistically significant*. And the method we've just described is called *significance testing*.


## What About Bayes?

You might be wondering what happened to Bayes' theorem and Bertrand's paradox here. Did we somehow just magically solve the problem of priors?

No. We didn't calculate $Pr(H \given E)$, instead we've been focusing on $Pr(E \given H)$. (Well, sort of. There's a wrinkle here we'll come back to later.)

When $H$ is a definite statistical hypothesis, you can calculate the probability of each outcome *under the assumption that $H$ is true*. If our hypothesis is that a coin is fair, you can calculate the probability of getting ten out of ten tails in a row.

Similarly, if our hypothesis is that each patient has only a 10% chance of being cured, we can calculate the probability that 90% of the patients in our test group will be cured. So we can calculate $Pr(E \given H)$ when $H$ is an explicit and exact statistical hypothesis.

The idea behind significance testing is to decide whether to reject $H$ just on the basis of $Pr(E \given H)$. (Well, again, sort of: wrinkle.) If it's too low for $H$ to be believable, then we reject $H$.


## Normal Approximations

But how exactly do we calculate these probabilities? How do we determine when the results fall below the 5% cutoff?

Computers do a lot of this work nowadays, especially in complicated cases. But there's a shortcut for doing it by hand, at least in simple cases like coin tosses. And it gives you a much deeper understanding of significance testing, so let's see how it goes.

Suppose I flip a coin 100 times and it lands heads 75 of those times. Is that suspicious? Should we wonder whether it's really fair?

We need to figure out how unlikely it is to get 75 or more heads when you flip a fair coin 100 times. This would be tedious to do with perfect precision, but there’s a trick for getting a good approximation.

If you flip a fair coin 100 times, the most likely outcome is 50 heads and 50 tails. A bit less likely is 51 heads, or 49 heads. A bit less likely still is 52 heads, or 48 heads. And so on.

The overall pattern looks like this:

![](img/100-Flips-Binomial-Distribution.png)

Notice how much it resembles the famous “bell curve” shape:

![](img/100-Flips-Normal-Approximation.png)\

In fact they line up almost perfectly:

![](img/100-Flips-Binomial-and-Normal.png)

So we can use a bell curve to approximate the probabilities we're after. But how does that help? Because bell curves have some handy mathematical properties.


## Characterizing a Bell Curve

Fun fact: a bell curve is completely characterized by just two numbers.

First is the center of the bell, called the *mean*, or $\mu$ for short. In our example $\mu = 50$. That’s the most likely outcome for $100$ flips of a fair coin. The general formula is:
    $$\mu = np.$$
Here $n$ is the number of tosses, $100$ in our example. And $p$ is the probability of heads on each toss, $1/2$ in our example. So $\mu = np = 100 \times 1/2 = 50$.

Second is the width of the bell, called the *standard deviation*, or $\sigma$ for short. The general formula for $\sigma$ is a bit mysterious:
    $$\sigma = \sqrt{np(1-p)}.$$
There's a good-but-complicated mathematical reason behind this formula, so let's just take it for granted. In our example $\sigma = \sqrt{np(1-p)} = \sqrt{100 \times 1/2 \times 1/2} = 5.$

Now that we know where the bell is centered ($\mu = 50$), and how wide
it is ($\sigma = 5$), we can estimate how probable various outcomes are. How?

Well, i's a handy mathematical fact that, 95% of the time, the result of our coin flipping experiment will be within two "standard deviations" of the mean. In other words, we should expect the number of heads to be in the $\mu \pm 2 \sigma$ range 95% of the time, if the coin really is fair.

In our example, $\mu \pm 2 \sigma$ is $50 \pm 10$, so the "95% of the the time" range is from 40 to 60 heads. But we got 75 heads. So that's a pretty big coincidence if the coin really is fair, the kind of thing that would happen less than 5% of the time.

So our result is statistically significant! The hypothesis that the coin is fair should be rejected, according to the method of significance testing.


## The 68-95-99 Rule

In general, the math of bell curves works like this. If we go 5 heads out from 50, the probability is less than 68%. That's *one* "standard deviation", or $1 \times \sigma$. If we go 10 heads out, which is *two* standard deviations ($2 \times \sigma$), the probability is less than 95%. And if we go 15 heads out, which is three standard deviations or $3 \times \sigma$, the probability is less than 99%.

So, in general, we can get a good sense of how surprising a result is by the following rule.

If it falls within $\sigma$ of the mean $\mu$, it’s not surprising; that’s what you’d expect most of the time, about 68\%. If it falls outside that range, it’s a little surprising; you’d expect that to happen sometimes, about a third of the time (100% – 68% = 32%).

But if it falls outside *that* range, then it’s quite surprising. You’d expect that to happen rarely, only about 5% of the time (100% – 95% = 5%). And if it falls outside even that range, then it’s *very* surprising. You’d expect that to happen very rarely, less than 1% of the time.


## Another Example

It takes practice to get the hang of all this, so let's do another example. This one is from a book by Ian Hacking:

> VisioPerfect has a run of 2,400 light bulbs. According to the
> company’s publicity, 96% of the bulbs manufactured by its production
> process are long life...
>
> The monthly magazine *Consumers’ Advocate* asserts that it tested a
> run of 2,400 bulbs---400 six packs---from VisioPerfect. It found 133
> short-life bulbs. (Hacking, p. 205)

Is this a statistically significant result? Should we reject the claims of VisioPerfect's publicists?

To find out, we start by calculating $\mu$ and $\sigma$. And for that we need $n$ and $p$.

In this example $n = 2,400$: that's the size of our sample of bulbs. And $p = .04$: that's the chance of getting a short-life bulb according to the hypothesis we're testing.

So using the formula from earlier, $\mu = np = 2,400 \times .04 = 96$. And $\sigma = \sqrt{np(1-p)} = \sqrt{2,400 \times .04 \times .96} = 9.6$.

Now, we observed 133 short-life bulbs. And according to the 68-95-99 rule, that's a statistically significant result if 133 is more than two standard deviations away from the mean. In other words, if it's outside the $\mu \pm 2 \sigma$ range. And in this case $\mu + 2\sigma = 115.2 < 133$, so the result is significant.

According to the method of significance testing then, we should reject VisioPerfect's advertised claim.


## Warning: Danger Ahead

Significance testing is confusing stuff. Trained scientists, and even
statisticians, often misunderstand it. And they misapply it in real studies as
a result.

In fact, the American Statistical Association [recently released a statement](https://www.amstat.org/newsroom/pressreleases/P-ValueStatement.pdf) to clarify the idea and prevent its misuse.

So what does it really mean for a result to be "statistically significant"? It means that *if* the null hypothesis is true, then a result this unusual was less than 5% likely to occur.

But that doesn't mean the chance the null hypothesis is true is now less than 5%! Making that leap would confuse $Pr(E \given H)$ with $Pr(H \given E)$.

It also doesn't mean the true hypothesis is significantly different from the null hypothesis. Imagine a coin that's just *slightly* biased towards heads---maybe it comes up heads 51% of the time. With enough flips, you could establish that the coin isn't fair, with statistical significance. But that doesn't mean it's significantly biased. It's barely biased at all: 51% rather than 50%.

The "significance" terminology isn't great---it may even be worse than "null hypothesis". But let's try not to get hung up on preferred nomenclature. It's the underlying logic of significance testing that concerns us here.


## Significance & Subjectivity

Let's tackle a big question we've been avoiding: why is the cutoff 5%? Nothing magical or special happens when a result crosses that line. It's just easy to calculate using the bell curve trick we just learned.

In fact, although 5% is the cutoff commonly used in social sciences like psychology and sociology, different areas of research have different customs. A cutoff of 1%, or even 0.1%, is common in some physical sciences.

Come to think of it, does the idea of a fixed cut off make sense at all? Sometimes a result is really far out there---the kind of thing you'd happen to expect less than one time in a thousand---and yet the null hypothesis shouldn’t be rejected. If you’re playing poker at a major casino and you’re dealt a straight, that doesn't mean the deck is stacked in your favour. You know casinos are tightly regulated; they use carefully calibrated automatic shufflers; and they want you to lose, not win!

Some statisticians, called "Bayesians", see the whole significance testing method as misguided. When we calculate significance levels, we’re basically calculating $Pr(E \given H)$: the probability of the result if the hypothesis is true. But Bayes’ theorem tells us this is just part of the information we need to calculate what we really want, namely $Pr(H \given E)$. We want to know how plausible the hypothesis is given our results. But for that, we need $Pr(H)$ and $Pr(E)$:

$$ Pr(H \given E) = Pr(H) \frac{Pr(E \given H)}{Pr(E)}. $$

Take the casino example. The null hypothesis $H$ is that the cards are being dealt at random. In this kind of case, $Pr(H)$ is very high: major casinos use standard decks and automated shuffling machines, and they're very tightly regulated. That's why, even though $Pr(E \given H)$ is low---it's very unlikely to get a straight when cards are being dealt at random---$Pr(H \given E)$ is still high. If you're basically certain the dealing is fair, you'll still be confident it's fair even if you're dealt a straight on your first hand.

So Bayesian statisticians see significance testing as a crude way of approximating Bayes’ theorem. Significance testing only *looks* objective on the surface, they say, because we're pretending $Pr(E \given H)$ is the only term in Bayes' theorem that matters. But other terms like $Pr(H)$ do matter, as the casino example shows. So the choice of cutoff is subjective, based on a personal judgment about $Pr(H)$.

This criticism is sharpened by a famous problem known as *Lindley’s paradox*.


## Lindley’s Paradox

Imagine you run a flower store, and you receive a large shipment of tulip bulbs. The supplier only sends two kinds of shipments:

- Type A: 25% of the bulbs grow red tulips, 75% yellow.
- Type B: 50% of the bulbs grow red tulips, 50% yellow.

But the label on this shipment has been scratched off. So, to determine which type it is, you select 48 random bulbs and plant them to see what colour they grow. 36 grow red, 12 yellow.

If it were me, I'd think this clearly shows the shipment is Type B, with 50% red bulbs. Even though your sample grew more than 50% red tulips, this would be even less likely if the shipment were Type A, with only 25% red bulbs. So the results of your experiment fit the Type B hypothesis much better than the Type A hypothesis.

What happens if we apply significance testing, though? We'll end up rejecting *both* hypotheses! Because the results don't fit either hypothesis particularly well, even though they fit the Type B hypothesis better than Type A.

Let's test the Type B hypothesis to see how this works out. You sampled 48 bulbs, so $n = 48$. And according to the Type B hypothesis, each bulb has a 50% chance of growing red, so $p = .5$. Applying our formulas for $\mu$ and $\sigma$ from earlier, we get:
  $$
    \begin{aligned}
      \mu &= 25,\\
      \sigma &\approx 3.5.
    \end{aligned}
  $$
So, according to the Type B hypothesis, 95% of the time we'd expect the number of red tulips in a sample of 48 to be between 18 and 32 (that's $25 \pm 2 \times 3.5$). Yet our actual result was 36, which is outside that range. (In fact it's even outside the 99% range.)

If you do the calculations for the Type A hypothesis, you'll find a parallel result. The observed 36 red tulips is statistically significant, so we should reject that hypothesis too.

But that means we would reject both hypotheses. Yet we know they're the only two possibilities! Something seems to have gone wrong.


## A Bayesian Analysis

Bayesian critics say the problem is that we ignored the prior probabilities. If we apply Bayes’ theorem, which takes account of all the information---including the fact that the only two possibilities are Type A and B---we get the sensible result. We'll conclude that the Type B hypothesis is true.

For example, if we treat the Type A and Type B possibilities as equally plausible, we can assign $Pr(H) = 1/2$. Bayes' theorem then tells us that $Pr(H \given E) = .999999998$ for the Type B hypothesis, compared to $.000000002$ for Type A. (The details of this calculation are pretty hairy, so I had my computer do it.)

Now, we know from previous chapters on Bertrand's paradox and the grue paradox that there are problems lurking here. By assuming $Pr(H) = 1/2$ in this calculation, we're essentially relying on the Principle of Indifference. And that way lies paradox.

But, the Bayesians say, Bayes' theorem will give similar results even if we set $Pr(H) = 1/3$ for the Type B hypothesis, or $Pr(H) = 1/4$. Bayes' theorem will always say the probability of this hypothesis goes up given the results of the experiment, because the results fit the Type B hypothesis better.

So, the say, we don't have to worry too much about what the correct value of $Pr(H)$ is.


## The Standoff

Trouble is, we need to know *how much* the Type B hypothesis goes up. Does it go up to a number that's basically 1, like .999999998? Or does it go up from a number that's basically 0 to a slightly larger number that's still close to 0, like .000000001?

What we should believe about $H$ depends on the value of $Pr(H \given E)$. Is it close to 1, close to 0, or somewhere in between? And that depends on what number we start with for $Pr(H)$. So there just doesn't seem to be any getting around the problem of priors.

On the other hand, significance testing seems to just be a way of pretending the problem doesn't exist. It may work well if it's used carefully and judiciously. But, critics say, it often isn't used that way. And even when it is used that way, that's just Bayes' theorem in disguise---making a rough guess about what $Pr(H)$ should be, yet pretending you've done no such thing.
