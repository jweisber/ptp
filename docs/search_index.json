[
["index.html", "Probability Through Paradox Preface", " Probability Through Paradox Jonathan Weisberg Preface This book introduces probability by way of its many paradoxes. When I first had the idea to write a book like this, I sat down to list as many puzzles and paradoxes of probability as I could think of off the top of my head. There were over twenty-five, right off the bat. After a bit more thought and discussion with friends on facebook, the number grew to well over thirty. I had always known there were plenty of probabilistic puzzles around. But I was still surprised to see them all listed in one place. And then I was struck by how much of what I know about probability is bound up with the items on that least. So I decided the book was a good idea. You can learn a lot about probability just by getting to know its paradoxes. It’s also a good way to learn about probability because puzzles and paradoxes are a common frame of reference for experts who talk and write about probability. They even serve as a shibboleth sometimes; fluency with these paradoxes is the secret-handshake by which initiates identify their peers. These considerations are especially true of philosophical approaches to probability, but they also apply to its mathematical and applied facets. So, although our approach will be primarily philosophical, I hope that you will find plenty of interest in this book whatever your orientation. "],
["the-monty-hall-problem.html", "1 The Monty Hall Problem", " 1 The Monty Hall Problem Let’s start with a puzzle so famous it’s appeared in Hollywood movies: the Monty Hall problem. "],
["ask-marilyn.html", "1.1 Ask Marilyn", " 1.1 Ask Marilyn Marilyn vos Savant had the highest IQ ever recorded, until the Guinness Book of Records dropped that category in 1990 because of concerns about IQ tests. In her “Ask Marilyn” column for Parade magazine, vos Savant answers hard questions from readers: puzzles, brain teasers, philosophical quandaries… you name it. When she published her answer to our first puzzle, a bunch of people with fancy credentials wrote in to complain she was wrong. Here was the question: Suppose you’re on a game show, and you’re given the choice of three doors. Behind one door is a car, behind the others, goats. You pick a door, say #1, and the host, who knows what’s behind the doors, opens another door, say #3, which has a goat. He says to you, “Do you want to pick door #2?” Is it to your advantage to switch your choice of doors? —Craig F. Whitaker, Columbia, Maryland If you’re like me, your gut says it doesn’t matter. There are two doors left; one has the car behind it; so they both have the same one-in-two chance of being a winner. Flip a coin, stick or move. It doesn’t matter. But Marilyn said you should switch! And you know what? She’s right. Why? Because the host knows where the car is, and he’s not going to just show it to you. That means he effectively just told you it’s behind behind door #2, unless you got lucky with your first pick of door #1. Because unless you got lucky on your first pick, his choice of which door to open was forced. He had to open door #3 to avoid showing you the car behind door #2. Again, that’s assuming you were unlucky on your first pick of door #1. But that’s a healthy two-in-three chance. So there’s a two-in-three chance door #2 has the car. Still don’t believe me? It’s okay, that’s normal. Here’s how Marilyn convinced her readers to switch. Imagine the same game, but with 100 doors. You pick door #1, and the host opens every other door except, let’s say, door #42. What do you think now, is it just a fifty-fifty shot? No way! Unless you got very lucky on your first pick and the car is behind door #1, you’ve just learned that the lucky door is #42. Still don’t believe me? That’s okay, that’s normal too! One of the most famous mathematicians of the 20th century wasn’t convinced either. Paul Erdős published over 1,500 papers in mathematics, more than any other mathematician ever. A lot of his work was even advanced research in probability theory! But when his friend, Andrew Vázsonyi, told him about this puzzle, he wasn’t having it. He kept asking Vázsonyi for a better explanation why he should switch. Vázsonyi even showed him a computer simulation he’d made of the game, where switching doors won the car two-thirds of the time. But Erdős still wanted a pure, mathematical explanation. Fun facts about Erdős by the way: he basically lived for math. He would travel the world, visiting friends at their houses, often uninvited. He would drink coffee (and, eventually, take amphetamines) and do math with his host, until they solved whatever problem the host had been working on before Erdős showed up. Then he’d move on to the next house, and the next problem. He was such a character, and so ubiquitous, that mathematicians make a game of tracing their connection to him. Every mathematician has an “Erdős number”. It’s kind of like six degrees of Kevin Bacon. Erdős is number zero. People who coauthored with Erdős have number 1. People who coauthored with them have number 2. Their coauthors have number 3, and so on. "],
["three-prisoners.html", "1.2 Three Prisoners", " 1.2 Three Prisoners In my last year as an undergraduate I took a class that changed my life. It literally defined my career as a philosopher. To this day, my career is devoted to the concepts, tools, and problems I met there for the first time. One of those problems, which my professor assigned as homework, goes like this: Three prisoners, A, B, and C, are condemned to die in the morning. But the king decides in the night to pardon one of them. He makes his choice at random and communicates it to the guard, who is sworn to secrecy. She can only tell the prisoners that one of them will be released at dawn. Prisoner A welcomes the news, as he now has a 1/3 chance of survival. Hoping to go even further, he says to the guard, “I know you can’t tell me whether I am condemned or pardoned. But at least one other prisoner must still be condemned, so can you just name one who is?”. The guard replies (truthfully) that B is still condemned. “Ok”, says A, “then it’s either me or C who was pardoned. So my chance of survival has gone up to 1/2”. Unfortunately for A, he is mistaken. But how? This homework problem got me so turned inside out, I ended up writing a computer program to run simulations of it! Just like Vázsonyi. (Fools and wise men think alike, I guess.) Well, my professor laughed. He called my computer simulation an “empirical” approach, and he didn’t mean it as a compliment. Like Erdős, he wanted me to use reason, not a brute-force, computerized experiment. Well, here’s one reasoned argument that prisoner A is mistaken. By A’s logic, their chances would also go down to 1/2 if the guard identified C instead of B. Because then there would be only one more prisoner to be released, with two candidates remaining: A and B. But if A’s chances would be the same no matter what the guard said, then the guard’s response couldn’t possibly be informative. If your chances of having a certain disease are the same whether a certain medical test comes up positive or not, then the test must be useless, right? This argument is right as far as it goes. But it doesn’t really explain where the prisoner went wrong. It just tells us she must’ve gone wrong somewhere. But where? I think the correct explanation involves a famous precept known as The Total Evidence Requirement. It says pretty much what it sounds like: to get the true probability of something, you have to take account of all the evidence you have. And the guard isn’t just telling A that B is still condemned. She’s also telling B, implicitly, that she chose to identify B as one of the condemned prisoners. It’s really counterintuitive that this extra information makes a difference. But it turns out to make all the difference in the world. Here’s one way to see how. A knows B can’t be the pardoned prisoner, because the guard said so. So there are two possibilities remaining: A is the pardoned prisoner. C is the pardoned prisoner. In the first scenario, where A has been pardoned, the guard had a choice between identifying B or C in her response. She chose to identify B, but she could have identified C instead. But in the second scenario, the guard had no choice. She had to name B, since she couldn’t tell A that she’s still condemned. So when the guard says “B is still condemned”, her report is exactly what you’d expect in the second scenario, where C was pardoned. Whereas in the first scenario, where A was pardoned, the guard’s report was half as likely. She could just as easily have named C instead. So the guard’s report fits twice as well with the second scenario. And that’s why A’s chance of survival is still 1/3. This is essentially the same solution we used for the Monty Hall problem. And we can use a diagram to visualize it. The king chooses one of the three prisoners at random, so each has the same 1/3 chance: Then we consider the guard’s options. If A is pardoned, she can choose to name either B or C. But if B is pardoned, she has to name C—otherwise she’ll give away A’s fate. And likewise, she has to name C if B is pardoned. So there are two possible outcomes where the guard names B: And look! One of them is half as likely as the other, namely the one where A was pardoned. So when the guard names B, A’s chance of having been pardoned is half that of still being condemned. In other words, her chance of survival is 1/3, versus a 2/3 chance of still being condemned. So A’s chances really do stay fixed. The guard’s report is no reprieve. It makes no difference to A’s chances, as seems right. "],
["monty-hall-again.html", "1.3 Monty Hall Again", " 1.3 Monty Hall Again Erdős left Vázsonyi’s house unsatisfied and unconvinced. But a few days later he called: someone had finally explained to his satisfaction why you should switch doors in the Monty Hall problem. “He proceeded to tell me the reasoning”, said Vázsonyi, “but I couldn’t fathom his explanation.” I guess we’ll never know what persuaded Erdős. But luckily we don’t need to, because the same technique we used on the three-prisoner puzzle works on Monty Hall. We can even use the same tree diagram, we just have to change the labels. The first branching-point is now the placement of the car, and each door has the same 1/3 chance. The second branching-point is Monty’s decision about which door to open. And just like the guard, his hand is forced in all but one case: Your initial choice was door #1, so if the car is behind door #2 Monty has to open door #3. Otherwise he’ll be showing you that you have the wrong door. Likewise, he has to open #2 if the car is behind #3. Only if the car is behind #1 does Monty have a choice. So there are two possible situations where Monty opens door #3. But one of them is half as likely as the other, namely the one where the car is behind door #1. So, when you see Monty open door #3, the car is probably behind door #2. Switch! The Total Evidence Requirement is clearly something to take seriously. To get the right answer, you have to take account of all the information you have. Monty isn’t just showing you that door #3 is a bust, he’s also likely giving something away with his choice of door to open. So it’s as important to consider how you get your information as it is to consider what information you got. These days there’s a lot of talk about the dangers filter bubbles. But the news has always been a selective window onto the world, because only some things sell. “Man Bites Dog” is news, as journalists say, while “Dog Bites Man” is not. Or, to take a more political and realistic example from the recent news, “Seven Die in Terrorist Attack on London Bridge” is news, “Thousands Die Quietly of Cancer” is not. "],
["the-gamblers-fallacy.html", "2 The Gambler’s Fallacy", " 2 The Gambler’s Fallacy My wife’s family keeps having girls. She’s one of three sisters (no brothers), and each sister has two daughters (no sons). That’s nine girls in a row! They’ve gone two generations with no boys yet. So family gatherings often turn to the obvious question: are we due for a boy next? Here are three different answers, each with a perfectly sensible looking rationale to back it up. Answer #1. Yes, the next baby is more likely to be a boy than a girl. Ten girls in a row is really unlikely. (Less than a tenth of a percent chance, if you want an exact number.) So the tenth baby will most likely be a boy. Answer #2. No, it’s even odds on boy vs. girl. A baby’s sex is determined by an isolated, random event. So it’s like a coin flip, a fifty-fifty shot every time. Answer #3. No, a girl is actually more likely! Girls run in the family, clearly. So although it could be a boy, in this family girls have the edge statistically speaking. Discussions like this tend to ignore the fact that a lot of people are born intersex. And ignoring intersex births has been the source of enough suffering in the world already, so let’s try to avoid that mistake. We’ll put our question this way: how does a string of nine girls in a row affect the chance of another girl? Do the odds go up? Down? Or do the stay the same? "],
["independence.html", "2.1 Independence", " 2.1 Independence It all hangs on whether the sex of each baby is independent of the others. Two events are independent when the outcome of one doesn’t change the probabilities of the other. A stock example of independence is sampling with replacement. Imagine an urn with 50 black marbles and 50 white ones. You draw a ball at random, put it back and give the urn a shake, and then draw again. Even if you draw white balls ten times in a row here, the odds of black-vs-white on the eleventh draw are still fifty-fifty. The draws are independent because you always put the marble from the previous draw back. That way every time you reach in the urn there just as many black balls as white ones. Now imagine the same exercise without replacement. Every time you draw a ball, you set it aside instead of putting it back. Now the draws are dependent. If you draw ten white balls in a row, the next draw is less likely to come up white. Now there are fifty black balls to just forty white ones. "],
["bias.html", "2.2 Bias", " 2.2 Bias Flips of an ordinary coin (like a Canadian Loony) also illustrate independence. Even if you get ten heads in a row, the eleventh toss is still fifty-fifty. That’s because ordinary coins—not the kind you get at a magic shop—are symmetric and evenly balanced. So each toss is… well, a tossup. If it’s really an ordinary coin, the initial ten heads in a row is just a coincidence. Coin flips aren’t just independent though, they’re also unbiased: heads and tails are equally likely. A process is biased if some outcomes are more likely than others. Like a loaded coin that comes up heads three quarters of the time is biased. "],
["fairness.html", "2.3 Fairness", " 2.3 Fairness So coin flips are both unbiased and independent, which makes them fair: fair = no bias + no dependence. Fair processes are important because nobody can get an edge. In a casino, the dice are fair, the roulette wheels are fair, the decks of cards are fair… And that means anybody who walks up to the roulette wheel or the craps table or the blackjack table can start gambling without any disadvantage. (How do casinos make money then? Well, in roulette for example, the payouts don’t match the odds of winning. The wheel is fair but the prices aren’t!) "],
["the-gamblers-fallacy-1.html", "2.4 The Gambler’s Fallacy", " 2.4 The Gambler’s Fallacy People sometimes forget that fair processes are independent, a mistake so tempting and common it has its own name: the gambler’s fallacy. If a roulette wheel comes up black five times in a row, some gamblers figure it’s “due” for red. If they get a bunch of bad hands in a row at poker, they figure they’re due for a good one. With a fair process, it’s unlikely for the same thing to keep happening over and over for a long time. If you flip a coin ten times in a row, you expect to get a mix of heads and tails. So when the same thing does happen many times in a row, people figure it has to change soon. But this way of thinking neglects independence! A fair process is also an independent process, by definition. It was really unlikely that you’d get a streak of ten heads in a row. But once the streak has happened, the eleventh toss is a fresh start—another fifty-fifty tossup. "],
["fallacies-vs-misfortunes.html", "2.5 Fallacies vs. Misfortunes", " 2.5 Fallacies vs. Misfortunes But wait: imagine you flip a coin a thousand times and it lands heads every time. Every. Damn. Time. Shouldn’t you at least be suspicious? It sure looks like something weird is going on, something that makes this coin land heads repeatedly. So shouldn’t you expect heads on the next flip? How could that be a fallacy? It’s not a fallacy! It’s perfectly good reasoning. It’s only a fallacy when you know the process is fair (or assume it is). And here, you doubt the process is fair, with good reason. The lesson: there’s a difference between a fallacy and misfortune. A fallacy is a logical error, a failure to correctly use the information you have. When you know a process is fair but neglect the independence that entails, that’s an error of logic. You should know better. But sometimes you just get bad information. If a fair coin really did land heads a thousand times in a row, you’d be forgiven for thinking it’s not actually fair. You’d be in the unfortunate position of having some misleading information—really misleading information in this example. (But don’t worry too much. Information this misleading is also really unlikely.) "],
["the-hot-hand.html", "2.6 The Hot Hand", " 2.6 The Hot Hand When a basketball player hits several shots in a row, they’re said to be on fire, which many people take seriously. They think the rest of the team should feed the ball to the player with the hot-hand because they’re more likely to make a shot. But a famous study published in 1985 found that a player’s shots are actually independent. Most people don’t know about that study, though. And certainly nobody knew what the result of the study would be before it was conducted! So a lot of believers in the hot hand were in the unfortunate position of just not knowing a player’s shots are independent. So the hot hand isn’t the same as the gambler’s fallacy. That doesn’t mean believers in the hot hand are off the logical hook, though. The same study also analyzed the reasoning that leads people to think a player’s shots are dependent. Their conclusion: people tend to see patterns in sequences of misses and hits even when they’re random. So there may be another fallacy at work. There might even be more than one. If players and fans want to think that “the zone” is a real place one can get into, then maybe they’re guilty of wishful thinking, too. Oh, one other thing. Some recent studies from Stanford and Harvard found that the hot-hand may actually be real after all! How can that be, what did the earlier studies miss? It’s still being looked into, but one possibility is: defense. When a basketball player gets in the zone, the other team ups their game. The hot player has to take harder shots. So the Harvard study added a correction to account for increased difficulty, and the Stanford study looked at baseball instead. Then they found evidence of streaks. “Applied statistics is hard,” as influential statistician Andrew Gelman once said. "],
["girls-revisited.html", "2.7 Girls Revisited", " 2.7 Girls Revisited So what about my wife’s family and their nine girls in a row? As best I can tell, the available evidence says girls running in the family isn’t a thing (or boys either). So the odds of a girl next are unchanged. Like the hot-hand though, most people don’t know about the research on this question. And that includes my in-laws. So even if they figure we’re due for a boy, they’re not guilty of the gambler’s fallacy. Are they guilty of a different fallacy though? Like the basketball fans who see non-existent patterns in random sequences of missed and hit shots? I’m going leave this one up to you, the reader. My in-laws might end up reading this book some day! "],
["the-taxicab-problem.html", "3 The Taxicab Problem", " 3 The Taxicab Problem In 1972, two psychologists who would go on to win a Nobel prize for their research into human reasoning, asked participants in one study the following question: A cab was involved in a hit and run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data: 85% of the cabs in the city are Green and 15% are Blue. A witness identified the cab as Blue. The court tested the reliability of the witness under the same circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. What is the probability that the cab involved in the accident was Blue rather than Green? It’s really tempting to say 80%, and most people answer something like that. But the right answer is much lower, almost by half: 41%. How could it be so low, when the witness gets each colour right 80% of the time? The short answer is: because there are so many more green cabs than blue. But that needs some explaining. True, the witness only mistakenly says “blue” 20% of the time when the cab is really green. But almost all the cabs are green, so his “blue” mistakes end up being fairly common. With so many green cabs rolling by, even mistaking only 20% of them for blue makes for a lot of false “blue” reports. Especially compared to the small number of blue cabs actually on the road. So a lot of the time when the witness says “blue”, the cab is really green. In fact, they’re wrong more often than they’re right when it comes to “blue” cabs. As usual, it’s way easier to see what’s going on with a diagram. So imagine there are just 100 cabs in town, 85 blue and 15 green. We’ll draw a square for each cab: The dashed blue line indicates the cabs the witness will identify as “blue”. It encompasses 80% of the blue squares and only 20% of the green squares. But with so many more green squares than blue ones, it ends up including more green squares than blue ones. We can also see where the 41% figure comes from now. There are 17 green squares in the dashed blue area; because that’s 20% of 85. And there are 12 blue squares in the dashed blue region; that’s 80% of 15. So all in all then there are 29 cabs the witness calls “blue” (17 + 12 = 29). But only 12 of those really are blue, and 12/29 is about 41%. "],
["conditional-probabilities.html", "3.1 Conditional Probabilities", " 3.1 Conditional Probabilities What makes the taxicab problem so confusing? Well for one, it’s easy to get these two things muddled: The chance the witness will say “blue” when a cab is blue: 80%. The chance a cab is blue when the witness says “blue”: 41%. Schematically, this is the difference between: blue \\(\\overset{\\small 80\\%}{\\Longrightarrow}\\) “blue” “blue” \\(\\overset{\\small 41\\%}{\\Longrightarrow}\\) blue. We’re being asked to calculate the second number in the taxicab problem, whereas it’s the first number that’s given to us. The problem says: The court […] concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. Which means they correctly identify blue cabs as “blue” 80% of the time, and likewise for green. But it’s not exactly super clear from the wording, so it’s easy to read it as saying “blue” \\(\\overset{\\small 80\\%}{\\Longrightarrow}\\) blue. But even once that’s sorted out, it’s still tricky. You might have assumed the order doesn’t matter: if it’s 80% in one direction, then it’s 80% in the other direction too, no? No. The numbers in each direction are actually independent. If you live in Toronto there’s a 100% chance you live in Canada, but not the other way around. Very few Canada-dwellers live in Toronto. Likewise, most humans are “pentadactyl”—they have five digits on each limb. But very few pentadactyl creatures are people. Just think of all the chimps, gorillas, cats, cows, and even bats! They’re all pentadactyl too. (Evolution is weird.) Once you’ve seen a few examples, a diagram helps illustrate the general point. Given two cases \\(A\\) and \\(B\\), they can overlap in all kinds of ways. We can have most of the As be Bs, but not vice versa: Or we can have most As be Bs and vice versa: In probability-speak, we’re learning crucial lessons about conditional probability: the chance of something under a certain condition. The probability the witness will say “blue” given that the cab really is blue is 80%. That’s usually written \\(Pr(W \\given B) = 80\\%\\): the probability of \\(W\\) (the witness saying “blue”) given \\(B\\) (the cab really being blue). Put another way, suppose for the moment the cab really is blue. How likely is it then the witness will say “blue”? 80%. The general format is: \\[ Pr(Q \\given P) = x \\mbox{ means the probability of $Q$, given $P$, is $x$.}\\] The \\(P\\) to the right of the \\(\\given\\) is the condition, the thing we are supposing for the moment is true. The \\(Q\\) to the left of the \\(\\given\\) is the thing we are considering the probability of, given \\(P\\) as an assumption. So in the taxicab problem we’re told \\(Pr(W \\given B)\\), the probability the witness will say “blue” given the cab really is blue. What we’re asked to figure out is \\(Pr(B \\given W)\\), the probability the cab really is blue given that the witness said “blue”. And using our diagram we found that was about 12/29, or about 41%. "],
["doctors-without-base-rates.html", "3.2 Doctors Without Base Rates", " 3.2 Doctors Without Base Rates So now you’re all trained up and immunized, right? You’ll never be one of those eighty-percent suckers again, right?? Turns out even people who should know better, people with extensive scientific and statistical training, fall prey to this fallacy. And that includes medical doctors, which you might find… concerning. Consider a relatively rare virus, like HIV. Very few North Americans have HIV and don’t know it, fewer than one in every thousand. Now imagine a highly accurate (and highly fictional) blood test. It always detects the presence of the virus, and only gives a false positive in a tenth of a percent of cases. If you take the blood test and you get a positive result, how worried should you be? The test is so accurate, it’s hard to see how there could be much room for optimism. But, of course, if it were that simple we wouldn’t be talking about it. Imagine taking a thousand random North Americans and giving them the blood test. The one person with HIV will test positive. But so will one of the 999 people who don’t have HIV. Because a tenth of a percent of 999 is basically one (0.999 if you want to be exact). So even if you test positive, there’s still a fifty-fifty shot you don’t have HIV. Out of every two positive tests, there’s one true and one false—so yours might be the false positive. One way of thinking about what’s going on here is that you have two pieces of relevant information. One the one hand, the test is highly accurate, which points to you having the virus. But on the other, very few people have the virus, which points the opposite way. So you have to work through the numbers to see how those two, conflicting pieces of information balance out. The tendency to focus on just the first piece of information, the accuracy of the test, is called “base rate neglect”. Because, well, the other piece of information, the rate of HIV in the overall population, is called the “base rate”. So doctors know better than to neglect the base right, right? Well, maybe not as much as you’d hope. In study after study, everyone from undergraduates with no statistical training, to medical students, to trained and experienced medical doctors seem prone to base rate neglect. For example, in one study of 160 gynecologists, only 21% got the right answer on a similar problem (breast cancer instead of HIV, mammograms instead of blood tests). But there is some good news. After a bit of training, 87% of those same gynecologists solved these problems correctly. Curiously, part of that training was one weird trick: translating the problem into natural frequencies. Instead of “the chance of a North American having HIV is a tenth of a percent”, try “one in every thousand North Americans has HIV”. Rather than frame things in terms of percentages or probabilities, frame them in terms of sets of individuals—a thousand North Americans, or a hundred taxicabs. That makes it much easier for humans to find the right answer, these psychologists say. You may have noticed this kind of natural frequency language in medical pamphlets, like at your doctors office. Because of this research on human reasoning, it’s increasingly common to explain the prevalence of some condition, or how to interpret a certain diagnosis, in natural frequency terms. And, you might have noticed, I used it too when I explained how to get the right answers! "],
["bayes-theorem.html", "3.3 Bayes’ Theorem", " 3.3 Bayes’ Theorem There’s also a formula you can use, and it’s such an important formula in so many fields that we should get to know it now. The formula tells you how to compute the probability of the hypothesis you’re interested in, given the evidence you’ve received. So let’s use \\(H\\) to stand for our hypothesis, and \\(E\\) for the evidence. Bayes’ Theorem \\[ Pr(H \\given E) = Pr(H)\\frac{Pr(E \\given H)}{Pr(E)} \\] We saw earlier that the direction matters with conditional probabilities. 100% of Torontonians live in Canada, but only some Canada-dwellers live in Toronto. So \\(Pr(H \\given E)\\) isn’t the same thing as \\(Pr(E \\given H)\\). One thing Bayes’ Theorem tells us, though, is that there is a relationship. You can get from \\(Pr(E \\given H)\\) to \\(Pr(H \\given E)\\), if you have some additional information. Specifically, you need to have: \\(Pr(H)\\), the probability of the hypothesis before the evidence \\(E\\) is taken into account. \\(Pr(E)\\), the unconditional probability of the evidence, the chance it would be true if you didn’t yet know yet whether \\(H\\) is true or not. And, it turns out, we have all that information in the kinds of problems we’ve been discussing in this chapter. Let’s apply Bayes’ Theorem to the taxicab example to illustrate. We want to know \\(Pr(B \\given W)\\), the probability the cab is blue (\\(B\\)) given the witness said it was (\\(W\\)). We know the reverse probability: \\(Pr(W \\given B) = 80/100\\), because the court found witness to be 80% accurate. So we just need… \\(Pr(B) = 15/100\\), because only 15% of cabs are blue, and \\(Pr(W) = 29/100\\), because we calculated the witness will identify 29 out of every 100 cabs as “blue” (17 of the green ones, 12 of the blue ones). So we can just plug all those numbers into Bayes’ theorem and get: \\[ \\begin{aligned} Pr(B \\given W) &amp;= Pr(B)\\frac{Pr(W \\given B)}{Pr(B)}\\\\ &amp;= 15/100 \\frac{80/100}{29/100}\\\\ &amp;\\approx .41 \\end{aligned} \\] This is actually the same calculation we used to solve this problem before. But now we know a general formula for doing the same calculation in any problem that has the same structure! That’s nice for a few reasons. One is that pictures and diagrams won’t always be manageable. Sometimes the numbers are just too large or too small. But also, once you have the formula you can program a computer to do the calculation for you! "],
["heuristics-biases.html", "3.4 Heuristics &amp; Biases", " 3.4 Heuristics &amp; Biases Daniel Kahneman and Amos Tversky are the two psychologists who first tried out the taxicab problem on experimental subjects in 1972. Ands they eventually won the Nobel Prize in economics for their work on human reasoning. They revolutionized the field, discovering numerous ways human reasoning violates the laws of probability theory and statistics, beyond just neglecting base rates. The research program they launched came to be known as the “heuristics and biases” program. The idea being that humans use heuristics—quick and dirty shortcuts—to make reasoning easier. But, although these shortcuts work pretty well, they don’t always. They result in certain biases, predictable patterns of error, like base rate neglect. Here are two more examples from their research. 3.4.1 The Bank Teller Fallacy This question is from a paper Kahneman and Tversky published in 1983: Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? Linda is a bank teller. Linda is a bank teller and is active in the feminist movement. Almost 85% of their subjects chose (2). But that can’t be right, and you don’t need to know anything about Linda to prove it. Imagine we gather a thousand random people in a big room. Some of them will be bank tellers; let’s gather them in a circle at the center of the room. Some of the people in that bank teller circle will be feminists. But there can’t be more feminist bank tellers in that circle than people in the circle So if you choose a random person, it’s more likely they’ll be a bank teller than both a bank teller and a feminist. Even if we just consider people who meet the description of Linda, the same is true. Suppose we make sure the thousand people gathered in our big room all meet the description of Linda. Our central circle of bank tellers might have more feminists in it. But it still couldn’t have more feminist bank tellers than bank tellers of any political stripe. There just can’t be more feminist bank tellers than bank tellers, even among the Linda-type people. So a randomly selected Linda-type person can’t be more likely to be a feminist bank teller than a bank teller (of whatever politics). So why do people choose (2)? Kahneman and Tversky suggested that people are relying on a “representativeness” heuristic. They’re gauging how much the description of Linda is representative of the class of bank tellers (not very) vs. the class of feminist bank tellers (more so). Ordinarily, representativeness is a decent guide to probability. But not always. 3.4.2 R is for Terror Where is the letter R more likely to occur in an English word, as the first letter or as the third letter Most people say first letter. But (of course) third position is actually more likely. (That’s not something you can just figure out by logic, mind you. You have to run a search on a dictionary and count.) So why do people tend to think first position is more likely? Kahneman and Tversky hypothesized another heuristic: “availability”. It’s easier to think of words that begin with R—they’re more readily available in memory. And the easier it is to imagine or recall an example of something, the more frequent it seems. "],
["social-distortions.html", "3.5 Social Distortions", " 3.5 Social Distortions Before moving let’s pause to reflect on the social and political implications of all this. If psychologists like Kahneman and Tversky are right, and our reasoning is distorted by heuristics like availability, then are those distortions reflected in the shape of our society? You could write whole books on this question, and people do. So I’ll just note one possible (and provocative) answer, represented in the following remark from psychologist Joshua D. Foster: the 2009 budget for homeland security (the folks that protect us from terrorists) will likely be about $50 billion. Don’t get us wrong, we like the fact that people are trying to prevent terrorism, but even at its absolute worst, terrorists killed about 3,000 Americans in a single year. And less than 100 Americans are killed by terrorists in most years. By contrast, the budget for the National Highway Traffic Safety Administration (the folks who protect us on the road) is about $1 billion, even though more than 40,000 people will die this year on the nation’s roads. That’s hardly the final word on this question. But I think it’s a decent place to start thinking about such matters. "],
["simpsons-paradox.html", "4 Simpson’s Paradox", " 4 Simpson’s Paradox In 1973, the University of California at Berkeley got worried about being sued for discriminating against women. In their graduate admissions that year, only 35% of female applicants had been admitted, compared to 44% of male applicants. A professor in the statistics department was recruited to look into it. He found that men and women actually had similar acceptance rates department by department. In fact, women had a higher acceptance rate across most of the larger departments. So how did women end up with a lower acceptance rate overall? Because, it turned out, women had applied more to the more selective departments. Many more women than men had applied to the English department, for example, which admitted only a small fraction of its applicants. That doesn’t mean sexism played no role here necessarily. You might wonder why women and men differed so much in their choices of where to apply, for example. And just because one group has a higher admissions rate doesn’t mean they aren’t being discriminated against. Harvard once put an admissions cap on Jews because they were being admitted in such large numbers! But it does mean the initial appearance of discrimination in Berkeley’s 1973 admissions was misleading. And it’s a great illustration of Simpson’s paradox, where a trend appears at one level of analysis yet reverses at a more fine-grained level. In fact, it’s even possible for one group to have higher admissions across the board, in every department, and still have a lower admission rate overall. Imagine a fictional case with just two departments. And let’s get all the information into a single chart by using width to display the number of applicants: The blue bars are taller than their partner red bars, and yet there’s much less blue area overall. So what’s the moral? There’s a kind of “part-whole” fallacy to watch out for with probabilities. What’s true of all the parts isn’t necessarily true of the whole. Just because a trend is present in every slice of a population doesn’t mean it’ll be present in the population overall. "],
["the-law-of-total-probability.html", "4.1 The Law of Total Probability", " 4.1 The Law of Total Probability There is a right way to do part-whole reasoning with probabilities, though! And it introduces us to a powerful companion principle to Bayes’ theorem from the last chapter. Consider this question: if 20% of all applicants to Chemistry are admitted, and 30% percent of applicants to Philosophy, what’s the overall admittance rate across these two departments? Well, it has to be somewhere between 20% and 30%. But what percentage exactly? It won’t necessarily be smack in the middle at 25%. Imagine 100 people apply to Chemistry and only 10 to philosophy. Then 23 people will be admitted out of 110. So the overall rate will be about 21%. In the reverse situation where 100 people apply to Philosophy and 10 to Chemistry, 32 people will be admitted out of 110, or about 29%. So the overall rate is closer to the departmental rate of the larger department. And if they’re equal sizes, then it will be smack in the middle at 25%. The Law of Total Probability describes this general pattern. A person’s chance of being admitted, \\(Pr(A)\\), is somewhere between their chances of being admitted to Chemistry if they apply there, \\(Pr(A|C)\\), and their chance of being admitted to Philosophy if they apply there, \\(Pr(A|P)\\): \\[ Pr(A|C) &lt; Pr(A) &lt; Pr(P). \\] (I’m assuming they applied to just one department, for simplicity.) But where exactly will \\(Pr(A)\\) fall in that interval? Well, the more likely it is they applied to Chemistry, the closer it will be to the left end. And the more likely it is they applied to Philosophy, the closer it’ll be to the right. Here’s another way to think about the same principle. Suppose you want to know whether \\(A\\) is true—whether you’ll get an A in your next philosophy class, for example. One way to get a handle on that is to break the question down into cases. Maybe the class will be boring, maybe not. We’ll use \\(B\\) for boring and \\(\\neg B\\) for not boring (the tilde symbol \\(\\neg\\) being a common way to symbolize negation). If the class is boring your chances of getting an A aren’t so good, let’s suppose. Wheres you’re likely to do well if it’s not boring. So the more likely it is the class is boring, the lower your chances of getting an A will be. That kind of reasoning is captured and made precise by: The Law of Total Probability \\[Pr(A) = Pr(A \\given B) Pr(B) + Pr(A \\given \\neg B) Pr(\\neg B).\\] In terms of our tree-diagramming technique, the LTP says to add up the final quantities from the \\(A\\)-and-\\(B\\) and \\(A\\)-and-\\(\\neg B\\) branches to get \\(Pr(A)\\). "],
["bayes-ltp-awesome.html", "4.2 Bayes + LTP = Awesome", " 4.2 Bayes + LTP = Awesome A big part of why the LTP is interesting is that it makes for a powerful combination with Bayes’ theorem. In fact, you often need the LTP to get to the point where you can apply Bayes’ theorem. Remember the problem of base rate neglect? In one example, we imagined a fictional blood test for HIV that always comes up positive when the virus is present. But real medical tests aren’t so dependable. They can generate false negatives as well as false positives. And the LTP helps us get the right answer in these more realistic cases. Here’s an example, still fictional, but more realistic: About 1 in every 100 people have Weisberg’s syndrome (an annoying but mostly harmless disease). There’s a blood test that’s 90% accurate: in 90% of cases where the disease is present, the test comes up positive, and in 90% of cases where it’s absent the test comes up negative. Suppose a randomly selected person tests positive. What are the chances they have the disease? Bayes’ theorem says that for any hypothesis \\(H\\) and piece of evidence \\(E\\): Bayes’ Theorem \\[ Pr(H \\given E) = Pr(H)\\frac{Pr(E \\given H)}{Pr(E)}. \\] We want to calculate \\(Pr(D \\given P)\\), the probability of having Weisberg’s disease given a positive blood test. So Bayes’ theorem says: \\[ Pr(D \\given P) = Pr(D) \\frac{Pr(P \\given D)}{Pr(P)}.\\] So we need the three numbers on the right hand side: \\(Pr(D) = 1/100\\), that’s the base rate given in the problem. \\(Pr(P \\given D) = 90/100\\), that’s the test-accuracy, also given. \\(Pr(P) = \\ldots\\) uh-oh, this number isn’t given in the description of the problem! You guessed it, LTP to the rescue! We can calculate the chance of a positive blood test, \\(Pr(P)\\), by breaking it into two cases. What’s the chance of a positive test if you really have the disease, and what’s the chance if you don’t. Then weigh each case according to the chance it’s true. In terms of the LTP, that means we calculate: \\[ \\begin{aligned} Pr(P) &amp;= Pr(P \\given D)Pr(D) + Pr(P \\given \\neg D)Pr(\\neg D)\\\\ &amp;= (90/100)(1/100) + (10/100)(99/100)\\\\ &amp;= 1,080/10,000\\\\ &amp;= 27/250. \\end{aligned} \\] Where did I get the 99/100 term from, you might be wondering? Well if 1 in 100 people have the disease then 99/100 people don’t. So the chance of a positive blood test is 27/250, which is about 11%. And now we have all three terms we need to apply Bayes’ theorem: \\[ \\begin{aligned} Pr(D \\given P) &amp;= 1/100 \\frac{90/100}{27/250}\\\\ &amp;= 0.08333\\ldots \\end{aligned} \\] Once again, the answer is pretty counterintuitive. Despite a test with 90% accuracy, there’s still a less than 10% chance you have the disease if you get a positive test. Base rates, don’t neglect ’em. But the new lesson is: Bayes’ theorem and the LTP together can solve some problems we wouldn’t be able to solve otherwise. In fact, you’ll often see Bayes’ theorem written with the LTP already built in to the denominator: \\[ Pr(H \\given E) = \\frac{Pr(E \\given H)Pr(H)}{ Pr(E \\given H) Pr(H) + Pr(E \\given \\neg H)Pr(\\neg H)}.\\] That’s ugly, but really useful if you’re doing these calculations on the regular. We’re more interested in the philosophical significance of Bayes’ theorem, though. And this messy version obscures a lot of philosophically interesting stuff, as we’ll see in the next few chapters. So we’ll stick to the simple version. "],
["the-raven-paradox.html", "5 The Raven Paradox", " 5 The Raven Paradox Generalization is an essential part of science. Without general principles, we couldn’t predict how things will turn out. Will this vaccine immunize you or make you sick? Does the rocket have enough fuel to make it into orbit? General principles of medicine and physics give us the answers here, if we can discover them. Generalities also help us understand the world. Why does your bicycle stay upright as long as the wheels are turning, then tip over when you stop? Why do rainbows appear after a rainstorm. General principles about momentum and electromagnetism illuminate these phenomena. But how does science verify a generalization? How do we establish that a principle is generally true? The obvious and classic answer: by verifying its instances. To establish that all electrons have negative charge, examine lots of electrons. To test whether all humans mortal, go and examine lots of humans. These reflections lead to the principle of scientific reasoning known as Nicod’s Criterion: Nicod’s Criterion A generalization is confirmed by each observed instance of it (unless you’ve already found a counterexample, of course). It’s a simple and plausible idea. So of course it leads to paradox. "],
["the-raven-paradox-1.html", "5.1 The Raven Paradox", " 5.1 The Raven Paradox In the mid-20th Century, one group of philosophers sought to discover the fundamental principles of scientific reasoning. They wanted a “logic of confirmation”, as Carl Hempel called it. Would Nicod’s Criterion be part of that logic? Maybe not: Suppose you’re an ornithologist testing the hypothesis that all ravens are black. You could go out looking for ravens, but you’re short on research funds. So instead you adopt the following, perverse method. You wander around campus looking for non-black things and verifying that they are not ravens: red shoes, brown walls, yellow pencils, green statues, etc. Eventually you bump into your department chair, who wants to know why you aren’t doing your research. “I am!”, you reply, “I’m just making clever use of Nicod’s Criterion to make the job easier.” “How in the world is that?!”, your chair asks in disbelief. Your cheery retort: “With each observation of a red shoe or a yellow pencil, I confirm the hypothesis that all non-black things are non-ravens. And that’s the same as proving the hypothesis that all ravens are black! Because the two hypothesis are logically equivalent.” Your chair is a logic nerd, it turns out, and so she recognizes you’re right about one thing here. The following two hypotheses are indeed logically equivalent: All ravens are black. All non-black things are non-ravens. Imagine collecting all the ravens in the world and all the black objects into a room, and placing them into designated circles: If all ravens are black, the only part of the raven circle with anything in it will be the part that overlaps with the black circle. The crescent region on the left will be empty: But that’s just what the second hypothesis says, too. If all the non-black things are non-ravens, then all the things outside the black circle must also be outside the raven circle. So again, the left crescent will be empty. And yet, your department chair flatly denies your request for more funding. She doesn’t even miss a beat; it’s a hard pass. “What gives?”, you protest. “You agreed these are equivalent hypotheses. Are you saying my observation only confirm one of them?” “No”, says your chair, “I agree with Carl Hempel’s famous Equivalence Condition: whatever confirms one of these hypotheses must confirm the other as well.” “So you’re questioning Nicod’s Criterion??” “No”, she replies, “not right now, anyway.” “So what’s the problem?” “Well, you are confirming that all ravens are black. But only very, very slowly. So slowly that it’s not worth doing. Each observation you make of a red shoe, or a yellow pencil, confirms the hypothesis. But by so little that it might as well be no support for the hypothesis at all.” What in the world is she talking about? "],
["the-hosiasson-lindenbaum-solution.html", "5.2 The Hosiasson-Lindenbaum Solution", " 5.2 The Hosiasson-Lindenbaum Solution It turns out your chair did a minor in philosophy, where she read about a little-known Polish logician named Janina Hosiasson-Lindenbaum, who was shot dead by the Nazis in 1942. But two years before that, she published an influential analysis of precisely the issue you and your chair are debating. She concluded that observing a red shoe does support the hypothesis that all ravens are black, but by a tiny, negligible amount. And, it turns out, philosophers nowadays generally agree that she was right. How could a red shoe have any bearing at all on ravens and their colouring? Well, any object you observed could, in principle, disprove your hypothesis. Each turn of your head could reveal (say) a white raven. When you see something else instead, like a red shoe, the hypothesis passes a sort of test. But it’s only a very weak sort of test. Most things you encounter aren’t black, and aren’t ravens. So you’ll probably turn your head to see something non-black, in which case it just has to be a non-raven for the hypothesis to avoid disproof. Since ravens aren’t a frequent sight, the hypothesis will probably pass this test. But if you go looking for ravens, then the hypothesis faces a more serious test. It says each raven you find will be black. Assuming you haven’t yet established anything about ravens’ colouring, the hypothesis could well turn out to be wrong. Some of the ravens you find could well be white. When you find they’re not, your hypothesis’ bona fides get a real, substantial boost. "],
["hats-grasshoppers.html", "5.3 Hats &amp; Grasshoppers", " 5.3 Hats &amp; Grasshoppers So is Nicod’s Criterion legit then? Your chair hinted at reservations, and for good reasons. Nicod’s Criterion seems plausible, and it applies just fine to lots of cases. But not to all cases. And that means it can’t be a fundamental scientific principle—part of the “logic of confirmation”. Here’s one example where it fails: Three philosophers go to dinner and check their hats at the door. After dinner the waiter returns their hats, and philosopher A notices she has B’s hat. Now consider this general statement: each philosopher has been given the wrong hat. Will this be confirmed or disconfirmed if B turns out to have A’s hat? The answer: it will be disconfirmed, even though this would be a positive instance of the general claim. If B finds that he has A’s hat, then that’s a case of getting the wrong hat. But it also means C will definitely get the right hat, since hers is the only hat left unaccounted for. I like that example because it’s tidy. It’s easy to keep track of just three hats in your head, and the consequences are perfectly certain. C must get her own hat if A and B have been given each other’s. But the tidiness here comes at the price of realism, as it so often does. So here’s a less tidy, and slightly more realistic, example: An entomologist friend of yours tells you about a remote island in the Atlantic Ocean, Pitcairn Island, which has no grasshoppers. In other words: all the grasshoppers in the world live outside of Pitcairn Island. A few years later, you and your friend take a trip to a neighbouring island, where you see many grasshoppers. You express some doubts about your friends prior assurances. But he insists that these are positive instances of his hypothesis: they are examples of grasshoppers living outside of Pitcairn Island. But, to settle the matter, he agrees to take the ferry with you over to Pitcairn Island. At the docks, the ferry arrives from Pitcairn and a horde of grasshoppers disembarks, swarming past you. “More confirmation of my hypothesis!”, your friend declares. “A whole swarm of grasshoppers outside of Pitcairn Island!” I hope you will agree that your friend’s scientific credentials might now be in doubt. "],
["the-grue-paradox.html", "6 The Grue Paradox", " 6 The Grue Paradox A lot of science works by extrapolating from observed patterns. Pollsters survey a sample of voters to gauge the leaning of the electorate as a whole, for example. If 54% of respondents prefer Candidate X, they conclude that 54% of all voters (give or take) prefer X. Climate scientists extrapolate from historical trends to estimate future trends. The planet has been steadily warming for the last 100 years. That’s one reason to expect it will continue to warm in the coming years. A fundamental principle of scientific inquiry seems to be something like: expect the unobserved to resemble the observed. Philosophers call this the Principle of Induction. "],
["a-gruesome-concept.html", "6.1 A Gruesome Concept", " 6.1 A Gruesome Concept But one philosopher, Nelson Goodman, identified a difficult puzzle about the principle of induction. It revolves around a bizarre concept of his invention, which he dubbed “grue”. There are two ways for an object to be grue. Some green things are grue, but only some. It depends on when we first encounter them. If our first observation of a green object happens before the year 2020, then it’s grue. So the Statue of Liberty is grue: it’s a green object that was first observed before the year 2020 (long before). But if our first encounter with a green object is in the year 2020 or later, then it’s not grue. Likewise if we never observe it—because it’s on the far side of the universe, for example, or buried deep underground. I said there are two ways for an object to be grue: some blue objects are grue, too. Not the ones observed before 2020, though. Instead it’s the ones that aren’t observed before 2020. If a blue object is observed for the first time after 2019, or it’s never observed at all, then it’s grue. So blue sapphires that won’t be mined before the year 2020 are grue. As usual, it helps to have a diagram: We can also define ‘grue’ in explicit, verbal terms: Grue An object is grue if EITHER (a) it is green and first observed before the year 2020, OR (b) it’s blue and not observed before 2020. To test your understanding, see if you can explain why the following are examples of grue things: the $20 bill in my pocket, Kermit the Frog, the first (Canadian) $5 bill to be printed in 2020, the first sapphire to be mined in 2020, and blue planets on the far side of the universe. Then see if you can explain why these things aren’t grue: fire engines, the Star of India, and the first $20 bill to be printed in 2020. Once you’ve got all those down, try this question: do grue objects change colour in the year 2020? It’s a common confusion to say they do. But no, grue objects don’t change colour. The Statue of Liberty is green and (let’s assume) it always will be. So it’s grue, and always will be, because it’s a green thing that was first observed before the year 2020. Part (a) of the definition of ‘grue’ guarantees that. The only way time comes into it is in determining which green things are grue (and which blue things). If a green thing is first observed before 2020, then it’s grue, ever and always. Likewise, if a blue thing is not first observed before 2020, then it’s grue, and always has been! "],
["the-paradox.html", "6.2 The Paradox", " 6.2 The Paradox So what’s the big deal about grue? Well ask yourself whether you’ve ever seen a grue emerald. I have. In fact, every emerald I’ve ever seen has been grue. And the same goes for every emerald you’ve ever seen. Every emerald anyone has ever seen has been grue. Why? Because they’re all green. And they’ve all been observed before the year 2020 (it’s 2017 as I write this). So they’re all grue the first way—they all satisfy part (a) of the definition. (Notice it’s an EITHER/OR definition, so you only have to satisfy one of the two parts to be grue.) So all the emeralds we’ve ever seen have been grue. Let’s apply the Principle of Induction then: All observed emeralds have been grue. Therefore all emeralds are grue. But if all emeralds are grue, then the first emeralds to be mined in 2020 will be grue. And that means they’ll be blue! Because they won’t have been observed before 2020, so the only way for them to be grue is to be blue. So there are blue emeralds out there, just waiting to be pulled out of the earth! Uh-oh, something has definitely gone off the rails here. But what? Another way put the challenge: we have two “patterns” in our observed data. The emeralds we’ve seen are uniformly green, but they’re also uniformly grue. We can’t project both these patterns into the future, though. They contradict each other starting in 2020. Now, the green pattern is the real one, obviously. The grue “pattern” is bogus, and no one but a philosopher would even bother thinking about it. So why is it bogus? What’s so special about green? Apparently the Principle of Induction has a huge hole in it! It says to extrapolate from observed patterns. But which patterns? Patterns are cheap, as any data scientist will tell you. Given a bunch of data points on an xy-plane, there are lots of ways to connect the dots. Even if they all lie on a straight line, you could draw an oscillating curve that passes through each point, or even a much wilder (and sillier) curve. Fun fact: deciding which patterns to use and which to ignore is a big part of what machine learning experts do. And it’s one reason humans are still essential to designing artificial intelligence. Thanks to our experience, and our genetic inheritance, we have lots of information about which patterns are likely to continue, and which are bogus “patterns” like grue. But how do we pass all that wisdom on to the machines, so that they can take it from here? How do we tell them the difference between green and grue? "],
["disjunctivitis.html", "6.3 Disjunctivitis", " 6.3 Disjunctivitis Here’s one very natural reply. The problem with grue is it’s a disjunctive concept: it’s defined using EITHER/OR. It suffers from “disjunctivitis”. But the beauty of Goodman’s puzzle is the neat way it exposes the flaw here. It allows us to make ‘green’ the disjunctive concept instead! How? Start by building grue a friend, a concept to fill in the missing spaces in our original diagram. We’ll call it “bleen”: Here’s how you’d define green in terms of grue and bleen then: Green An object is green if EITHER (a) it’s grue and first observed before the year 2020, OR (b) it’s bleen and not observed before 2020. Now maybe you’re thinking: you could define green that way, but that’s not how it’s actually defined. In reality, we already understand the concept of green, and we have to learn the concept of grue from its disjunctive definition. The problem is, that’s just a fact about us humans, not about grue vs. green. That’s just the way we homo sapiens happen to be built (or maybe socialized, or both). Some bizarre species of alien could grow up thinking in grue/bleen terms instead. And when they landed on Earth, we’d have to explain our green/blue language to them using an EITHER/OR definition. Then they would be looking at us thinking: you guys have a very weird, disjunctive way of thinking! What could we say to them to establish the superiority of our way of thinking? It’s been more than 70 years since Goodman first posed this challenge. Yet no answer has emerged as the clear and decisively correct one. "],
["time-dependence.html", "6.4 Time Dependence", " 6.4 Time Dependence Another common reply to Goodman’s challenge is to say that ‘grue’ is defective because it’s time-dependent. It means different things depending on the time an object is first observed. But the same reversal of fortunes that toppled the “disjunctivitis” diagnosis happens here. We can define green in terms of grue and bleen. And when we do, it’s green that’s the time-dependent concept, not grue. So we’re left in the same spot. We need some way of showing that the “true” order of definition is the one we’re used to. By what criterion can we say that green is more fundamental, more basic, than grue? "],
["the-moral.html", "6.5 The Moral", " 6.5 The Moral Though it may seem just cute or merely a curiosity, Goodman’s puzzle is actually profound. I see it as one of the deepest and most troubling problems we’ll encounter in this book, in fact. The reason why will become clearer soon, especially in the next chapter. But to give you the gist of what we’re dealing with, here is the lesson I (and many others) take from the grue paradox. Whatever the logic of science is, it cannot be written down or summarized in a few, simple principles. The Principle of Induction is barely even a sketch waiting to be filled in. And it can’t be filled in, at least not with the tools we presently have, like logic and probability. Which observed patterns should we expect to apply in general? Which ones should we tell the machines to extrapolate from, and which to ignore? We know intuitively how to answer this question in many cases. We know that green is legit while grue is bogus. But we have no way of making this tacit knowledge explicit or general, so that it can be programmed into a machine. "],
["bertrands-paradox.html", "7 Bertrand’s Paradox", " 7 Bertrand’s Paradox Wait, wait, wait: what are statistics textbooks filled with then, if not principles of good scientific reasoning? Aren’t there well established laws of probability? Can’t scientists just follow those and not worry about all this gruesome nonsense? You would think so, but no. "],
["the-cube-factory.html", "7.1 The Cube Factory", " 7.1 The Cube Factory Consider this example from the influential philosopher of science Bas van Fraassen: A factory makes cubes whose sides are always between 1 and 3 feet long. What is the probability the next cube they make will have sides between 1 and 2 feet long? The obvious answer is 1/2. The 1-to-2 range is half of the 1-to-3 range, so half the time you’d expect cubes whose sides are between 1 and 2 feet long, as opposed to between 2 and 3 feet long. But now consider this question: what’s the probability the volume of the cube will be between 1 and 8 cubic feet? The answer is a bit less obvious now, but with just a little arithmetic we get 7/26. How? Well, the range of possible volumes in cubic feet is \\(1^3\\) to \\(3^3\\), which is \\(1\\) to \\(27\\). And we want the chance the actual volume will be between \\(1\\) and \\(8\\). So that’s 7/26 of the total range: \\((8-1)/(27-1) = 7/26\\). But here’s the kicker: these two questions are actually one and the same, yet we got two different answers. How are they the same? Because a cube having sides between 1 and 2 feet long is equivalent to having volume between 1 and 8 cubic feet. If the sides are 1 foot long, the volume is 1 cubic foot. If the sides are 2 feet long, the volume is 8 cubic feet. So sides 1-to-2 feet = volume 1-to-8 cubic feet. So we have two answers to the same question. What’s the chance the next cube will be between 1 and 2 feet on a side? Put another way: that it will have volume between 1 and 8 cubic feet? We got 1/2 on our first go, but only 7/26 on the second. Which answer is right? "],
["the-principle-of-indifference.html", "7.2 The Principle of Indifference", " 7.2 The Principle of Indifference If you’re thinking we must have screwed up our arithmetic somewhere, you can go back and check. But I promise you, that’s not the cause of the problem. It’s something else, a famous principle we relied on without mentioning explicitly: The Principle of Indifference If all you know is there are \\(n\\) possibilities, each possibility has the same probability: \\(1/n\\). If all you know is that a coin has two sides, each has probability 1/2. If all you know is a die has six sides, each has probability 1/6. When the range of possibilities lies on a continuum—like if the length of a cube’s sides has to be somewhere between 1 and 3—there’s an extension of this basic idea. The Principle of Indifference II If the range of possible outcomes is an interval of size \\(x\\), and you have no information about where in that interval the truth lies, the probability of any subinterval of length \\(y\\) is \\(y/x\\). The cube factory example demonstrates a problem with this principle: it gives contradictory results depending how we frame the question. If we think about things in terms of the lengths of the sides, then \\(x = 2\\) and \\(y = 1\\), so the probability is \\(1/2\\). But if we think in terms of volume instead, then \\(x = 26\\) and \\(y = y\\), so the probability is \\(7/26\\). The problem isn’t in the arithmetic. It comes from a curious—but uncontroversial!—mathematical fact. The same set of possibilities can be represented on different scales (side-length vs volume), and its relative size on each scale can be different. The volume scale is the cube of the side-length scale, and cubing numbers results in larger increases for larger numbers. So cubing the 1-to-2 range increases its length, but cubing the 2-to-3 range increases it even more. The Principle of Indifference says to use the size of a range of possibilities to determine its probabilities. But size on which scale? That’s the problem. "],
["how-widespread-is-the-problem.html", "7.3 How Widespread is the Problem?", " 7.3 How Widespread is the Problem? The cube factory example isn’t just some rarified edge case, sadly. This problem shows up everywhere. There’s always more than one way to frame these questions. Take a train trip, for example. Maybe all you know is that the trip is 60 miles long, and the train always arrives somewhere between 11:50 and 12:10. If you apply the Principle of Indifference to that 20 minute interval, you’ll think it’s fifty-fifty whether the train will arrive by noon. But if instead you apply the principle to the train’s average speed, you’ll get a different answer. Even simple seeming “finite” situations aren’t immune to the problem. Suppose I tell you my friend is buying a car today, either a Honda or a Toyota. So it’s 1/2 probability for each, right? Except that he’s considering two models of Honda: Civic and Accord. Ok, so then it’s 1/3 each. Or is it 1/4 for each model of Honda, and 1/2 for Toyota? And what about all the different colours he might buy? And the prices he might pay? And what time will the purchase be completed? We can keep dividing up the space of possibilities finer and finer, without limit. A finite number of possible outcomes can always be infinitely divided. So, at some point, we’ll need to apply the Principle of Indifference to a continuous range of possibilities, to an interval of some length. And what length? Well, that will depend, once again, on how we frame the situation. "],
["the-laws-of-probability.html", "7.4 The Laws of Probability", " 7.4 The Laws of Probability The Principle of Indifference goes back hundreds of years. The greats who created our modern theory of probability, like Thomas Bayes and Pierre-Simon Laplace, relied on it heavily. You won’t find it in a modern math or stats textbook, though. Problems like the cube factory were well known by the late 1800’s, thanks to the mathematician Pierre Bertrand (though he used different examples). So by the time the laws of probability were codified in the 1930’s, the Principle of Indifference was on the outs. What will you find in a modern textbook then? Just three, simple rules. First, probabilities are always numbers between 0 and 1. This is barely even a rule, really more agreeing on a scale—like labeling a volume dial from 0 to 11. Except probabilities only go up to 1. (But check this out.) Second, if something must happen, then its probability is 1. If that sounds obvious, that’s kind of the point. This second law just establishes 1 as the top of the scale. (You could actually do things upside down and make 0 the “top” of the probability scale! But it’d be really counterintuitive to work that way.) Third and finally, if two possibilities are mutually exclusive—if they can’t both happen—then the chance one or the other will happen is the probability of the first plus the probability of the second. The chance a fair die will land either three or five is 1/6 + 1/6 = 2/6, for example. Amazingly, you can derive all kinds of interesting and sophisticated things about probability from just these three, simple rules. In fact, you can basically derive the whole mathematical theory! (You have to soup up the third law a bit for some of the more advanced results. It has to be extended to cases with infinite possibilities instead of just two. But it’s really just the same idea applied to a larger number of things.) But just as important is what these laws don’t say. For example, they don’t say heads and tails are equally likely when you have no information about a coin flip. They say the chance of heads is some number between 0 and 1, just as the chance of tails is. They also say the chances of heads and tails have to add up to 1. But that doesn’t mean they have to be equal. They could be 1/3 and 2/3, or 1/10 and 9/10, or even 0 and 1! Without the Principle of Indifference, the laws of probability are kind of empty. They tell you that if the chance of heads is 1/2, then the chance of tails has to be 1/2. But they don’t tell you the chance of heads is 1/2. It could be anywhere from 0 to 1 for all the textbooks say. That’s the price of abandoning the Principle of Indifference. "],
["the-problem-of-priors.html", "7.5 The Problem of Priors", " 7.5 The Problem of Priors So what probabilities should we start with when doing a scientific study or statistical analysis? This is known as the problem of priors. Recall Bayes’ theorem: \\[ Pr(H \\given E) = Pr(H) \\frac{Pr(E \\given H)}{Pr(E)}. \\] The quantity \\(Pr(H)\\) is called the prior probability of \\(H\\), because it’s the probability before we get information \\(E\\). Whereas \\(Pr(H \\given E)\\) is called the posterior probability, the probability of \\(H\\) after we get information \\(E\\). To calculate \\(Pr(H \\given E)\\) using Bayes’ theorem, you need prior probabilities like \\(Pr(H)\\) (and also \\(Pr(E)\\) and \\(Pr(E \\given H)\\)). There was a time when mathematicians would have happily relied on the Principle of Indifference to settle those prior probabilities. But thanks to Bertrand’s Paradox, the Principle of Indifference lost its respectability. And now there’s no accepted recipe for settling prior probabilities. "],
["grue-all-over-again.html", "7.6 Grue All Over Again", " 7.6 Grue All Over Again Bertrand’s paradox and the grue paradox from the previous chapter are suspiciously similar. For one thing, they are both problems of “language dependence”. If you grew up speaking and thinking in terms of grue and bleen, you’d apply the Principle of Induction very differently from the way you actually do. You’d think all emeralds are grue. And you’d expect the first emeralds mined in the New Year to be a different colour (specifically, blue). Similarly, the probabilities we get from the Principle of Indifference depend on the language in which we frame a problem like the cube factory. If we think about the size of the next cube in terms of length, we get a probability of 1/2. But if we think in terms of volume we get 7/26. But also, both paradoxes are manifestations of the problem of priors. They both challenge us to say, in a principled way, how likely each possibility is to start with. In the case of the grue paradox, we need some principled reason for saying it’s intrinsically more likely that all emeralds are green than that they’re all grue. If we can’t say why “all green” is inherently more plausible than “all grue”, then it doesn’t matter how many green emeralds we’ve observed up to now. Both hypothesis fit our observations perfectly: they both say all emeralds observed before 2018 will be green. To break the standoff, we need some criterion for favouring the green language over the grue one. "],
["where-this-leaves-us.html", "7.7 Where This Leaves Us", " 7.7 Where This Leaves Us Where does all this leave us? Not in a great place. The probabilities we start with determine the fate of science. If we start with sensible probabilities, we get sensible conclusions—like that all emeralds are green. Otherwise we get nonsense: all emeralds are grue. But Bertrand’s paradox overturned what we thought was the sensible way of establishing prior probabilities. The Principle of Indifference was supposed to tell us what priors to start with, until it ran smack into contradictions like the cube factory example. So the problem of priors is nasty indeed. And, I’m sorry to tell you, it has no accepted solution. There is a hacky sort of workaround, which is widely used across the sciences. And though it’s taught to every aspiring young scientist as the accepted method, the fact that it’s a hacky workaround is not usually mentioned. Probably because a lot of people don’t see it that way. So let’s see what the method is, and you can decide for yourself. "],
["lindleys-paradox.html", "8 Lindley’s Paradox", " 8 Lindley’s Paradox So how do scientists test their theories? How do they decide when an experiment supports a hypothesis, and when it refutes it? The short answer: they ask whether the results would be too much of a coincidence if the hypothesis were true. "],
["coincidence.html", "8.1 Coincidence", " 8.1 Coincidence Suppose the government starts minting a new, three dollar coin. We take one and flip it ten times. It lands heads every time. That would be a big coincidence if the coin were fair. Too much of a coincidence, in fact. The chances of getting ten out of ten heads with a fair coin are less than one in a thousand. So the hypothesis that the coin is fair has been tested, and it has failed that test. From our little experiment, we conclude that the coin is not fair, but biased. Another example: in a test of a new cancer drug, 90% of patients who take the drug are cured, compared to only 10% in the placebo group. That, again, would be a big coincidence if the drug were ineffective. Too much of a coincidence. So the hypothesis that the drug is ineffective has been tested, and it has failed the test. We conclude instead that the drug has beneficial effects. "],
["the-general-formula.html", "8.2 The General Formula", " 8.2 The General Formula Here’s the general formula for this method: State the hypothesis you want to test—that the coin is fair, that the drug is ineffective, etc. This is called the null hypothesis. (Because, traditionally, the default assumption is that the data are random, with no pattern.) Gather your data—flip the coin ten times, administer the drug to a group of patients, etc. Imagine the hypothesis is true, and consider how likely the data you’ve observed would be if it were true. If the answer is “very unlikely”, then reject the hypothesis. How unlikely is “very unlikely”, exactly? How much of a coincidence is too much of a coincidence? Traditionally the cutoff is 5% or less. (We’ll come back to the reason behind this tradition soon.) When the results fall in the range below the cutoff, they’re called statistically significant. And the method we’ve just described is called significance testing. "],
["what-about-bayes.html", "8.3 What About Bayes?", " 8.3 What About Bayes? You might be wondering what happened to Bayes’ theorem and Bertrand’s paradox here. Did we somehow just magically solve the problem of priors? No. We didn’t calculate \\(Pr(H \\given E)\\), instead we’ve been focusing on \\(Pr(E \\given H)\\). (Well, sort of. There’s a wrinkle here we’ll come back to later.) When \\(H\\) is a definite statistical hypothesis, you can calculate the probability of each outcome under the assumption that \\(H\\) is true. If our hypothesis is that a coin is fair, you can calculate the probability of getting ten out of ten tails in a row. Similarly, if our hypothesis is that each patient has only a 10% chance of being cured, we can calculate the probability that 90% of the patients in our test group will be cured. So we can calculate \\(Pr(E \\given H)\\) when \\(H\\) is an explicit and exact statistical hypothesis. The idea behind significance testing is to decide whether to reject \\(H\\) just on the basis of \\(Pr(E \\given H)\\). (Well, again, sort of: wrinkle.) If it’s too low for \\(H\\) to be believable, then we reject \\(H\\). "],
["normal-approximations.html", "8.4 Normal Approximations", " 8.4 Normal Approximations But how exactly do we calculate these probabilities? How do we determine when the results fall below the 5% cutoff? Computers do a lot of this work nowadays, especially in complicated cases. But there’s a shortcut for doing it by hand, at least in simple cases like coin tosses. And it gives you a much deeper understanding of significance testing, so let’s see how it goes. Suppose I flip a coin 100 times and it lands heads 75 of those times. Is that suspicious? Should we wonder whether it’s really fair? We need to figure out how unlikely it is to get 75 or more heads when you flip a fair coin 100 times. This would be tedious to do with perfect precision, but there’s a trick for getting a good approximation. If you flip a fair coin 100 times, the most likely outcome is 50 heads and 50 tails. A bit less likely is 51 heads, or 49 heads. A bit less likely still is 52 heads, or 48 heads. And so on. The overall pattern looks like this: Notice how much it resembles the famous “bell curve” shape: In fact they line up almost perfectly: So we can use a bell curve to approximate the probabilities we’re after. But how does that help? Because bell curves have some handy mathematical properties. "],
["characterizing-a-bell-curve.html", "8.5 Characterizing a Bell Curve", " 8.5 Characterizing a Bell Curve Fun fact: a bell curve is completely characterized by just two numbers. First is the center of the bell, called the mean, or \\(\\mu\\) for short. In our example \\(\\mu = 50\\). That’s the most likely outcome for \\(100\\) flips of a fair coin. The general formula is: \\[\\mu = np.\\] Here \\(n\\) is the number of tosses, \\(100\\) in our example. And \\(p\\) is the probability of heads on each toss, \\(1/2\\) in our example. So \\(\\mu = np = 100 \\times 1/2 = 50\\). Second is the width of the bell, called the standard deviation, or \\(\\sigma\\) for short. The general formula for \\(\\sigma\\) is a bit mysterious: \\[\\sigma = \\sqrt{np(1-p)}.\\] There’s a good-but-complicated mathematical reason behind this formula, so let’s just take it for granted. In our example \\(\\sigma = \\sqrt{np(1-p)} = \\sqrt{100 \\times 1/2 \\times 1/2} = 5.\\) Now that we know where the bell is centered (\\(\\mu = 50\\)), and how wide it is (\\(\\sigma = 5\\)), we can estimate how probable various outcomes are. How? Well, i’s a handy mathematical fact that, 95% of the time, the result of our coin flipping experiment will be within two “standard deviations” of the mean. In other words, we should expect the number of heads to be in the \\(\\mu \\pm 2 \\sigma\\) range 95% of the time, if the coin really is fair. In our example, \\(\\mu \\pm 2 \\sigma\\) is \\(50 \\pm 10\\), so the “95% of the the time” range is from 40 to 60 heads. But we got 75 heads. So that’s a pretty big coincidence if the coin really is fair, the kind of thing that would happen less than 5% of the time. So our result is statistically significant! The hypothesis that the coin is fair should be rejected, according to the method of significance testing. "],
["the-68-95-99-rule.html", "8.6 The 68-95-99 Rule", " 8.6 The 68-95-99 Rule In general, the math of bell curves works like this. If we go 5 heads out from 50, the probability is less than 68%. That’s one “standard deviation”, or \\(1 \\times \\sigma\\). If we go 10 heads out, which is two standard deviations (\\(2 \\times \\sigma\\)), the probability is less than 95%. And if we go 15 heads out, which is three standard deviations or \\(3 \\times \\sigma\\), the probability is less than 99%. So, in general, we can get a good sense of how surprising a result is by the following rule. If it falls within \\(\\sigma\\) of the mean \\(\\mu\\), it’s not surprising; that’s what you’d expect most of the time, about 68%. If it falls outside that range, it’s a little surprising; you’d expect that to happen sometimes, about a third of the time (100% – 68% = 32%). But if it falls outside that range, then it’s quite surprising. You’d expect that to happen rarely, only about 5% of the time (100% – 95% = 5%). And if it falls outside even that range, then it’s very surprising. You’d expect that to happen very rarely, less than 1% of the time. "],
["another-example.html", "8.7 Another Example", " 8.7 Another Example It takes practice to get the hang of all this, so let’s do another example. This one is from a book by Ian Hacking: VisioPerfect has a run of 2,400 light bulbs. According to the company’s publicity, 96% of the bulbs manufactured by its production process are long life… The monthly magazine Consumers’ Advocate asserts that it tested a run of 2,400 bulbs—400 six packs—from VisioPerfect. It found 133 short-life bulbs. (Hacking, p. 205) Is this a statistically significant result? Should we reject the claims of VisioPerfect’s publicists? To find out, we start by calculating \\(\\mu\\) and \\(\\sigma\\). And for that we need \\(n\\) and \\(p\\). In this example \\(n = 2,400\\): that’s the size of our sample of bulbs. And \\(p = .04\\): that’s the chance of getting a short-life bulb according to the hypothesis we’re testing. So using the formula from earlier, \\(\\mu = np = 2,400 \\times .04 = 96\\). And \\(\\sigma = \\sqrt{np(1-p)} = \\sqrt{2,400 \\times .04 \\times .96} = 9.6\\). Now, we observed 133 short-life bulbs. And according to the 68-95-99 rule, that’s a statistically significant result if 133 is more than two standard deviations away from the mean. In other words, if it’s outside the \\(\\mu \\pm 2 \\sigma\\) range. And in this case \\(\\mu + 2\\sigma = 115.2 &lt; 133\\), so the result is significant. According to the method of significance testing then, we should reject VisioPerfect’s advertised claim. "],
["warning-danger-ahead.html", "8.8 Warning: Danger Ahead", " 8.8 Warning: Danger Ahead Significance testing is confusing stuff. Trained scientists, and even statisticians, often misunderstand it. And they misapply it in real studies as a result. In fact, the American Statistical Association recently released a statement to clarify the idea and prevent its misuse. So what does it really mean for a result to be “statistically significant”? It means that if the null hypothesis is true, then a result this unusual was less than 5% likely to occur. But that doesn’t mean the chance the null hypothesis is true is now less than 5%! Making that leap would confuse \\(Pr(E \\given H)\\) with \\(Pr(H \\given E)\\). It also doesn’t mean the true hypothesis is significantly different from the null hypothesis. Imagine a coin that’s just slightly biased towards heads—maybe it comes up heads 51% of the time. With enough flips, you could establish that the coin isn’t fair, with statistical significance. But that doesn’t mean it’s significantly biased. It’s barely biased at all: 51% rather than 50%. The “significance” terminology isn’t great—it may even be worse than “null hypothesis”. But let’s try not to get hung up on preferred nomenclature. It’s the underlying logic of significance testing that concerns us here. "],
["significance-subjectivity.html", "8.9 Significance &amp; Subjectivity", " 8.9 Significance &amp; Subjectivity Let’s tackle a big question we’ve been avoiding: why is the cutoff 5%? Nothing magical or special happens when a result crosses that line. It’s just easy to calculate using the bell curve trick we just learned. In fact, although 5% is the cutoff commonly used in social sciences like psychology and sociology, different areas of research have different customs. A cutoff of 1%, or even 0.1%, is common in some physical sciences. Come to think of it, does the idea of a fixed cut off make sense at all? Sometimes a result is really far out there—the kind of thing you’d happen to expect less than one time in a thousand—and yet the null hypothesis shouldn’t be rejected. If you’re playing poker at a major casino and you’re dealt a straight, that doesn’t mean the deck is stacked in your favour. You know casinos are tightly regulated; they use carefully calibrated automatic shufflers; and they want you to lose, not win! Some statisticians, called “Bayesians”, see the whole significance testing method as misguided. When we calculate significance levels, we’re basically calculating \\(Pr(E \\given H)\\): the probability of the result if the hypothesis is true. But Bayes’ theorem tells us this is just part of the information we need to calculate what we really want, namely \\(Pr(H \\given E)\\). We want to know how plausible the hypothesis is given our results. But for that, we need \\(Pr(H)\\) and \\(Pr(E)\\): \\[ Pr(H \\given E) = Pr(H) \\frac{Pr(E \\given H)}{Pr(E)}. \\] Take the casino example. The null hypothesis \\(H\\) is that the cards are being dealt at random. In this kind of case, \\(Pr(H)\\) is very high: major casinos use standard decks and automated shuffling machines, and they’re very tightly regulated. That’s why, even though \\(Pr(E \\given H)\\) is low—it’s very unlikely to get a straight when cards are being dealt at random—\\(Pr(H \\given E)\\) is still high. If you’re basically certain the dealing is fair, you’ll still be confident it’s fair even if you’re dealt a straight on your first hand. So Bayesian statisticians see significance testing as a crude way of approximating Bayes’ theorem. Significance testing only looks objective on the surface, they say, because we’re pretending \\(Pr(E \\given H)\\) is the only term in Bayes’ theorem that matters. But other terms like \\(Pr(H)\\) do matter, as the casino example shows. So the choice of cutoff is subjective, based on a personal judgment about \\(Pr(H)\\). This criticism is sharpened by a famous problem known as Lindley’s paradox. "],
["lindleys-paradox-1.html", "8.10 Lindley’s Paradox", " 8.10 Lindley’s Paradox Imagine you run a flower store, and you receive a large shipment of tulip bulbs. The supplier only sends two kinds of shipments: Type A: 25% of the bulbs grow red tulips, 75% yellow. Type B: 50% of the bulbs grow red tulips, 50% yellow. But the label on this shipment has been scratched off. So, to determine which type it is, you select 48 random bulbs and plant them to see what colour they grow. 36 grow red, 12 yellow. If it were me, I’d think this clearly shows the shipment is Type B, with 50% red bulbs. Even though your sample grew more than 50% red tulips, this would be even less likely if the shipment were Type A, with only 25% red bulbs. So the results of your experiment fit the Type B hypothesis much better than the Type A hypothesis. What happens if we apply significance testing, though? We’ll end up rejecting both hypotheses! Because the results don’t fit either hypothesis particularly well, even though they fit the Type B hypothesis better than Type A. Let’s test the Type B hypothesis to see how this works out. You sampled 48 bulbs, so \\(n = 48\\). And according to the Type B hypothesis, each bulb has a 50% chance of growing red, so \\(p = .5\\). Applying our formulas for \\(\\mu\\) and \\(\\sigma\\) from earlier, we get: \\[ \\begin{aligned} \\mu &amp;= 25,\\\\ \\sigma &amp;\\approx 3.5. \\end{aligned} \\] So, according to the Type B hypothesis, 95% of the time we’d expect the number of red tulips in a sample of 48 to be between 18 and 32 (that’s \\(25 \\pm 2 \\times 3.5\\)). Yet our actual result was 36, which is outside that range. (In fact it’s even outside the 99% range.) If you do the calculations for the Type A hypothesis, you’ll find a parallel result. The observed 36 red tulips is statistically significant, so we should reject that hypothesis too. But that means we would reject both hypotheses. Yet we know they’re the only two possibilities! Something seems to have gone wrong. "],
["a-bayesian-analysis.html", "8.11 A Bayesian Analysis", " 8.11 A Bayesian Analysis Bayesian critics say the problem is that we ignored the prior probabilities. If we apply Bayes’ theorem, which takes account of all the information—including the fact that the only two possibilities are Type A and B—we get the sensible result. We’ll conclude that the Type B hypothesis is true. For example, if we treat the Type A and Type B possibilities as equally plausible, we can assign \\(Pr(H) = 1/2\\). Bayes’ theorem then tells us that \\(Pr(H \\given E) = .999999998\\) for the Type B hypothesis, compared to \\(.000000002\\) for Type A. (The details of this calculation are pretty hairy, so I had my computer do it.) Now, we know from previous chapters on Bertrand’s paradox and the grue paradox that there are problems lurking here. By assuming \\(Pr(H) = 1/2\\) in this calculation, we’re essentially relying on the Principle of Indifference. And that way lies paradox. But, the Bayesians say, Bayes’ theorem will give similar results even if we set \\(Pr(H) = 1/3\\) for the Type B hypothesis, or \\(Pr(H) = 1/4\\). Bayes’ theorem will always say the probability of this hypothesis goes up given the results of the experiment, because the results fit the Type B hypothesis better. So, the say, we don’t have to worry too much about what the correct value of \\(Pr(H)\\) is. "],
["the-standoff.html", "8.12 The Standoff", " 8.12 The Standoff Trouble is, we need to know how much the Type B hypothesis goes up. Does it go up to a number that’s basically 1, like .999999998? Or does it go up from a number that’s basically 0 to a slightly larger number that’s still close to 0, like .000000001? What we should believe about \\(H\\) depends on the value of \\(Pr(H \\given E)\\). Is it close to 1, close to 0, or somewhere in between? And that depends on what number we start with for \\(Pr(H)\\). So there just doesn’t seem to be any getting around the problem of priors. On the other hand, significance testing seems to just be a way of pretending the problem doesn’t exist. It may work well if it’s used carefully and judiciously. But, critics say, it often isn’t used that way. And even when it is used that way, that’s just Bayes’ theorem in disguise—making a rough guess about what \\(Pr(H)\\) should be, yet pretending you’ve done no such thing. "],
["decisions-decisions.html", "9 Decisions, Decisions", " 9 Decisions, Decisions We’ve been talking a lot about what to believe, not so much what to do. But “probability is the very guide of life”, as Bishop Butler famously wrote in 1736. What career should you pursue? Which people should you befriend and which should you avoid? Which foods should you eat? Such questions depend on the likely outcomes of your choice. But depend how? Is there a simple formula for making decisions? There is. It was discovered over 350 years ago, by a French mathematician and philosopher named Blaise Pascal, in order to help his friend with a gambling problem. "],
["goods-and-odds.html", "9.1 Goods and Odds", " 9.1 Goods and Odds Imagine you’re at a casino and you’ve been given a voucher for one free game of dice. The game is simple. The house will roll a six-sided die once, and you must choose one of the following bets: Bet A: if the die lands on an even number you win $2. Bet B: if the die lands on a high number (5 or 6) you win $3. Which bet should you choose? The first has better odds, a one-half chance of winning compared to one-in-three. But the second has better rewards: $3 rather than $2. How to weigh these considerations against one another? That’s the question decision theory tries to answer. A choice can be better or worse in two ways. It can have better odds, or it can have better “goods”—better payouts in our casino example. Our goal is to find a formula that combines odds and goods to reach a decision. "],
["the-long-run.html", "9.2 The Long Run", " 9.2 The Long Run Well, consider what would happen if you were to play all night, choosing the same bet over and over again. If you chose Bet A, you’d win $2 half the time. If you played a thousand games, you’d expect to win about 500 of them. At $2 a pop, that’s $1,000 over 1,000 games, an average of $1 per game. This is called the expected monetary value of the bet. It’s not the amount you’d expect to win playing just once, notice. You’ll either win $2 or $0. But in the long run, you’d expect to win $1 on average. In decision theory we write \\(EMV(A) = \\$1\\): the expected monetary value of Bet A is $1. What about Bet B? If you chose it over and over again, you’d win $3 a third of the time. If you played 900 games, you’d expect to win about 300 of them. At $3 a pop, that’s $900 over 900 games, an average of $1 per game again. So \\(EMV(B) = \\$1\\), the same as Bet A! If EMV is our guide to the best choice, then these two bets are equally good. Either one would be rational choice. But what’s the underlying formula here? Let’s make it explicit. "],
["the-emv-formula.html", "9.3 The EMV Formula", " 9.3 The EMV Formula Each choice has multiple possible outcomes. If you take Bet A for example, you could win $2 or you could win nothing. What we did in thinking about the long run was essentially to multiply the probability of each outcome against the payout. Half the time you’ll win $2 in the long run, so we multiply \\(1/2 \\times \\$2\\). And half the time you’ll win $0, so we multiply \\(1/2 \\times \\$0\\). Then we combine those products by adding them together: \\[ \\begin{aligned} EMV(A) &amp;= 1/2 \\times \\$2 + 1/2 \\times \\$0\\\\ &amp;= \\$1. \\end{aligned} \\] Likewise for Bet B: a third of the time you’ll win $3, so we multiply \\(1/3 \\times \\$3\\). And two thirds of the time you’ll win nothing so we multiply \\(2/3 \\times \\$0\\). Then we add up: \\[ \\begin{aligned} EMV(B) &amp;= 1/3 \\times \\$3 + 2/3 \\times \\$0\\\\ &amp;= \\$1. \\end{aligned} \\] There won’t always be just two possible outcomes, of course. Sometimes a choice will have three, or four, or any number of possible outcomes \\(n\\). The general formula labels the possible outcomes \\(O_1, O_2, \\ldots, O_n\\), and calculates the EMV of a choice C thus: \\[ EMV(C) = Pr(O_1) \\times \\$O_1 + Pr(O_2) \\times \\$O_2 + \\ldots + Pr(O_n) \\times \\$O_n. \\] "],
["utility.html", "9.4 Utility", " 9.4 Utility Now, a lot of people would choose Bet A over Bet B, I suspect. I know I would. Because I don’t much care whether I win $2 or $3, I care more about the thrill of winning anything at all. (Why else would I bet at a casino? Not to make money, that’s for sure!) So money isn’t everything, there’s also the fun of winning a game. And of course there are more needs too, like food, sleep, love, friendship, and (at least for me) Wi-Fi. And then there are the less fundamental goods we value, like creature comforts, cushy vacations, gadgets and conveniences, and so on. To extend our decision formula to cover all these variegated goods, we need to place them all on a single, numerical scale. This scale is called utility. It’s a numerical measure of how valuable or desirable an outcome is to the person making the decision. In one way, utility is extremely subjective. Tastes differ, so what has high utility and what has low utility differs from person to person. A trip to New York City has high utility for many people, but not for me (too crowded!). In another way though, utility is an objective matter. It is an objective fact about me that a trip to New York City has low utility for me, indeed, it’s an objective fact you can easily know. You can take my word for it, for example. But even if you were still skeptical, you could offer me a trip to New York and see how much I’d be willing to pay for it. (Answer: you’d have to pay me. A lot.) "],
["quantifying-utility.html", "9.5 Quantifying Utility", " 9.5 Quantifying Utility Still, you’re probably wondering how we can quantify people’s personal priorities and tastes. How can we pin a number on the degree to which someone likes or dislikes a trip to New York? We need actual numbers if we’re going to replace the dollar amounts in the EMV formula with utilities! The trick to conjuring up these numbers was first discovered by the philosopher Frank Ramsey in the 1920’s. Sadly, Ramsey died unexpectedly at the age of 26, and the idea was lost until the 1940’s when it was rediscovered independently. The polymath John von Neumann (also a godfather of computer science) and economist Oskar Morgenstern published a classic book in 1944, Theory of Games and Economic Behavior, which still defines the fields of economics and decision theory to this day. So what’s the big idea? The core of it is just this: the more utility an outcome has for you, the less risk you’ll be willing to accept in trading it for a shot at something better. Imagine your top choice for a vacation spot is Barcelona, and your bottom choice is Kiev. New York is somewhere in between. Now imagine further that you’re holding a ticket to New York, and I hold one ticket to Barcelona and one to Kiev. I offer to gamble in exchange for your New York ticket: we’ll roll a six-sided die, and if it lands either 4, 5, or 6, you get my Barcelona ticket. Otherwise you get the Kiev ticket. Would you take this deal? What if you had to roll a 5 or higher to get the Barcelona ticket? What if you had to roll a 6?? Suppose you’re only willing to trade your New York ticket for this gamble if a 4 or better wins the Barcelona ticket. Then you must place a good amount of value on that New York ticket. You’re not willing to part with it unless you have a decent shot at Barcelona over Kiev. But if you’re even willing to make the trade when a roll of 6 is required to win the Barcelona ticket, then you must not like New York that much. You’re willing to part with it in exchange for a long shot at Barcelona, even though you’ll probably end up going to Kiev. In that case, a trip to New York can’t be all that much better than a trip to Kiev in your eyes. Using this basic idea, we can actually place a precise number on the utility a trip to New York has for you. Since Barcelona is your top choice, we’ll say it has utility 1. (Just like with probability, we use 1 as the top of the scale for simplicity. We could use a different number without changing anything of importance.) And since Kiev is your bottom choice, we’ll stipulate it has utility 0. On this 0-to-1 utility scale, where does New York place? If you aren’t willing to trade your New York ticket for the Barcelona/Kiev gamble unless the chance of winning is at least 1/2, then it’s half way up that scale. A trip to New York has utility 1/2 for you. But if you’d be willing to make the trade when the chances are as low as 1/3, then it’s only a third of the way up that scale. The utility of a trip to New York is 1/3 for you then. The general recipe is this. To locate outcome \\(O\\) on a scale from the best option \\(B\\) to the worst option \\(W\\), we set \\(U(B) = 1\\) and \\(U(W) = 0\\). Then we offer to trade \\(O\\) for a gamble with probability \\(p\\) of outcome \\(W\\), and probability \\(1-p\\) of outcome \\(B\\). The lowest value \\(p\\) can take, with you still willing to make the trade, gives us is the utility of \\(O\\): \\(U(O)\\). "],
["expected-utility.html", "9.6 Expected Utility", " 9.6 Expected Utility With utilities precisely quantified, we can extend our decision formula to non-monetary decisions. Recall, the EMV of a choice \\(C\\) was: \\[ EMV(C) = Pr(O_1) \\times \\$O_1 + Pr(O_2) \\times \\$O_2 + \\ldots + Pr(O_n) \\times \\$O_n. \\] So we just replace dollars with “utiles” to define expected utility, or \\(EU\\): \\[ EU(C) = Pr(O_1) \\times U(O_1) + Pr(O_2) \\times U(O_2) + \\ldots + Pr(O_n) \\times U(O_n). \\] The fundamental rule of decision theory is to choose the option with the highest expected utility. (If multiple options have maximal expected utility, then any one of them is a reasonable choice.) But why this formula? Why should we choose options that maximize expected utility, instead of some other formula? It’s a good question. Following this rule looks pretty sensible in a wide variety of circumstances, and so it’s by far the most widely endorsed rule. But sometimes it gives strange advice, and so some people think it should be revised. That’s part of what we’ll be looking at in the next few chapters. "],
["the-st-petersburg-paradox.html", "10 The St. Petersburg Paradox", " 10 The St. Petersburg Paradox In 1713 a mathematician named Nicolaus Bernoulli contemplated the following game: I’m going to flip a fair coin as many times as needed until it lands heads. When it does land heads, I’m going to stop and pay you a dollar amount according to the following scheme: If the heads happens on the first toss, you get $2. If the heads happens on the second toss, you get $4. If the heads happens on the third toss, you get $8. Etc. The general pattern here is that you get \\(\\$(2 \\times 2 \\times \\ldots) = \\$2^n\\), where \\(n\\) is the number of tosses. If it takes 3 tosses to get a heads, then you get \\(\\$2^3 = \\$8\\). If it takes 10 tosses, you get \\(\\$2^{10} = \\$1,024\\). And so on. This might seem uncharacteristically generous of me, and in truth it is. I’m actually going to charge you a fee up front to get in on this game. The question is, how much are you willing to pay? Probably not very much. After all, you’ll probably only win a few bucks. The probability of winning at most $8 is 7/8, for example, or 87.5%. And the probability is about 98% that you’ll win less than $100. What’s the EMV though? Well there’s a 1/2 chance of winning $2, a 1/4 chance of winning $4, a 1/8 chance of winning $8… so: \\[ \\begin{aligned} EMV &amp;= 1/2 \\times \\$2 + 1/4 \\times \\$4 + \\ldots + 1/2^n \\times \\$2^n + \\ldots\\\\ &amp;= \\$1 + \\$1 + \\ldots + \\$1 + \\ldots\\\\ &amp;= \\infty. \\end{aligned} \\] Now here’s the paradox. A game with infinite expected value is one you should be willing to pay any price to pay. Because infinity minus any finite number is still infinity. Whether I demand $10, $100, or $1,000,000 to play, the game still has infinite expected value. And yet, intuitively, the game isn’t worth more than a few dollars. "],
["bernoullis-solution.html", "10.1 Bernoulli’s Solution", " 10.1 Bernoulli’s Solution Nicholas Bernoulli devised this puzzle to challenge the idea that we should use expected value to determine the best decision. But the puzzle gets its name from his cousin Daniel Bernoulli, who published a famous response in the St. Petersburg Academy Proceedings in 1738. Daniel’s answer to his cousin’s puzzle was based on an insight of yet another mathematician, Gabriel Cramer. The value of a dollar, Cramer realized, depends on how many dollars you heavy already. To someone destitute, a dollar can make the difference between eating lunch and going hungry. But to a billionaire, one more dollar makes no noticeable difference. Or consider how a million dollars might change your life, compared to (say) Mark Zuckerberg’s. With a million dollars you might quit your job, or change careers entirely, or move from a small rented apartment to a large house. But for Mark Zuckerberg, well… I suspect there isn’t much he could do with that extra million that he can’t do already. So, Bernoulli concluded, while the dollar amounts may double and double in the St. Petersburg game, the utilities do not. Because winning $4 is not twice as valuable as winning $2; winning $8 is not twice as good as winning $2; and so on. As the dollar amounts double, the utility to be had does increase, but by less and less each time. Exactly how much better is $4 than $2 then? And $8 over $4? Bernoulli argued that as dollar rewards go up, utility increases “logarithmically”. In visual terms: More precisely, Bernoulli’s idea was that the utility of \\(\\$x\\) is given by the logarithm function, \\(\\ln\\). So \\(U(\\$x) = \\ln(\\$x)\\). Bernoulli then demonstrated that the expected utility of the St. Petersburg game—as opposed to its expected monetary value—ends up being finite. More exactly, it ends up being \\(\\ln(4)\\), the equivalent in utiles of about $4. Which lines up pretty well with what most people are willing pay to play! "],
["st-petersburgs-revenge.html", "10.2 St. Petersburg’s Revenge", " 10.2 St. Petersburg’s Revenge Sadly, Bernoulli’s solution only offers short-lived relief. The paradox bounces right back. We just have to change the rewards so that they double in utiles instead of dollars. Imagine the game now works as follows: If the heads happens on the first toss, you get 2 utiles. If the heads happens on the second toss, you get 4 utiles. If the heads happens on the third toss, you get 8 utiles. Etc. The exact same calculation we did with money now shows that this game has infinite expected utility. So you should be willing to sacrifice anything to play. Hmmm… can we always double the utility of the rewards though? Is there no limit on how much good the game can deliver? Not if we take Bernoulli’s solution seriously. There is no upper bound on the logarithm function, it just keeps increasing forever and ever. The increases do get smaller and smaller as the number of dollars increases, as the graph suggests. But they are still limitless. If you go far enough out to the right, you will eventually double the amount of utility you started from. There’s even a simple equation for figuring out how many more dollars you need to double the utility. (For the mathematically inclined, the dollar payouts go like this: \\(\\$e^2, \\$e^4, \\$e^6\\), etc.) On the other hand, there’s only so much money in the world. Even if the government agreed to create as much money as needed to cover the game, there would only be so much you could do with it. At some point, there just won’t be any more material goods to buy. And there’s only so much you can pay other people to do to make your life—or the world as a whole—better. "],
["god-comes-to-st-petersburg.html", "10.3 God Comes to St. Petersburg", " 10.3 God Comes to St. Petersburg Still, these limitations don’t seem like the right kind. Our world may be imperfect. And our potential may be bounded by our physical and social natures. But shouldn’t the laws of reason transcend those contingencies? Shouldn’t the rational decision still be rational even if our lives had infinite potential? Imagine God appears to you and offers to play the St. Petersburg game. The rewards now are days in paradise. And with God’s limitless powers, there is no limit to the amount of good you can enjoy there: you will never get bored of heaven’s rewards. And no more of this logarithm business! Each day in heaven adds as much good to your existence as the last. God will see to that. Now the St. Petersburg game really does have infinite expected utility. But God also demands a price to play, to be exacted in days in hell. To play the divine St. Petersburg game, you must first spend a thousand years with the damned. Each day in hell is an agony, but a finite one (we may suppose). So God’s price is finite. But it is incredibly steep. And most likely that millennium of suffering will only be rewarded with handful of days in heaven. Your chances of winning more than 64 days in heaven, for example, are less than 2%. Will you play God’s St. Petersburg game? Even if I were convinced it was the rational choice, I don’t think I could bring myself to do it. "],
["you-bet-your-afterlife.html", "11 You Bet Your Afterlife", " 11 You Bet Your Afterlife The modern theories of probability and decision were born to solve gambling problems. A prominent gambler in 17th Century France, the Chevalier de Méré was also an amateur mathematician, and friend to one of the great mathematicians of his time, Blaise Pascal. The Chevalier’s interest in gambling prompted Pascal to develop the fundamentals of probability and expected utility (with the help of another great mathematician, Pierre de Fermat). But Pascal himself was a deeply religious Catholic. For him, the most interesting gamble didn’t involve money, but something more ultimate: the afterlife. "],
["pascals-wager.html", "11.1 Pascal’s Wager", " 11.1 Pascal’s Wager According to Catholicism, God punishes nonbelievers with an eternity of damnation. But believers spend eternity in paradise. So, Pascal reckoned, the smart bet is to believe in God. If you’re a nonbeliever, you probably aren’t impressed by Pascal’s reasoning thus far. You’ve probably considered it before in fact, and dismissed it as incomplete. After all, there are benefits to being a nonbeliever, like sleeping in on Sundays. Plus there’s the intellectual good of having truth on your side. If you’re right that there is no God, well, then it’s better to remain skeptical than to be taken in. Maybe you also count reason on your side. The evidence and arguments favour skepticism about God, according to nonbelievers. And isn’t it better to be rational and believe only what the evidence supports? Come to think of it, if the evidence and arguments against God’s existence weigh heavily enough, you’re much more likely to live a well-rested life of truth and reason than to end up in hell. And isn’t the whole idea of expected utility that we should weigh potential consequences according to their chances of actually occurring? This is where the sophistication of Pascal’s analysis takes hold. There is a crucial difference in the rewards facing believers and nonbelievers. If the believers are right, their reward is infinitely good, an eternity in paradise. Whereas if the nonbelievers are right, the rewards are finite—a bit of extra sleep and some intellectual bragging rights. So the true decision table looks like this, according to Pascal: God No God Believe ∞ –100 Don’t Believe –∞ 100 Where did I get 100 from? I made it up. Because, it turns out, it doesn’t matter how big the finite rewards are when weighed against infinite ones. To see why, let’s calculate the expected utilities of your two options: \\[ \\begin{aligned} EU(\\mbox{Believe}) &amp;= Pr(\\mbox{God}) \\cdot \\infty + Pr(\\mbox{No God}) \\cdot -100\\\\ &amp;= \\infty - \\mbox{something finite}\\\\ &amp;= \\infty.\\\\ EU(\\mbox{Don&#39;t Believe}) &amp;= Pr(G) \\cdot -\\infty + Pr(\\neg G) \\cdot 100\\\\ &amp;= -\\infty + \\mbox{something finite}\\\\ &amp;= -\\infty. \\end{aligned} \\] If this calculation is right, then believing isn’t just the smart bet. It’s actually infinitely better than non-believing! "],
["infinite-arithmetic.html", "11.2 Infinite Arithmetic", " 11.2 Infinite Arithmetic But is the calculation right? Some of the steps are mysterious since we don’t usually do multiplication and addition/subtraction with infinite quantities. Two special features of infinity lie behind these steps, so let’s think them through. First, infinity times any positive number is infinity. That’s why \\(Pr(\\mbox{God}) \\cdot \\infty = \\infty\\), for example. But why? Because if you keep adding a positive number to itself, there’s no limit to how big the result can get. If you add 1/10 + 1/10 + 1/10 + …, you’ll eventually pass 1, and then 2, and then 3, and eventually every finite number. Likewise if you add 1/100 to itself infinitely many times. Or any positive number, no matter how small. Second, infinity minus any finite number is always infinite. That’s why \\(\\infty - \\mbox{something finite} = \\infty\\). Why? Because if you start with an infinite set of objects and remove finitely many of them, you’ll always have an infinite number of objects remaining. Imagine you have an infinite stack of $5 bills, for example, with serial numbers #1, #2, #3, etc. If I take the first ten bills, you still have an infinite fortune left over: bills #11, #12, #13, etc. In fact, no matter how many bills I take from your stack, as long as it’s a finite number, you’ll have an infinite fortune remaining. Whatever the highest numbered bill I take—let’s call it bill #\\(n\\)—you still have bills #(\\(n+1\\)), #(\\(n+2\\)), #(\\(n+3\\)), etc. "],
["choosing-what-you-believe.html", "11.3 Choosing What You Believe", " 11.3 Choosing What You Believe If I offered you a hundred dollars to believe you’re standing on the surface of the sun right now, you couldn’t do it. You might try hard. You might even put on a good show in the hopes of convincing me you’ve earned the money. But you couldn’t really believe you’re standing on the sun. The evidence of your eyes—of all your sensory organs—just won’t let you. And isn’t the nonbeliever in the same position? Even if Pascal is right that believing in God is the smart bet, the evidence and arguments still make plain that God does not exist. At least, that’s how nonbelievers see it. God’s absence is apparent everywhere, evident in every imperfection of our world—in the needless suffering of starving children, the torment of factory farmed animals, etc. Pascal understood that believing isn’t something we just decide to do. But, he pointed out, we do many things that influence what we will believe down the line. If you choose to spend your time with religious people, reading religious books, and attending religious services, then you are apt to wind up a believer. At the very least, you are more likely to be receptive to religious arguments and evidence if you immerse yourself in a religious lifestyle. And with all that’s at stake, shouldn’t you do whatever you can to increase your chances of redemption? "],
["the-many-gods-problem.html", "11.4 The Many Gods Problem", " 11.4 The Many Gods Problem One thing Pascal doesn’t seem to have accounted for, however, is the variety of religions in the world. Besides the God of Catholicism there is also the God of Islam, of Judaism, of Sikhism, of Zoroastrianism, etc. Not to mention the many polytheistic religions! Each religion has its own theory of the afterlife, and its own scheme of rewards for believers, and punishments for nonbelievers. But the true God might even be none of these. Maybe the true God is bashful, and rewards atheists with eternity in paradise while punishing believers, of all creeds, with eternity in hell! Such possibilities may be farfetched, but Pascal’s whole argument hinges on the fact that it doesn’t matter how unlikely God’s existence is. The whole point was that it doesn’t matter how small \\(Pr(\\mbox{God})\\) is. As long as it’s not zero, it becomes infinite when weighed against the infinite reward of heaven. So we have to take all possible Gods into account. How? By expanding our table. We need a column for each possible religion/god. And we need a row for each faith we might adopt: Catholicism Judaism Islam … Bashful No God Believe Catholicism ∞ –∞ –∞ … –∞ –100 Believe Judaism –∞ ∞ –∞ … –∞ –100 Believe Islam –∞ ∞ –∞ … –∞ –100 ⋮ Don’t Believe –∞ –∞ –∞ … ∞ 100 We could quibble about certain details here. For example, Judaism’s conception of the afterlife is pretty different from Catholicism’s. There’s actually a maximum eleven-month sentence for the damned, according to one tradition! So Catholics may not face a consequence of –∞ under the Judaism column. But let’s skip straight to the punch line: regardless of these details, there’s no way to decide now using expected utility. Because the calculation requires subtracting infinity from infinity, and no such mathematical operation is well-defined. Suppose we try to calculate the expected utility of believing in Catholicism, for example. We’re going to have \\(Pr(\\mbox{Catholicism}) \\cdot \\infty\\) in the mix, which is equal to \\(\\infty\\) as we saw earlier. But we’re also going to have \\(Pr(\\mbox{Bashful}) \\cdot -\\infty\\) in there, which equals \\(-\\infty\\). So at some point we’re going to have to deal with \\(\\infty - \\infty\\). But why is that a problem? "],
["infinite-differences.html", "11.5 Infinite Differences", " 11.5 Infinite Differences Why is \\(\\infty - \\infty\\) not a well-defined mathematical expression? Because there’s no unique answer to the question: how many things are left over when you remove infinitely many objects from an infinite set. Recall our infinite stack of bills, #1, #2, #3, etc. Now suppose I take away all of them, an infinite quantity. How many are left over? Well, none, obviously: I took them all away. So \\(\\infty - \\infty = 0\\), right? Not so fast. Imagine instead I take away just the even-numbered bills: #2, #4, #6, etc. That’s an infinite quantity of bills, too. But now you have an infinite number of bills remaining! You have all the odd-numbered bills: #1, #3, #5, and so on forever. So… maybe \\(\\infty - \\infty = \\infty\\)? In fact, we can make \\(\\infty - \\infty\\) equal any number. Suppose I take away every bill numbered #2 and higher. Then you have one bill left, #1. Or I could take away every bill numbered #3 and higher, so you’d have two bills left, #1 and #2. And so on. So there’s no unique answer to the question: how many things are left behind when you remove an infinite number of objects from an infinite set? And so there’s no well-defined operation \\(\\infty - \\infty\\). "],
["conclusion.html", "11.6 Conclusion", " 11.6 Conclusion Where does all this leave us? Pascal’s argument depended on God dolling out infinite rewards. Otherwise the probability of God’s existence might be low enough that the benefits of a nonreligious life outweigh the threat of damnation. But once we account for the many Gods there might be, the infinite rewards and punishments render the whole idea of expected utility useless. So maybe it was a mistake to treat heaven an hell as infinitely good/bad outcomes? Maybe. But then we can’t evaluate Pascal’s Wager without pinning down just how good/bad the consequences of belief nonbelief are. And how likely/unlikely the existence of each kind of god is. Apparently there is no shortcut around the hard work of assessing the case for God’s existence—or the Gods’ existence. "],
["the-human-element.html", "12 The Human Element", " 12 The Human Element Three hundred years after Pascal and &amp; co., another French mathematician devised a famous challenge for the idea of expected utility. In the years following World War II, the idea that human decisions could be boiled down to a single equation was taking hold in the United States. I mentioned the classic 1944 book by John von Neumann and Oskar Morgenstern back in Chapter 9. Well, ten years later another classic was born in The Foundations of Statistics, written by von Neumann’s former assistant, Leonard Savage. Both books derived the expected utility rule from first principles, establishing it as the gold standard in decision making. But some French mathematicians disliked this approach. As Ian Hacking writes, they “thought that there was something mechanical, unthinking, and terribly ‘American’ about having a blind rule to compute your free choices.” And in 1953, Maurice Allais developed a famous puzzle to demonstrate. "],
["the-allais-paradox.html", "12.1 The Allais Paradox", " 12.1 The Allais Paradox Suppose I offer you a choice: you can have a million dollars with no strings attached, or you can gamble. The gamble has an 89% chance of delivering a million dollars, a 10% chance of delivering five million, and a 1% chance of delivering nothing. Which will you choose, the guaranteed million or the gamble with a shot at five? Most people favour the sure million. A million dollars would reshape their whole life. And even though five million dollars would make an even bigger difference, the difference between one million and five million isn’t enough for them to risk walking away from this opportunity empty handed. Even though the risk would only be a 1% chance, they’d rather just take the safe million. Now imagine a different choice. This time both options are gambles, with very similar odds. The main difference is in the potential payoffs. The first gamble has an 89% chance of paying nothing and an 11% chance of winning a million. The second gamble has a 90% chance of paying nothing and a 10% chance of paying five million. Which will you choose, the slightly safer shot at a million or the slightly riskier shot at five million? Most people favour the second option, the slightly riskier shot at five million. There is no sure thing now, in fact whichever way you go you’ll probably walk away empty handed. And if you’re just willing to accept a small, 1% increase in the chance of leaving empty handed, you can have a shot at five million instead of one million. For most people, that exchange seems worth it. So what? Well, in fact most people have both these preferences: they’d rather have the safe million in the first choice, but take the risk at five million in the second. But this violates the law of expected utility. "],
["allais-eu-and-you.html", "12.2 Allais, EU, and You", " 12.2 Allais, EU, and You How can I say people are violating the expected utility rule here if I don’t know their personal utilities? Isn’t it possible that, for these people, the difference in value between getting nothing and getting a million dollars makes sense of these preferences? Nope. And that’s part of the beauty and ingenuity of Allais’ paradox. No matter what a person’s utilities are for $0, $1M, and $5M, the popular choices violate the expected utility rule. In fact it only takes a few lines of arithmetic to see why. First let’s labels the options here: 1A 1B 2A 2B $1M 89%: $1M 10%: $5M 1%: $0 89%: $0 11%: $1M 90%: $0 10%: $5M Most people choose 1A over 1B and 2B over 2A, so we’re considering the possibility that \\(EU(1A) &gt; EU(1B)\\) and \\(EU(2A) &gt; EU(2B)\\). Let’s write out the expected utility formula for each option: \\[ \\begin{aligned} EU(1A) &amp;= U(\\$1M),\\\\ EU(1B) &amp;= .89 \\cdot U(\\$1M) + .1 \\cdot U(\\$5M) + .01 \\cdot U(\\$0M),\\\\ EU(2A) &amp;= .89 \\cdot U(\\$0M) + .11 \\cdot U(\\$1M),\\\\ EU(2B) &amp;= .9 \\cdot U(\\$0M) + .1 \\cdot U(\\$5M). \\end{aligned} \\] A little arithmetic will show that \\(EU(1A) - EU(1B) = EU(2A) - EU(2B)\\): \\[ \\begin{aligned} EU(1A) - EU(1B) &amp;= -.01 \\cdot U(\\$0M) + .11 \\cdot U(\\$1M) - .1 \\cdot U(\\$5M),\\\\ EU(2A) - EU(2B) &amp;= -.01 \\cdot U(\\$0M) + .11 \\cdot U(\\$1M) - .1 \\cdot U(\\$5M).\\\\ \\end{aligned} \\] So if 1A has higher expected utility than 1B, the same has to be true for 2A over 2B! And this holds no matter what we plug in for the utility values. Because we left those untouched, as placeholders. So however you value money, there’s just no way to have \\(EU(1A) &gt; EU(1B)\\) but \\(EU(2A) &lt; EU(2B)\\). "],
["savages-response.html", "12.3 Savage’s Response", " 12.3 Savage’s Response In a way, this actually makes sense. After all, both choices involve the same tradeoff. Imagine you’re holding a million dollars, and contemplating trading it for gamble 1B. What you’re contemplating is taking on a 1% risk of nothing in exchange for a 10% shot at five million dollars. And that’s the same tradeoff as in the choice between 2A and 2B: are you willing to take on an extra 1% risk of nothing in exchange for a 10% chance at five million? The only difference between the two choices is the context in which this tradeoff is contemplated. In the first context there is a safe option, you can keep your million and walk away. But in the second context, there is no safe option. You must face an 89% chance of leaving empty handed. The only question is whether you’re willing to accept an added 1% risk in exchange for the shot at five million. Should this difference in context make a difference to whether you accept the tradeoff? Leonard Savage, the champion of expected utility, famously felt the allure of context here. As he wrote in his landmark 1954 book: When the two situations were first presented, I immediately expressed preference for Gamble [1A] as opposed to Gamble [1B] and for Gamble [2B] as opposed to Gamble [2A], and I still feel an intuitive attraction to these preferences. But I have accepted the following way of looking at things… Savage’s way of looking at things was to imagine the gambles being determined by a lottery with a hundred tickets labelled #1 to #100: #1 #2–11 #12–100 1A $1M $1M $1M 1B $0 $5M $1M 2A $1M $1M $0 2B $0 $5M $0 Now, if you’re choosing between 1A and 1B, you wouldn’t care if you knew the ticket would be one of #12 to #100. Either way you get $1M. So all that matters is which option you would prefer if you know the ticket would be one of #1 through #11. And if you would choose 1A then, you should make the same choice between 2A and 2B. Because it’s the exact same choice when the ticket drawn will be one of #1 through #11. And it doesn’t matter which you choose if it’s one of the others. Savage encoded this idea in a famous principle he dubbed “the Sure-thing Principle”: Sure-Thing Principle If you would choose A over B if you knew \\(X\\) was true, and also if you knew \\(X\\) was false, then you should choose A over B when you don’t know whether \\(X\\) is true or not. Allais’ paradox shows that people don’t always follow this principle. Although it may sound obvious, it has implications that aren’t so obvious. It enforces a kind of “context free” decision making that humans don’t always find intuitive or comfortable. For many of us, it matters whether there’s a safe option—whether we could have the million dollars regardless, whether the ticket drawn will be one of #1–#11 or not. "],
["dr-savages-prescription.html", "12.4 Dr. Savage’s Prescription", " 12.4 Dr. Savage’s Prescription Savage’s answer to Allais illustrates a core conceptual distinction, one that arises again and again in philosophy. In Savage’s view, the expected utility rule is prescriptive rule, not descriptive. It says what people should, not what they will do. Just as people don’t always take their medicine as prescribed, people don’t always choose the option with the highest expected utility. We’ve already seen many ways in which humans are error-prone when it comes to reasoning with probabilities: base-rate neglect, the gambler’s fallacy, etc. Savage viewed Allais’ paradox similarly. It’s just another way humans fail to follow core principles of good reasoning, like the Sure-thing Principle. In fact Savage made the Sure-thing Principle a foundational axiom of his theory. It’s one of the eponymous Foundations of Statistics his book examines, and one central achievement of the book was to derive the law of expected utility from a small list of such assumptions. More than sixty years later, the debate over the Sure-thing Principle still hasn’t been entirely settled. "],
["the-ellsberg-paradox.html", "12.5 The Ellsberg Paradox", " 12.5 The Ellsberg Paradox When Chelsea Manning and Edward Snowden leaked classified material to the public, they found an advocate in Daniel Ellsberg. Ellsberg had leaked the famous Pentagon Papers to newspapers in 1971, revealing the U.S. government lies to the American public about the Vietnam war. Ellsberg was charged with espionage and conspiracy. Luckily for him, the government bungled the case. At Ellsberg’s trial all sorts of shenanigans emerged. White House officials had even broken into his psychiatrist’s office to find embarrassing information. The judge dismissed the charges. And the same White House burglars later became notorious for the most famous break-in in American politics, the Watergate scandal. But before lighting the fuse that ultimately destroyed Richard Nixon’s presidency, Ellsberg was known for his work in decision theory. In 1963 he challenged Savage’s Sure-thing Principle with puzzles like this one. Imagine an urn with 90 balls. 30 are red, and 60 are either black or white, but in unknown proportion. There might be anywhere from 0 to 60 black balls. A ball will be drawn at random, and you must choose between the following: 1A: win $100 if the ball is red, 1B: win $100 if the ball is black. You also face a second choice: 2A: win $100 if the ball is either red or white, 2B: win $100 if the ball is either black or white. Most people choose 1A over 1B, since you know what you’re getting with 1A: a 1/3 chance at the $100. Whereas 1B might give worse odds, maybe even no chance at all if there are no black balls. At the same time, most people choose 2B over 2A, and for a similar reason. With 2B, you know you’re getting a 2/3 chance at the $100. While 2A might give much worse odds, maybe even as low as 1/3 if there are no white balls in the urn. Like in the Allais paradox, this popular combination of choices violates the expected utility rule. The calculation that shows this is pretty similar to the one we did with Allais though, so let’s not rehearse it here. Instead let’s think about what Ellsberg is showing us here. "],
["ellsberg-allais.html", "12.6 Ellsberg &amp; Allais", " 12.6 Ellsberg &amp; Allais Ellsberg’s paradox is strongly reminiscent of Allais’. More than the two-choice structure they share, both also exploit a human preference for the known. In the Allais paradox we prefer the sure million, and in the Ellsberg paradox we prefer to know our chances. The kind of risk at play in each paradox has a different character, notice. In Allais’ paradox all the probabilities are known, and in one case we can even know the outcome. If you choose the safe million, you know what your fate will be. But in the Ellsberg paradox, you never know the outcome. The most you can know is the chance of each outcome. And yet, our preference for the known still takes hold. We still prefer to go with what we know, even if all we can know is the chance of each outcome. Is this preference for known risks rational, or irrational? Well, it violates Savage’s Sure-thing Principle. Consider Ellsberg’s dilemma as a decision table: Red Black White 1A $100 $0 $0 1B $0 $100 $0 2A $100 $0 $100 2B $0 $100 $100 If you knew a white ball was going to be drawn, you wouldn’t care which option you chose. And if you knew a white ball wouldn’t be drawn, then options 1A and 2A would be equivalent. So consistency would seem to demand selecting 2A if you selected 1A. Many decision theorists find this reasoning compelling. But more than a few turn it on its head and say so much the worse for the Sure-thing Principle. "],
["conclusion-1.html", "12.7 Conclusion", " 12.7 Conclusion Psychologists have tested the Allais and Ellsberg paradoxes on real human subjects many times, with pretty consistent results. These results have provided much insight into the psychology of human choice. They’ve even shaped a Nobel prize winning theory about how human decisions differ from the expected utility rule. But in philosophy, the study of decision theory tends to focus on how we should reason, not how we actually do reason. And as a prescriptive theory, the expected utility rule still looms large. It has its rivals, and is by no means the only game in town. But 350 years after Pascal and Fermat first dreamt it up, it remains the most popular game in town, by a wide margin. "],
["a-missing-ingredient.html", "13 A Missing Ingredient?", " 13 A Missing Ingredient? Our last decision puzzle is the simplest in a way, but also perhaps the most troubling. It sets probability to the side, focusing instead on questions of value. "],
["the-self-torturer.html", "13.1 The Self-Torturer", " 13.1 The Self-Torturer Suppose you volunteer for an experiment at the psychology lab: A small, portable device runs an electric current through your body. The device has a dial with a thousand settings. The lowest setting is so weak you can’t even feel it, while setting 1,000 is excruciating—almost enough to make you black out. But the difference between adjacent settings is so mild you can’t tell the difference. Not between settings 1 and 2, nor between 500 and 501, nor between 999 and 1,000. Each day, the experimenters will offer you $10,000 to turn the dial up one notch. And there is no turning back: once you accept the day’s offer, you must live with the pain for the rest of your life. What setting will you stop at? Maybe you’re thinking this is easy. A few hundred grand is worth some mild, lifelong discomfort. But no amount of money is worth living in debilitating pain for the rest of your days. So you’ll go up to some low setting, say 20, and then stop. You’ll be $200,000 richer, and the pain will be tolerable. But here’s the catch: why not go up to 21 instead? You can’t tell the difference between 20 and 21. And another $10,000 would have its uses. So aren’t you leaving money on the table? No matter what setting you stop at, the same question will arise. You could always have upped the dial by one and walked away ten grand richer with no noticeable difference in pain. Apparently, no matter what level you stop at, it’s irrational. You’ll always be leaving ten grand on the table. You’ll always be walking away from “free” money. Unless of course you go all the way up to setting 1,000. But then you’ll live the rest of your life in so much pain you won’t even be able to enjoy your winnings. "],
["the-ice-cream-conundrum.html", "13.2 The Ice Cream Conundrum", " 13.2 The Ice Cream Conundrum The self-torturer’s dilemma seems far fetched and fantastical, but we actually face such dilemmas every day. When there’s a tub of ice cream in the freezer, you have to stop eating from it at some point. Some bite must be your last of the day. Yet no single bite makes the difference between being as healthy as you’d like and not. If you take 50 bites, why did you stop there? You could have enjoyed just one more bite without any noticeable difference to your health. So aren’t you denying yourself pleasure unnecessarily, and isn’t that irrational? "],
["the-moral-1.html", "13.3 The Moral", " 13.3 The Moral Up to now we’ve focused on tradeoffs between risks and rewards. But there’s no element of chance in these puzzles, no risks at play. You know the consequences of eating ice cream. And we could allow you to try out various settings on the self-torture device before starting the experiment. These puzzles focus our attention on the reward side of the equation. The problem is how to weigh competing goods, like money vs. bodily comfort, or gustatory pleasure vs. health. At what point should you decline an increase in one good in exchange for the other? Decision theory doesn’t say. In the theory of expected utility, we leave this problem to the decision maker. We just assume that she has a utility function, which we then use to advise her on the other sort of tradeoff: risks vs. rewards, probabilities multiplied by utilities. In a way then, the puzzle of the self-torturer is a counterpoint to Bertrand’s paradox. Just as the laws of probability don’t tell us what probabilities to plug into the expected utility formula, the theory of utility doesn’t tell us the relevant utilities. According to one popular school of thought, leaving these questions unanswered is the right thing to do. They do not have objective answers, say many decision theorists. People differ in their tastes, and in their beliefs. So it is only fitting that we leave it to them to decide the values of the \\(Pr\\) and \\(U\\) functions themselves. But I have my doubts. It may be hard to say what the correct probabilities are, or which bit of ice cream should be your last. But hard questions can still have answers. They can even have answers when we have no hope of ever knowing what those are. "],
["the-lottery-paradox.html", "14 The Lottery Paradox", " 14 The Lottery Paradox In a recent segment on climate change, John Oliver mocked people who debate the existence of climate change, comparing it to questions like Do owls exist?, or Are there hats?. Citing the “mountain of research” proving the existence of climate change, he quipped: “The debate on climate change should not be whether it exists,” he said, “it’s what we should do about it.” Sometimes, the answer to a question becomes so clear, that we stop considering it. We accept the established answer as correct and proceed accordingly. But how certain does something have to be before we accept it as the correct answer? Is 90% certainty enough? Or should we put the cutoff at 95%, or maybe even 99%? Suppose we play it safe, setting the threshold at 99%. And consider this scenario: You hold one of a hundred lottery tickets. A single winner will be selected at random. Since your ticket’s chance of winning is 99%, you conclude it will lose. The other tickets have the same chance though, so you conclude they will lose too. But that means no ticket will win, which you know is not the case. Did we set the threshold too low? Would boosting it to 99.9% help? No. A higher threshold just needs a larger lottery, with more tickets, for the same puzzle to arise. With 1,000 tickets, each has a 99.9% chance of losing. Should you conclude that they’ll all lose then? "],
["three-principles.html", "14.1 Three Principles", " 14.1 Three Principles The lottery paradox turns on three assumptions, each plausible when considered in isolation. The first is that there is some level of certainty, some threshold of probability, which is sufficient for believability. In other words, there is some number \\(t\\) such that: The Threshold Principle If \\(Pr(A) \\geq t\\), then you should believe \\(A\\). Maybe \\(t = .9\\), or \\(.95\\), or even \\(.99\\). But the lottery paradox shows how any threshold shy of 1 collides with two further principles. The second is that you can combine beliefs you already hold to form new beliefs. We do this all the time, concluding that Cairo is in Africa, for example, because it’s the capital of Egypt, which is an African country. Stripping this idea down to a bare minimum, we have: The Conjunction Principle If it is rational to believe \\(A\\), and it is rational to believe \\(B\\), then it is rational to believe their conjunction, \\(A \\&amp; B\\). In the lottery paradox this leads you to conclude that both tickets #1 and #2 will lose, and #3, and so on up through ticket #100. In other words, all the tickets will lose, even though one must win. Here we meet our last principle, which forbids believing anything outright contradictory: No Contradictions It is never rational to believe an outright contradiction, a statement of the form \\(A \\,\\&amp;\\, \\neg A\\). Yet this is precisely where the Threshold and Conjunction Principles have taken us: to the belief that no ticket will win and yet one will. "],
["a-practical-dimension.html", "14.2 A Practical Dimension?", " 14.2 A Practical Dimension? Shouldn’t the certainty needed for belief depend on the context, though? The more is at stake, the more confidence we’ll demand before accepting a conclusion. Yet the Threshold Principle makes no mention of a contextual element. There’s no allowance that \\(t\\) might vary depending on what’s at stake. Could that be what’s missing from our analysis? No, and we can see this a few different ways. One is to imagine the lottery having extremely low stakes. Maybe the prize is a paltry $1. Or maybe you don’t hold one of the tickets yourself, you’re just idly speculating about a lottery others are participating in. There may well be a pragmatic element to acceptance. But this doesn’t help us solve the lottery paradox. Wherever the context fixes the threshold, a large enough lottery will then put each ticket’s chances of losing above that (contextually determined) threshold. "],
["conjunctivitis.html", "14.3 Conjunctivitis", " 14.3 Conjunctivitis Henry Kyburg, who made the lottery paradox famous in the 1960s, blamed the Conjunction Principle. You should believe of each ticket individually that it will lose, said Kyburg. But you can’t conclude from there that they’ll all lose. Although the Conjunction Principle may seem tempting, the lottery example shows it to be pathological—the temptation to embrace it is an illness we must combat, “conjunctivitis”. One thing to consider about Kyburg’s solution is that, even without the Conjunction Principle, your beliefs are still inconsistent as a collection. There’s no one contradictory thing you believe. But your world-view, taken as a whole, is impossible. It just can’t be that each individual ticket loses and yet one wins. Is this something we just have to live with? (We’ll come back to this in the next chapter, when we meet the preface paradox.) A second worry about Kyburg’s solution is that it puts the concept of belief in peril. What’s the point of believing something if you can’t count on it in logical reasoning? Surely it’s okay to combine your beliefs sometimes, like when you conclude that Cairo is in Africa because it’s the capital of Egypt, which is an African country. So when is it okay to combine beliefs, and when not? The obvious answer is: it’s okay when the combined belief is still highly probable. But if probability is the ultimate decider in what to believe, then what’s the point of believing? What you believe doesn’t inform what other beliefs you’ll adopt; that all depends on probabilities. And your decisions about what to do will also be determined by the probabilities, via the expected utility rule. If accepting something as true doesn’t affect what else you accept, or what you choose to do, what effects does it have? What role does belief play in our mental lives, if not to guide our reasoning about what to think and do? "],
["degrees-of-belief.html", "14.4 Degrees of Belief", " 14.4 Degrees of Belief In the 1960s and ’70s, Richard Jeffrey embraced this line of thinking and followed it all the way it to its logical conclusion. The whole idea of belief is pointless, he thought, at least the way it’s traditionally conceived. Probability really is the ultimate arbiter of what to think and do, so that must be all there is to belief. We may be used to thinking and talking about beliefs in on/off terms. But really, belief comes in degrees. We might say that you believe \\(2+2 = 4\\), and that Bill Gates is the richest person alive. But one of these beliefs is much stronger than the other. You are more confident that \\(2+2=4\\) than that Bill Gates is the richest living person. So we should replace the concept of “belief” with “degrees of belief”. The idea that belief is a discrete, yes/no thing is just a naive bit of outmoded psychology, Jeffrey said. Instead, belief comes on a continuum from no belief at all (0) to perfect certainty (1). The fundamental requirement of consistent belief isn’t avoiding contradictions, on this view. It’s having degrees of belief that obey the laws of probability. If you’re 70% confident it will rain today, you should be 30% confident it won’t. And in the lottery paradox, your degrees of belief do obey the laws of probability. You are 99% certain that ticket #1 will lose, and likewise for ticket #2, or any other ticket. And this is perfectly consistent with the laws of probability. After all, if we repeated this lottery over and over again every day for many years, each ticket would lose 99% of the time. A concern for Jeffrey’s view is whether it is psychologically realistic. Is it really true that humans never take a conclusion as given? Are we really always hedging under the hood, working with the high probability that something is true, rather than just the flat assumption that it is true? For my part, I’m skeptical. It’s not just that it doesn’t feel like I’m working probabilistically when I look up and see that the sun is out. Introspection about the workings of the mind is a notoriously unreliable way of doing serious psychology, after all. It’s also that I see disproof of Jeffrey’s view when I look at the models psychologists have actually developed of human reasoning. These models often seem to include reasoning process based on yes/no assumptions. But this is a live controversy, and the psychological study of human reasoning is still in its early stages. So, skeptical as I am, I think it’s still too early to say categorically that Jeffrey was wrong. (An irony I hope he would have appreciated.) "],
["mind-your-sources.html", "14.5 Mind Your Sources", " 14.5 Mind Your Sources A different angle on the lottery paradox challenges The Threshold Principle instead. Being highly probable isn’t by itself enough to make something believable, say many philosophers. Other factors matter too. What other factors? We can’t review all the proposals out there, but here is one representative, due to Dana Nelkin. Nelkin points out that your basis for thinking your lottery ticket will lose is purely statistical. All you know is that the ticket has a 99% chance of losing. And this sets it apart from mundane cases where belief is reasonable. If you see a car accident while walking to work, your belief that there was an accident is based on direct, visual observation. Or, if a friend tells you they saw an accident, your belief is based on testimony. Beliefs based on direct observation, or testimony, or other legitimate grounds, have something important in common, according to Nelkin. There is a connection between the event your belief is about, and the belief you form. The accident causes your belief; it explains why you formed that belief. But in the lottery paradox, your belief that (say) ticket #42 will lose isn’t caused or explained by that ticket losing. The drawing may not even have happened yet (and future events can’t cause past events!). So, Nelkin says, you should believe ticket #42 will lose, even though it’s highly probable. The problem now is that some beliefs about future events are reasonable. Suppose I turn my stove on and go watch TV while I wait for it to heat up. I expect my stove to be hot in five minutes’ time, but that future event doesn’t cause or explain why I believe it will happen. Rather, a common cause—me turning the stove on—explains both my belief, and the future hotness. Likewise, in the lottery paradox, the arrangements made at the lottery commission’s office are a common cause. Those arrangements explain why I believe ticket #42 will lose. And when that ticket does lose, those same arrangements explain why it did (it was just one of a hundred, so it was extremely likely to do so). Still, many philosophers have held on to the idea that there’s something distinctive about “purely statistical” evidence. It may be hard to say what separates the high probability in the lottery paradox from more mundane grounds for a belief, like observation and testimony. But there does seem to be something odd about believing that a given lottery ticket will lose. One way to get at this oddity is to consider what you would say after the drawing, where let’s suppose ticket #67 is selected as the winner. If someone claimed to know all along that ticket #42 would lose, you might retort that they couldn’t have known that. “You had identical reasons for thinking ticket #67 would lose,” you might say, “and yet it won.” For whatever reason, it seems you can’t know ticket #42 will lose, even if you end up being right that it does. And this seems to speak against believing it’ll lose. Perhaps because to believe something just is to take yourself to know it, as some philosophers have held. "],
["taking-stock.html", "14.6 Taking Stock", " 14.6 Taking Stock So what’s the moral of the lottery paradox? We’ve learned that there’s a conflict between some very elementary and appealing ideas. If we want a threshold of probability, beyond which we can take a conclusion as settled, then we have to abandon the idea that “settled” really means settled. We can’t just take the conclusions we’ve drawn and combine them freely. Otherwise, we’re liable to reach improbable conclusions, even contradictory ones. What we should do with that lesson, however, isn’t so clear just yet. Does that mean there’s no such threshold? Or does it mean that there’s really no such thing as a “settled” belief? Looking at some more paradox in this neighbourhood may help us get clearer on these matters. "],
["the-preface-paradox.html", "15 The Preface Paradox", " 15 The Preface Paradox Our next paradox emerged around the same time as the lottery paradox: Imagine you’ve written a non-fiction book, a grade four history textbook let’s say. You did your research carefully and thoroughly, and two experts independently checked the manuscript for errors. After correcting the errors they found, you thank them for their help in the book’s preface. You then write: “I take full responsibility for the errors that must inevitably remain.” Apparently you believe your book contains some falsehoods. That’s what you’re taking responsibility for, after all. But you also believe each statement made in the body of the book, otherwise you wouldn’t have written what you did. So your beliefs are logically inconsistent. You believe there is at least one falsehood. But you also believe the first sentence is true, and the second, the third, etc. How might you resolve this inconsistency? Suppose you retract your admission in the preface, you suspend judgment about whether any errors remain. That would be absurdly immodest! It’s incredibly unlikely you’ve written the first error-free textbook in history. Maybe you could retract some of the claims in the main body of the book instead? But you’ve checked them all carefully. Which ones should you retract? Beside, you’d have to retract a lot of them. Even a single chapter is almost certain to contain some falsehoods, so you’d have to pare the book down so much there’d barely be anything left. Apparently, there is no good way to make your beliefs consistent here. "],
["everyones-problem.html", "15.1 Everyone’s Problem", " 15.1 Everyone’s Problem This problem affects everyone, not just the authors of textbooks. Imagine we wrote down everything you believe as a book, even the mundane stuff like \\(2 + 3 = 5\\), or snow is white. It may not be a best seller, but it is philosophically interesting in another way. For consider: are any of the statements in this book false? Almost certainly. Everybody makes mistakes, in fact you know you’ve made mistakes before. But that means we have to include that in the preface of your “belief book”. The preface should say: some of the statements in this book are false. So you’re in the same position as the author of our history textbook: your beliefs are inconsistent. You believe each statement in the body of the book, but you also believe some of those statements must be false. Now, sometimes we believe things we shouldn’t. We jump to conclusions, or fall prey to wishful thinking, or fail to respect the available evidence for any number of other reasons. But notice, even if you were perfectly rational, you’d still be in the same dilemma. Even if we only ever believed what our evidence warranted, we’d still have some false beliefs. Because even beliefs based entirely in strong evidence have some small chance of being wrong. And if you believe enough things, these slivers of error pile up. "],
["comparison-with-the-lottery-paradox.html", "15.2 Comparison with the Lottery Paradox", " 15.2 Comparison with the Lottery Paradox The preface paradox is strongly reminiscent of the lottery paradox. Both involve a large collection of beliefs, each of which is highly probable individually. But taken all together, they are inconsistent. There is one major difference, though. The individual beliefs in the preface paradox are not based on “purely statistical” evidence. They are based on research that draws on the testimony of historical records, direct observations of physical evidence, and so on. That means we can’t escape the paradox by blaming The Threshold Principle. We can’t say, á la Nelkin, that the source of the problem is believing things solely on the basis of high statistical probability. Because now we’re considering ordinary beliefs, based on sources like visual observation and testimony. And if these beliefs aren’t warranted, then none of our beliefs are! If we’re to avoid widespread skepticism then, we have to solve the paradox in some way that preserves our ordinary, mundane beliefs. So it’s natural to view the preface paradox as a repudiation of The Conjunction Principle. Imagine what would happen if you conjoined your belief in the book’s first statement with the second, and the third, and so on all the way up to the very last sentence. You’d conclude that the whole book was true through and through, and thus there are no errors anywhere! But that would be absurdly immodest. Can we just abandon The Conjunction Principle and be done with both paradoxes then, the lottery and the preface? "],
["two-worries.html", "15.3 Two Worries", " 15.3 Two Worries It’s not so easy (you’ll be shocked to learn). For one thing, abandoning The Conjunction Principle leaves us with the same problem that faced Kyburg. It seems to make belief pointless. What’s the point of believing something if you can’t use it as a basis upon which to form further beliefs? But also, we might still have to reject The Threshold Principle anyway! As we noted in the last chapter, there’s something fishy about believing (say) ticket #42 will lose. When the day of the drawing comes, and ticket #67 is selected as the winner, could you claim to have known #42 would lose all along? It seems not. How could you know when you had identical reasons to think ticket #67 would lose, and it actually won? So there’s still much we don’t understand. And indeed, this second problem—the unknowability of “lottery propositions”—takes us directly to our next puzzle. "],
["the-harman-vogel-paradox.html", "16 The Harman-Vogel Paradox", " 16 The Harman-Vogel Paradox Our next puzzle is usually put in terms of what we know, as opposed to what to believe. But it’s a direct follow on to the lottery paradox. "],
["dude-wheres-your-car.html", "16.1 Dude, Where’s Your Car?", " 16.1 Dude, Where’s Your Car? Consider this story: Anika parks her car in Lot 8 and goes to class. “Where’s your car?”, asks her friend. “Lot 8,” Anika replies. Unfortunately Anika’s friend has been reading about the lottery paradox: “How do you know it hasn’t been stolen and moved? Cars are stolen in this town every day. Maybe you’re a ‘winner’ in today’s Car Theft Lottery.” Anika rolls her eyes. “Don’t get clever”, she says, “of course I can’t know about that. I just know where I parked it.” At first it seems entirely natural to say that Anika knows where her car is. But then, contemplating the “lottery-esque” nature of car theft, it also becomes natural to say she can’t know whether her car has been stolen. Yet these two things are in direct tension: if her car is still in Lot 8, it can’t have been stolen and moved. To make the tension explicit, consider this plausible-seeming, general principle about knowledge and logic: Single Premise Closure If you know \\(P\\), and you deduce \\(Q\\) from \\(P\\), then you know \\(Q\\). The basic idea is that logical reasoning is a way to extend your knowledge. If something follows from the knowledge you already have, and you realize that it follows, then you know that further thing too. The puzzle is that Anika can’t extend her knowledge in this way. She can’t say, “well my car is in Lot 8, like I said, so it can’t have been stolen and moved.” That would be perverse! And yet, that’s what the Single Premise Closure Principle seems to license. "],
["key-ingredients.html", "16.2 Key Ingredients", " 16.2 Key Ingredients The puzzle trades on two key ingredients. The first is the unknowability of what we might call “lottery propositions”. We noted that it seems weird to claim you knew ticket #42 would lose, even when it actually does lose. Because you had the same reasons to think #67 would lose, and it actually won. But lots of situations have a similar feel, even though they’re not about actual lotteries. Anika can’t know that her car isn’t one of the ones to be stolen today. Because even if it doesn’t get stolen, her reasons for thinking it wouldn’t would have applied just as well to one of the cars that did. And these “lottery-esque” situations are everywhere. Will you be living in the same city next year? Maybe not: you might ‘win’ the Great Heart Attack Lottery. Will you have the same, modest bank balance you presently do a year from now? Maybe not: you might win the actual lottery. The second ingredient is the idea that logic is a way to extend your knowledge, captured in the Single Premise Closure principle. This idea is similar to one we touched on with the lottery and the preface paradoxes. But there are two important differences. Most importantly, we’re talking about reasoning based on a single assumption now. The Conjunction Principle entitled you to collect together many assumptions, letting the risks of error pile up. But there’s no “accrual of risk” in the kind of logical reasoning we’re examining here. There is very little risk that Anika is wrong about her car being in Lot 8, and it follows from that assumption alone that it hasn’t been stolen and moved elsewhere. A second difference is that we’re talking about knowledge now, instead of (rational) belief. It’s natural to say, at least at first, that Anika knows where her car is parked. And even though there’s no added risk in concluding that her car hasn’t been stolen, she can’t do basic logical reasoning using that knowledge?! What gives?? "],
["the-skeptical-threat.html", "16.3 The Skeptical Threat", " 16.3 The Skeptical Threat Notice how Anika’s story ends: she concedes that she only knows where she parked her car. And there’s something natural about that concession. So maybe she never knew it was in Lot 8 in the first place? That’s a dangerous road to go down. Lottery-esque propositions are ubiquitous, we noted. So Anika’s predicament is widespread. If we go this way then, we’ll have to admit we don’t know most of the things we take ourselves to know. You don’t know whether you’ll be rich or poor next year. You don’t even know whether you’ll be alive! At most, you know whether these things are probable. But we don’t know whether they are true. Lovers of probability might delight in this result. But it goes pretty hard against both common sense, and common discourse. Is the most we can know really that something is more or less probable? If so, a lot of the way we think and talk is off the mark, even though it seems entirely reasonable. Let’s try to avoid doing so much violence to common sense, if we can. "],
["probability-intransitivity.html", "16.4 Probability &amp; Intransitivity", " 16.4 Probability &amp; Intransitivity Here’s a curious fact about probability: it doesn’t travel well. Sometimes probabilistic support doesn’t carry over from one thing to another. More exactly: sometimes \\(A\\) makes \\(B\\) probable, and \\(B\\) makes \\(C\\) probable, but \\(A\\) doesn’t make \\(C\\) probable. Here’s a simple example. Most Canadians live in North America, and most North American residents live in the U.S. But most Canadians do not live in the U.S. They almost all live in Canada! So if I told you that Anika is a Canadian citizen, you’d be right think she probably lives in North American. And if I had told you instead only that she lives in North America, you’d have been right to think she probably lives in the U.S. But from the information that she lives in Canada, you had better not get to thinking she lives in the U.S. In fact, things can even get a bit weirder. Sometimes \\(A\\) increases the probability of \\(B\\), and \\(B\\) logically entails \\(C\\), yet \\(A\\) is irrelevant to \\(C\\). For example, suppose I draw a random card from a normal deck. If it’s an ace, that makes it more likely to be the ace of spades (up from \\(1/52\\) to \\(1/4\\)). And if it’s the ace of spades then it must be a spade. But the fact it’s an ace has no bearing on whether it’s a spade. That probably stays fixed at \\(1/4\\). "],
["transitivity-intransitivity.html", "16.5 Transitivity &amp; Intransitivity", " 16.5 Transitivity &amp; Intransitivity Some relations are transitive, some are not. If \\(a\\) is taller than \\(b\\), and \\(b\\) is taller than \\(c\\), then \\(a\\) is taller than \\(c\\). But if \\(a\\) is nice to \\(b\\), and \\(b\\) is nice to \\(c\\), that doesn’t necessarily mean (sadly) that \\(a\\) is nice to \\(c\\). Famously, reasoning involving certainties is transitive. Or, as logicians would say, the relation of logical entailment is transitive. If \\(A\\) entails \\(B\\) and \\(B\\) entails \\(C\\), then \\(A\\) entails \\(C\\). But, as we’ve just seen, the uncertain counterpart to this relation is not transitive. The ‘makes probable’ relation isn’t transitive (think: Canadian citizens living in North America). And even when \\(A\\) makes \\(B\\) probable and \\(B\\) makes \\(C\\) perfectly certain, that still doesn’t necessarily mean \\(A\\) makes \\(C\\) probable (think: aces and spades). This is one way in which probabilistic reasoning is fundamentally different from logical (“deductive”) reasoning. And it generates curious cases like Anika’s. "],
["anika-intransitivity.html", "16.6 Anika &amp; Intransitivity", " 16.6 Anika &amp; Intransitivity Let’s put Anika’s situation in the \\(A\\)-\\(B\\)-\\(C\\) format: \\(A\\) = I remember parking in Lot 8. \\(B\\) = My car is now in Lot 8. \\(C\\) = My car hasn’t been stolen and moved somewhere else. \\(A\\) makes \\(B\\) probable because (let’s assume) Anika’s memory is usually reliable. And because cars usually stay where we park them. They don’t always stay put, sadly. Which is why this is a case of reasoning with uncertainties. But the uncertainty is mild, because usually they do stay put. \\(B\\) entails \\(C\\) because it’s impossible that the car has been stolen and moved elsewhere if it’s in Lot 8. That’s what “elsewhere” means: anywhere but Lot 8. Now the last bit: why does \\(A\\) have no bearing on \\(C\\)? Think about it this way. Imagine Anika is one of the unlucky winners of today’s Great Car Theft Lottery. Would she still remember parking her car in Lot 8? Yes, she would. Whether her car has been stolen or not, she will remember the same thing: parking in Lot 8. So that memory doesn’t discriminate between the possible cases where her car is stolen vs. not-stolen. "],
["a-solution.html", "16.7 A Solution?", " 16.7 A Solution? How does this help us solve our puzzle? It seems to tell us Single Premise Closure is false. Whether you can deduce \\(Q\\) from \\(P\\) depends on how you know \\(P\\) in the first place. If your knowledge of \\(P\\) is based on something that also supports \\(Q\\), then go ahead: deduce \\(Q\\) from \\(P\\). If you know the ice cream is vanilla based on tasting it, then you can deduce it’s not chocolate. The flavour you tasted supports both that it is vanilla (\\(P\\)) and that it is not chocolate (\\(Q\\)). But if the basis for your knowledge of \\(P\\) has no bearing on \\(Q\\), then inferring \\(Q\\) from \\(P\\) would be bad form. You don’t want to go around basing beliefs on things that aren’t relevant to them. "],
["more-intransitivity.html", "16.8 More Intransitivity", " 16.8 More Intransitivity The intransitivity of probability may also help us understand other paradoxes we’ve been looking at, like the preface paradox. There, the research you did for your book makes each statement in the book probable. And those statements all together entail that the book has no errors. But your research doesn’t support this “no-errors” conclusion. Even with all the research you did (and the double-checking of your peers), some errors are inevitable. The structure is a bit different from Anika’s case, though. The preface paradox illustrates a failure of collective transitivity: \\(A\\) makes \\(B_1\\) probable, and \\(B_2\\) probable, …, and \\(B_n\\) probable. \\(B_1\\)–\\(B_n\\) together entail \\(C\\). But \\(A\\) doesn’t make \\(C\\) probable. In terms of a diagram, the preface paradox looks like this: Whereas Anika’s case looks like this: There are still more forms of (in)transitivity, and we’ll encounter one soon. But first let’s try our hand at another paradox using what we’ve already learned. "],
["dogmatism-bootstrapping.html", "17 Dogmatism &amp; Bootstrapping", " 17 Dogmatism &amp; Bootstrapping Here’s one more puzzle about deploying your existing knowledge. Oliver wants to know whether Manchester United won last night’s match. So he goes to The Guardian website and sees that they did. Oliver now knows Manchester won. He then reasons that any evidence he might encounter to the contrary must be misleading. For example, if The Standard reports they lost, it must be an error since Manchester won. Later that day Oliver happens to be reading an article on The Standard website, when he notices the latest sports courses listed in the margin. Sure enough, The Standard reports a loss for Manchester. Oliver concludes they made a mistake. Something seems very wrong with Oliver’s reasoning. Imagine he had visited to The Standard website first instead: then he would have dismissed The Guardian’s report that Manchester won, on the grounds that they lost (as he first read in The Standard.) But shouldn’t the order be irrelevant? At the end of the day, he has two contrary reports from reliable sources. So he should be undecided about who won. He should suspend judgment until he can figure out which source made a mistake. "],
["single-premise-closure-again.html", "17.1 Single Premise Closure Again", " 17.1 Single Premise Closure Again This puzzle shares a key ingredient of the Harman-Vogel puzzle: the idea that reasoning logically from something you know is a way to gain further knowledge. Oliver knows Manchester won. He deduces from there that any evidence to the contrary is misleading, thus The Standard’s report to the contrary must be misleading. This conclusion seems too strong, though. It seems to entitle him to ignore The Standard’s report, which seems just bull headed. (Hence the name “dogmatism paradox”.) And yet, this seems to be what’s licensed by the Single Premise Closure principle from the previous chapter. If you know something, can’t you also know the things that follow directly from it? "],
["intransitivity-again.html", "17.2 Intransitivity Again", " 17.2 Intransitivity Again The Dogmatism Puzzle has an intransitive structure, much like the Harman-Vogel paradox. The report in The Guardian makes it highly likely Manchester won. The Guardian is a highly reliable source after all, at least about mundane stuff like sports scores. To fix ideas, let’s suppose Manchester’s chances were 50% to start with, and The Guardian’s report then makes it almost certain they won: 99%. Now, if Manchester won, then The Standard must be mistaken when they report a loss instead. That’s just elementary logic. But the report in The Guardian doesn’t make it very likely The Standard is mistaken. In fact, the probability is just 50% if we assume the two papers are equally reliable. So we have another case where \\(A\\) makes \\(B\\) likely, and \\(B\\) makes \\(C\\) certain, yet \\(A\\) doesn’t make \\(C\\) likely. In terms of a diagram: Which looks a lot like the diagram we had in the case of Anika: "],
["the-bootstrapping-puzzle.html", "17.3 The Bootstrapping Puzzle", " 17.3 The Bootstrapping Puzzle Our last puzzle in this neighbourhood has a somewhat different structure: Suppose you know The New York Times to be a reliable source, albeit an imperfect one. One morning you go to their website and read their first headline, which reports some fact \\(P\\). You thereby come to know \\(P\\). You then think to yourself, “The Times reported \\(P\\), and \\(P\\) is true. So they got that one right.” You then repeat the same reasoning for the next statement \\(Q\\), and the next statement \\(R\\), and so on. You conclude that they’re on a roll today—The Times may even be a more reliable source than you thought. So far today, everything they’ve reported has been true! Once again, the reasoning here looks pretty dubious. Your only reason for believing \\(P\\) (and \\(Q\\), and \\(R\\)) is that’s what The Times reports. How can you assess The Times’ credibility by using them as the corroborating source?? And yet, each step in your reasoning here seems legitimate, considered in isolation. If a newspaper is reliable, it’s legitimate to believe what it says. So your belief in \\(P\\) is perfectly reasonable. Further, it’s obvious where you got this information. You can plainly see you’re reading The Times. So your belief that The Times reported \\(P\\) is reasonable too. So you’ve established a positive instance of The Times’s reliability, an instance where what they said is true. They reported \\(P\\), and indeed \\(P\\) is true. And you establish another such instance with \\(Q\\), and another still with \\(R\\). That’s only three positive instances, of course, which is a very small sample. So you can only increase your trust in The Times by a teeny, tiny bit. But any positive increase at all in your trust feels wrong. You’ve done nothing to independently verify these reports! "],
["circularity-to-the-rescue.html", "17.4 Circularity to the Rescue?", " 17.4 Circularity to the Rescue? It’s tempting to diagnose the problem in terms of circularity. For example, we might say that the problem is an illegitimate use of “rule circular” reasoning: Rule Circularity A piece of reasoning is rule circular if it relies on a given rule in the process of assessing that very rule’s reliability. In our example, the rule woud be: believe what the Times reports. The problem is that rule circular is actually perfectly fine sometimes! Suppose you test how reliable your memory is by playing a round of the card game Memory. At the end of the game, you can rely on your memory of your performance to evaluate your memory’s accuracy. Or take this example: suppose The Times reports on three independent studies conducted by universities around the world. All three studies found The Times more reliable than people think. If you read about these studies in The Times, it’s still right to boost your view of their reliability on that basis. Sure, they might be lying, or maybe just exaggerating these studies’ findings. But they usually report pretty objectively. And they’d get called out pretty quickly if they fabricated or exaggerated studies of their own reliability, damaging their own reputation. So there’s a good chance the report is accurate, and they really are more reliable than you previously thought. "],
["intransitivity-again-1.html", "17.5 Intransitivity Again", " 17.5 Intransitivity Again So what’s the right diagnosis? We have another transitivity failure. The fact that The Times reports \\(P\\) supports \\(P\\), because they’re generally reliable. And the fact that The Times reports \\(P\\) and \\(P\\) is true, supports their reliability, at least a teeny tiny bit, because it’s a positive data point. But The Times reporting \\(P\\) says nothing about their reliability. The structure here is a little bit different than before, though. For one thing, all steps in the reasoning are probabilistic. There are no deductive inferences, no certainties. But more interestingly, the steps are cumulative. The assumption \\(P\\) has to be combined with the premise it was based on, that The Times reported \\(P\\), in order to boost our trust in the The Times. In \\(A\\)-\\(B\\)-\\(C\\) terms, this is a failure of cumulative transitivity. \\(A\\) supports \\(B\\). \\(A\\) and \\(B\\) together support \\(C\\). But \\(A\\) by itself does not support \\(C\\). But this wrinkle notwithstanding, the basic lesson is much the same as before. What you can infer from on an established conclusion depends on the basis of that conclusion in the first place. "],
["the-vitali-paradox.html", "18 The Vitali Paradox", " 18 The Vitali Paradox Now for something completely different. Are there some questions where probability just doesn’t apply? Are there some things where there just can’t be any fact of the matter what the chances are? Yes. And we’re going to describe one in this chapter. It’s not that the probability is zero in these cases, mind you. It’s that there is no fact of the matter what the chances are. The question, “what is the chance of \\(X\\)?” just has no answer. It can’t have an answer, even for God. This chapter involves more mathematical nitpickery than previous ones, by the way. But you don’t need any advanced math background. And the basic ideas can be grasped even without mastering all the gory details. So if the details are putting you to sleep, go ahead and gloss over to get the big picture. "],
["background.html", "18.1 Background", " 18.1 Background Let’s recall some basic concepts and symbols: The symbol \\(\\mathbb{Q}\\) represents the set of all rational numbers, i.e. ratios of whole numbers. (The \\(\\mathbb{Q}\\) is for “quotient”.) These numbers can all be written in the form \\(m/n\\), where \\(m\\) and \\(n\\) are whole numbers, and \\(n \\neq 0\\). Examples include \\(1/2\\), \\(-35/7\\), and \\(8/9\\), and plenty more. Infinitely more, in fact. The symbol \\(\\mathbb{R}\\) stands for the set of real numbers. These include the rational numbers \\(\\mathbb{Q}\\), but also the numbers that can’t be expressed as a ratio. Examples of these “irrational” numbers are \\(\\pi\\), \\(\\sqrt{2}\\), and \\(\\pi - 3\\). We’re going to be focused just on numbers from \\(0\\) to \\(1\\), however. And we’ll write \\([0,1]\\) for the set of real numbers in this range: \\(0 \\leq x \\leq 1\\). This is called the unit interval. "],
["first-steps.html", "18.2 First Steps", " 18.2 First Steps Let’s start by separating the numbers in \\([0,1]\\) into distinct subgroups. Specifically, we’ll put two numbers in the same subgroup if the distance between them is a rational number. Definition If \\((x - y) \\in \\mathbb{Q}\\) then we write \\(x \\sim y\\). So, for example, since \\(3/4 - 1/2 = 1/4\\) and \\(1/4\\) is a rational number, \\(3/4 \\sim 1/2\\). For another example, consider the decimal part of \\(\\pi\\), namely \\(\\pi - 3 = 0.14159\\ldots\\). Compare that with \\(\\pi - 31/10 = 0.04159\\ldots\\). These two numbers are separated by \\(1/10\\), a rational number. So \\((\\pi - 3) \\sim (\\pi - 31/10)\\). Now here’s an important fact about this relation \\(\\sim\\) we’ve created: Fact 1 The relation \\(\\sim\\) is transitive: if \\(x \\sim y\\) and \\(y \\sim z\\), then \\(x \\sim z\\). Why? Well suppose \\(x \\sim y\\) and \\(y \\sim z\\). Then \\(x - y = r \\in \\mathbb{Q}\\), and \\(y - z = s \\in \\mathbb{Q}\\). So \\(z = y - s\\). So \\(x - z = x - y - s = r - s\\). And \\(r - s\\) is rational (it’s a difference of rational numbers). So \\(x \\sim z\\). Now let’s use this \\(\\sim\\) relation to make our divvying up of \\([0,1]\\) official. We put two numbers \\(x\\) and \\(y\\) from \\([0,1]\\) in the same subset \\(S_x\\) just in case \\(x \\sim y\\). Here’s another important fact, about the resulting subsets \\(S_x\\): Fact 2 The resulting subsets \\(S_x\\) form a partition of the unit interval. That is, every element of \\([0,1]\\) falls into one, and only one, subset \\(S_x\\). Why? Well suppose \\(z\\) is included in \\(S_x\\) and in \\(S_y\\). Then we have \\(z \\sim x\\) and \\(z \\sim y\\). So \\(x \\sim y\\), by transitivity (Fact 1). So really, \\(z\\) is in just one of our subsets, since \\(S_x\\) and \\(S_y\\) are really the same set. "],
["the-vitali-set.html", "18.3 The Vitali Set", " 18.3 The Vitali Set Now we come to the set we’re really interested in: the Vitali set, \\(V\\). \\(V\\) is composed by selecting one element from each subset \\(S_x\\). So we take just one element of \\(S_x\\), one element of \\(S_y\\), etc. And we put all these elements together in a single set \\(V\\). Definition The Vitali set, \\(V\\), contains one and only one element from each \\(S_x\\). Which element should we choose from each \\(S_x\\)? It doesn’t matter. Just choose an arbitrary element from each. (For the math geeks: we’ve just used the notorious Axiom of Choice.) "],
["rotations.html", "18.4 Rotations", " 18.4 Rotations Now imagine the elements in the Vitali set as points scattered around the unit circle. We can rotate this set of points around the circle “rigidly”. That is, we can move each point around the circle clockwise by the same distance \\(d\\). The result will be a new subset of \\([0,1]\\), which we’ll call \\(V \\oplus d\\). Notice, the distances between the points stay fixed when we do these rotations. So the rotated set has the same “shape” as the original. We’re going to focus just on the “rational rotations”: rotations \\(V \\oplus r\\) where \\(r\\) is a rational number. Fact 3 Rational rotations of \\(V\\) do not overlap. Why not? Well suppose \\(V \\oplus r\\) and \\(V \\oplus s\\) had some element \\(y\\) in common. Because \\(y \\in (V \\oplus r)\\), we have \\(x \\sim y\\) for some \\(x \\in V\\). And because \\(y \\in (V \\oplus s)\\), we have \\(y \\sim z\\) for some \\(z \\in V\\). But then \\(x \\sim z\\), contradicting the fact that \\(V\\) contains just one element from each \\(S_x\\). One last fact to establish: Fact 4 The rational rotations of \\(V\\) cover the unit interval. In other words, every point of \\([0,1]\\) falls in some rational rotation or other. Why? Well, every point \\(y\\) of \\([0,1]\\) was in one of our original subsets \\(S_x\\). So if \\(y\\) wasn’t selected to go into \\(V\\), then \\(x \\sim y\\) for some \\(x\\) that was put into \\(V\\). So \\(y\\) is a rational distance from some \\(x \\in V\\). Together Facts 3 and 4 tell us that the rational rotations of \\(V\\) form a partition of the unit interval. "],
["the-kicker.html", "18.5 The Kicker", " 18.5 The Kicker Suppose you wake up in the middle of the night wondering what the time after the hour is—where the minute hand might be pointing. What is \\(Pr(V)\\), the probability the minute hand is pointing to an element of \\(V\\)? Well it can’t be \\(0\\). Because if \\(Pr(V) = 0\\), then \\(Pr(V \\oplus r) = 0\\) for every \\(r\\). (Because \\(V\\) and \\(V \\oplus r\\) always have the same “shape”.) And by the law of Additivity, \\(\\sum_r Pr(V \\oplus r) = 1\\). So it must be some positive number, right? Nope. If \\(Pr(V) = a &gt; 0\\), then \\(Pr([0,1]) = a + a + a + \\ldots = \\infty\\), but it should be \\(1\\)! So there is no number \\(Pr(V)\\)! In fact, there’s no number for any of its rotations either: \\(Pr(V \\oplus r)\\) is undefined. And yet, these possibilities are exhaustive. \\(V\\) and its rotations cover all the points on the unit circle. So the minute hand must be pointing to one of them. But there’s no fact of the matter how likely it is to be pointing to any one of them. "]
]
