[
["index.html", "Probability Through Paradox Preface", " Probability Through Paradox Jonathan Weisberg Preface This book introduces probability by way of its many paradoxes. When I first had the idea to write a book like this, I sat down to list as many puzzles and paradoxes of probability as I could think of off the top of my head. There were over twenty-five, right off the bat. After a bit more thought and discussion with friends on facebook, the number grew to well over thirty. I had always known there were plenty of probabilistic puzzles around. But I was still surprised to see them all listed in one place. And then I was struck by how much of what I know about probability is bound up with the items on that least. So I decided the book was a good idea. You can learn a lot about probability just by getting to know its paradoxes. It’s also a good way to learn about probability because puzzles and paradoxes are a common frame of reference for experts who talk and write about probability. They even serve as a shibboleth sometimes; fluency with these paradoxes is the secret-handshake by which initiates identify their peers. These considerations are especially true of philosophical approaches to probability, but they also apply to its mathematical and applied facets. So, although our approach will be primarily philosophical, I hope that you will find plenty of interest in this book whatever your orientation. "],
["the-monty-hall-problem.html", "1 The Monty Hall Problem", " 1 The Monty Hall Problem Let’s start with a puzzle so famous it’s appeared in Hollywood movies: the Monty Hall problem. "],
["ask-marilyn.html", "1.1 Ask Marilyn", " 1.1 Ask Marilyn Marilyn vos Savant had the highest IQ ever recorded, until the Guinness Book of Records dropped that category in 1990 because of concerns about IQ tests. In her “Ask Marilyn” column for Parade magazine, vos Savant answers hard questions from readers: puzzles, brain teasers, philosophical quandaries… you name it. When she published her answer to our first puzzle, a bunch of people with fancy credentials wrote in to complain she was wrong. Here was the question: Suppose you’re on a game show, and you’re given the choice of three doors. Behind one door is a car, behind the others, goats. You pick a door, say #1, and the host, who knows what’s behind the doors, opens another door, say #3, which has a goat. He says to you, “Do you want to pick door #2?” Is it to your advantage to switch your choice of doors? —Craig F. Whitaker, Columbia, Maryland If you’re like me, your gut says it doesn’t matter. There are two doors left; one has the car behind it; so they both have the same one-in-two chance of being a winner. Flip a coin, stick or move. It doesn’t matter. But Marilyn said you should switch! And you know what? She’s right. Why? Because the host knows where the car is, and he’s not going to just show it to you. That means he effectively just told you it’s behind behind door #2, unless you got lucky with your first pick of door #1. Because unless you got lucky on your first pick, his choice of which door to open was forced. He had to open door #3 to avoid showing you the car behind door #2. Again, that’s assuming you were unlucky on your first pick of door #1. But that’s a healthy two-in-three chance. So there’s a two-in-three chance door #2 has the car. Still don’t believe me? It’s okay, that’s normal. Here’s how Marilyn convinced her readers to switch. Imagine the same game, but with 100 doors. You pick door #1, and the host opens every other door except, let’s say, door #42. What do you think now, is it just a fifty-fifty shot? No way! Unless you got very lucky on your first pick and the car is behind door #1, you’ve just learned that the lucky door is #42. Still don’t believe me? That’s okay, that’s normal too! One of the most famous mathematicians of the 20th century wasn’t convinced either. Paul Erdős published over 1,500 papers in mathematics, more than any other mathematician ever. A lot of his work was even advanced research in probability theory! But when his friend, Andrew Vázsonyi, told him about this puzzle, he wasn’t having it. He kept asking Vázsonyi for a better explanation why he should switch. Vázsonyi even showed him a computer simulation he’d made of the game, where switching doors won the car two-thirds of the time. But Erdős still wanted a pure, mathematical explanation. Fun facts about Erdős by the way: he basically lived for math. He would travel the world, visiting friends at their houses, often uninvited. He would drink coffee (and, eventually, take amphetamines) and do math with his host, until they solved whatever problem the host had been working on before Erdős showed up. Then he’d move on to the next house, and the next problem. He was such a character, and so ubiquitous, that mathematicians make a game of tracing their connection to him. Every mathematician has an “Erdős number”. It’s kind of like six degrees of Kevin Bacon. Erdős is number zero. People who coauthored with Erdős have number 1. People who coauthored with them have number 2. Their coauthors have number 3, and so on. "],
["three-prisoners.html", "1.2 Three Prisoners", " 1.2 Three Prisoners In my last year as an undergraduate I took a class that changed my life. It literally defined my career as a philosopher. To this day, my career is devoted to the concepts, tools, and problems I met there for the first time. One of those problems, which my professor assigned as homework, goes like this: Three prisoners, A, B, and C, are condemned to die in the morning. But the king decides in the night to pardon one of them. He makes his choice at random and communicates it to the guard, who is sworn to secrecy. She can only tell the prisoners that one of them will be released at dawn. Prisoner A welcomes the news, as he now has a 1/3 chance of survival. Hoping to go even further, he says to the guard, “I know you can’t tell me whether I am condemned or pardoned. But at least one other prisoner must still be condemned, so can you just name one who is?”. The guard replies (truthfully) that B is still condemned. “Ok”, says A, “then it’s either me or C who was pardoned. So my chance of survival has gone up to 1/2”. Unfortunately for A, he is mistaken. But how? This homework problem got me so turned inside out, I ended up writing a computer program to run simulations of it! Just like Vázsonyi. (Fools and wise men think alike, I guess.) Well, my professor laughed. He called my computer simulation an “empirical” approach, and he didn’t mean it as a compliment. Like Erdős, he wanted me to use reason, not a brute-force, computerized experiment. Well, here’s one reasoned argument that prisoner A is mistaken. By A’s logic, their chances would also go down to 1/2 if the guard identified C instead of B. Because then there would be only one more prisoner to be released, with two candidates remaining: A and B. But if A’s chances would be the same no matter what the guard said, then the guard’s response couldn’t possibly be informative. If your chances of having a certain disease are the same whether a certain medical test comes up positive or not, then the test must be useless, right? This argument is right as far as it goes. But it doesn’t really explain where the prisoner went wrong. It just tells us she must’ve gone wrong somewhere. But where? I think the correct explanation involves a famous precept known as The Total Evidence Requirement. It says pretty much what it sounds like: to get the true probability of something, you have to take account of all the evidence you have. And the guard isn’t just telling A that B is still condemned. She’s also telling B, implicitly, that she chose to identify B as one of the condemned prisoners. It’s really counterintuitive that this extra information makes a difference. But it turns out to make all the difference in the world. Here’s one way to see how. A knows B can’t be the pardoned prisoner, because the guard said so. So there are two possibilities remaining: A is the pardoned prisoner. C is the pardoned prisoner. In the first scenario, where A has been pardoned, the guard had a choice between identifying B or C in her response. She chose to identify B, but she could have identified C instead. But in the second scenario, the guard had no choice. She had to name B, since she couldn’t tell A that she’s still condemned. So when the guard says “B is still condemned”, her report is exactly what you’d expect in the second scenario, where C was pardoned. Whereas in the first scenario, where A was pardoned, the guard’s report was half as likely. She could just as easily have named C instead. So the guard’s report fits twice as well with the second scenario. And that’s why A’s chance of survival is still 1/3. This is essentially the same solution we used for the Monty Hall problem. And we can use a diagram to visualize it. The king chooses one of the three prisoners at random, so each has the same 1/3 chance: Then we consider the guard’s options. If A is pardoned, she can choose to name either B or C. But if B is pardoned, she has to name C—otherwise she’ll give away A’s fate. And likewise, she has to name C if B is pardoned. So there are two possible outcomes where the guard names B: And look! One of them is half as likely as the other, namely the one where A was pardoned. So when the guard names B, A’s chance of having been pardoned is half that of still being condemned. In other words, her chance of survival is 1/3, versus a 2/3 chance of still being condemned. So A’s chances really do stay fixed. The guard’s report is no reprieve. It makes no difference to A’s chances, as seems right. "],
["monty-hall-again.html", "1.3 Monty Hall Again", " 1.3 Monty Hall Again Erdős left Vázsonyi’s house unsatisfied and unconvinced. But a few days later he called: someone had finally explained to his satisfaction why you should switch doors in the Monty Hall problem. “He proceeded to tell me the reasoning”, said Vázsonyi, “but I couldn’t fathom his explanation.” I guess we’ll never know what persuaded Erdős. But luckily we don’t need to, because the same technique we used on the three-prisoner puzzle works on Monty Hall. We can even use the same tree diagram, we just have to change the labels. The first branching-point is now the placement of the car, and each door has the same 1/3 chance. The second branching-point is Monty’s decision about which door to open. And just like the guard, his hand is forced in all but one case: Your initial choice was door #1, so if the car is behind door #2 Monty has to open door #3. Otherwise he’ll be showing you that you have the wrong door. Likewise, he has to open #2 if the car is behind #3. Only if the car is behind #1 does Monty have a choice. So there are two possible situations where Monty opens door #3. But one of them is half as likely as the other, namely the one where the car is behind door #1. So, when you see Monty open door #3, the car is probably behind door #2. Switch! The Total Evidence Requirement is clearly something to take seriously. To get the right answer, you have to take account of all the information you have. Monty isn’t just showing you that door #3 is a bust, he’s also likely giving something away with his choice of door to open. So it’s as important to consider how you get your information as it is to consider what information you got. These days there’s a lot of talk about the dangers filter bubbles. But the news has always been a selective window onto the world, because only some things sell. “Man Bites Dog” is news, as journalists say, while “Dog Bites Man” is not. Or, to take a more political and realistic example from the recent news, “Seven Die in Terrorist Attack on London Bridge” is news, “Thousands Die Quietly of Cancer” is not. "],
["the-gamblers-fallacy.html", "2 The Gambler’s Fallacy", " 2 The Gambler’s Fallacy My wife’s family keeps having girls. She’s one of three sisters (no brothers), and each sister has two daughters (no sons). That’s nine girls in a row! They’ve gone two generations with no boys yet. So family gatherings often turn to the obvious question: are we due for a boy next? Here are three different answers, each with a perfectly sensible looking rationale to back it up. Answer #1. Yes, the next baby is more likely to be a boy than a girl. Ten girls in a row is really unlikely. (Less than a tenth of a percent chance, if you want an exact number.) So the tenth baby will most likely be a boy. Answer #2. No, it’s even odds on boy vs. girl. A baby’s sex is determined by an isolated, random event. So it’s like a coin flip, a fifty-fifty shot every time. Answer #3. No, a girl is actually more likely! Girls run in the family, clearly. So although it could be a boy, in this family girls have the edge statistically speaking. Discussions like this tend to ignore the fact that a lot of people are born intersex. And ignoring intersex births has been the source of enough suffering in the world already, so let’s try to avoid that mistake. We’ll put our question this way: how does a string of nine girls in a row affect the chance of another girl? Do the odds go up? Down? Or do the stay the same? "],
["independence.html", "2.1 Independence", " 2.1 Independence It all hangs on whether the sex of each baby is independent of the others. Two events are independent when the outcome of one doesn’t change the probabilities of the other. A stock example of independence is sampling with replacement. Imagine an urn with 50 black marbles and 50 white ones. You draw a ball at random, put it back and give the urn a shake, and then draw again. Even if you draw white balls ten times in a row here, the odds of black-vs-white on the eleventh draw are still fifty-fifty. The draws are independent because you always put the marble from the previous draw back. That way every time you reach in the urn there just as many black balls as white ones. Now imagine the same exercise without replacement. Every time you draw a ball, you set it aside instead of putting it back. Now the draws are dependent. If you draw ten white balls in a row, the next draw is less likely to come up white. Now there are fifty black balls to just forty white ones. "],
["bias.html", "2.2 Bias", " 2.2 Bias Flips of an ordinary coin (like a Canadian Loony) also illustrate independence. Even if you get ten heads in a row, the eleventh toss is still fifty-fifty. That’s because ordinary coins—not the kind you get at a magic shop—are symmetric and evenly balanced. So each toss is… well, a tossup. If it’s really an ordinary coin, the initial ten heads in a row is just a coincidence. Coin flips aren’t just independent though, they’re also unbiased: heads and tails are equally likely. A process is biased if some outcomes are more likely than others. Like a loaded coin that comes up heads three quarters of the time is biased. "],
["fairness.html", "2.3 Fairness", " 2.3 Fairness So coin flips are both unbiased and independent, which makes them fair: fair = no bias + no dependence. Fair processes are important because nobody can get an edge. In a casino, the dice are fair, the roulette wheels are fair, the decks of cards are fair… And that means anybody who walks up to the roulette wheel or the craps table or the blackjack table can start gambling without any disadvantage. (How do casinos make money then? Well, in roulette for example, the payouts don’t match the odds of winning. The wheel is fair but the prices aren’t!) "],
["the-gamblers-fallacy-1.html", "2.4 The Gambler’s Fallacy", " 2.4 The Gambler’s Fallacy People sometimes forget that fair processes are independent, a mistake so tempting and common it has its own name: the gambler’s fallacy. If a roulette wheel comes up black five times in a row, some gamblers figure it’s “due” for red. If they get a bunch of bad hands in a row at poker, they figure they’re due for a good one. With a fair process, it’s unlikely for the same thing to keep happening over and over for a long time. If you flip a coin ten times in a row, you expect to get a mix of heads and tails. So when the same thing does happen many times in a row, people figure it has to change soon. But this way of thinking neglects independence! A fair process is also an independent process, by definition. It was really unlikely that you’d get a streak of ten heads in a row. But once the streak has happened, the eleventh toss is a fresh start—another fifty-fifty tossup. "],
["fallacies-vs-misfortunes.html", "2.5 Fallacies vs. Misfortunes", " 2.5 Fallacies vs. Misfortunes But wait: imagine you flip a coin a thousand times and it lands heads every time. Every. Damn. Time. Shouldn’t you at least be suspicious? It sure looks like something weird is going on, something that makes this coin land heads repeatedly. So shouldn’t you expect heads on the next flip? How could that be a fallacy? It’s not a fallacy! It’s perfectly good reasoning. It’s only a fallacy when you know the process is fair (or assume it is). And here, you doubt the process is fair, with good reason. The lesson: there’s a difference between a fallacy and misfortune. A fallacy is a logical error, a failure to correctly use the information you have. When you know a process is fair but neglect the independence that entails, that’s an error of logic. You should know better. But sometimes you just get bad information. If a fair coin really did land heads a thousand times in a row, you’d be forgiven for thinking it’s not actually fair. You’d be in the unfortunate position of having some misleading information—really misleading information in this example. (But don’t worry too much. Information this misleading is also really unlikely.) "],
["the-hot-hand.html", "2.6 The Hot Hand", " 2.6 The Hot Hand When a basketball player hits several shots in a row, they’re said to be on fire, which many people take seriously. They think the rest of the team should feed the ball to the player with the hot-hand because they’re more likely to make a shot. But a famous study published in 1985 found that a player’s shots are actually independent. Most people don’t know about that study, though. And certainly nobody knew what the result of the study would be before it was conducted! So a lot of believers in the hot hand were in the unfortunate position of just not knowing a player’s shots are independent. So the hot hand isn’t the same as the gambler’s fallacy. That doesn’t mean believers in the hot hand are off the logical hook, though. The same study also analyzed the reasoning that leads people to think a player’s shots are dependent. Their conclusion: people tend to see patterns in sequences of misses and hits even when they’re random. So there may be another fallacy at work. There might even be more than one. If players and fans want to think that “the zone” is a real place one can get into, then maybe they’re guilty of wishful thinking, too. Oh, one other thing. Some recent studies from Stanford and Harvard found that the hot-hand may actually be real after all! How can that be, what did the earlier studies miss? It’s still being looked into, but one possibility is: defense. When a basketball player gets in the zone, the other team ups their game. The hot player has to take harder shots. So the Harvard study added a correction to account for increased difficulty, and the Stanford study looked at baseball instead. Then they found evidence of streaks. “Applied statistics is hard,” as influential statistician Andrew Gelman once said. "],
["girls-revisited.html", "2.7 Girls Revisited", " 2.7 Girls Revisited So what about my wife’s family and their nine girls in a row? As best I can tell, the available evidence says girls running in the family isn’t a thing (or boys either). So the odds of a girl next are unchanged. Like the hot-hand though, most people don’t know about the research on this question. And that includes my in-laws. So even if they figure we’re due for a boy, they’re not guilty of the gambler’s fallacy. Are they guilty of a different fallacy though? Like the basketball fans who see non-existent patterns in random sequences of missed and hit shots? I’m going leave this one up to you, the reader. My in-laws might end up reading this book some day! "],
["the-taxicab-problem.html", "3 The Taxicab Problem", " 3 The Taxicab Problem In 1972, two psychologists who would go on to win a Nobel prize for their research into human reasoning, asked participants in one study the following question: A cab was involved in a hit and run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data: 85% of the cabs in the city are Green and 15% are Blue. A witness identified the cab as Blue. The court tested the reliability of the witness under the same circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. What is the probability that the cab involved in the accident was Blue rather than Green? It’s really tempting to say 80%, and most people answer something like that. But the right answer is much lower, almost by half: 41%. How could it be so low, when the witness gets each colour right 80% of the time? The short answer is: because there are so many more green cabs than blue. But that needs some explaining. True, the witness only mistakenly says “blue” 20% of the time when the cab is really green. But almost all the cabs are green, so his “blue” mistakes end up being fairly common. With so many green cabs rolling by, even mistaking only 20% of them for blue makes for a lot of false “blue” reports. Especially compared to the small number of blue cabs actually on the road. So a lot of the time when the witness says “blue”, the cab is really green. In fact, they’re wrong more often than they’re right when it comes to “blue” cabs. As usual, it’s way easier to see what’s going on with a diagram. So imagine there are just 100 cabs in town, 85 blue and 15 green. We’ll draw a square for each cab: The dashed blue line indicates the cabs the witness will identify as “blue”. It encompasses 80% of the blue squares and only 20% of the green squares. But with so many more green squares than blue ones, it ends up including more green squares than blue ones. We can also see where the 41% figure comes from now. There are 17 green squares in the dashed blue area; because that’s 20% of 85. And there are 12 blue squares in the dashed blue region; that’s 80% of 15. So all in all then there are 29 cabs the witness calls “blue” (17 + 12 = 29). But only 12 of those really are blue, and 12/29 is about 41%. "],
["conditional-probabilities.html", "3.1 Conditional Probabilities", " 3.1 Conditional Probabilities What makes the taxicab problem so confusing? Well for one, it’s easy to get these two things muddled: The chance the witness will say “blue” when a cab is blue: 80%. The chance a cab is blue when the witness says “blue”: 41%. Schematically, this is the difference between: blue \\(\\overset{\\small 80\\%}{\\Longrightarrow}\\) “blue” “blue” \\(\\overset{\\small 41\\%}{\\Longrightarrow}\\) blue. We’re being asked to calculate the second number in the taxicab problem, whereas it’s the first number that’s given to us. The problem says: The court […] concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. Which means they correctly identify blue cabs as “blue” 80% of the time, and likewise for green. But it’s not exactly super clear from the wording, so it’s easy to read it as saying “blue” \\(\\overset{\\small 80\\%}{\\Longrightarrow}\\) blue. But even once that’s sorted out, it’s still tricky. You might have assumed the order doesn’t matter: if it’s 80% in one direction, then it’s 80% in the other direction too, no? No. The numbers in each direction are actually independent. If you live in Toronto there’s a 100% chance you live in Canada, but not the other way around. Very few Canada-dwellers live in Toronto. Likewise, most humans are “pentadactyl”—they have five digits on each limb. But very few pentadactyl creatures are people. Just think of all the chimps, gorillas, cats, cows, and even bats! They’re all pentadactyl too. (Evolution is weird.) Once you’ve seen a few examples, a diagram helps illustrate the general point. Given two cases \\(A\\) and \\(B\\), they can overlap in all kinds of ways. We can have most of the As be Bs, but not vice versa: Or we can have most As be Bs and vice versa: In probability-speak, we’re learning crucial lessons about conditional probability: the chance of something under a certain condition. The probability the witness will say “blue” given that the cab really is blue is 80%. That’s usually written \\(Pr(W \\given B) = 80\\%\\): the probability of \\(W\\) (the witness saying “blue”) given \\(B\\) (the cab really being blue). Put another way, suppose for the moment the cab really is blue. How likely is it then the witness will say “blue”? 80%. The general format is: \\[ Pr(Q \\given P) = x \\mbox{ means the probability of $Q$, given $P$, is $x$.}\\] The \\(P\\) to the right of the \\(\\given\\) is the condition, the thing we are supposing for the moment is true. The \\(Q\\) to the left of the \\(\\given\\) is the thing we are considering the probability of, given \\(P\\) as an assumption. So in the taxicab problem we’re told \\(Pr(W \\given B)\\), the probability the witness will say “blue” given the cab really is blue. What we’re asked to figure out is \\(Pr(B \\given W)\\), the probability the cab really is blue given that the witness said “blue”. And using our diagram we found that was about 12/29, or about 41%. "],
["doctors-without-base-rates.html", "3.2 Doctors Without Base Rates", " 3.2 Doctors Without Base Rates So now you’re all trained up and immunized, right? You’ll never be one of those eighty-percent suckers again, right?? Turns out even people who should know better, people with extensive scientific and statistical training, fall prey to this fallacy. And that includes medical doctors, which you might find… concerning. Consider a relatively rare virus, like HIV. Very few North Americans have HIV and don’t know it, fewer than one in every thousand. Now imagine a highly accurate (and highly fictional) blood test. It always detects the presence of the virus, and only gives a false positive in a tenth of a percent of cases. If you take the blood test and you get a positive result, how worried should you be? The test is so accurate, it’s hard to see how there could be much room for optimism. But, of course, if it were that simple we wouldn’t be talking about it. Imagine taking a thousand random North Americans and giving them the blood test. The one person with HIV will test positive. But so will one of the 999 people who don’t have HIV. Because a tenth of a percent of 999 is basically one (0.999 if you want to be exact). So even if you test positive, there’s still a fifty-fifty shot you don’t have HIV. Out of every two positive tests, there’s one true and one false—so yours might be the false positive. One way of thinking about what’s going on here is that you have two pieces of relevant information. One the one hand, the test is highly accurate, which points to you having the virus. But on the other, very few people have the virus, which points the opposite way. So you have to work through the numbers to see how those two, conflicting pieces of information balance out. The tendency to focus on just the first piece of information, the accuracy of the test, is called “base rate neglect”. Because, well, the other piece of information, the rate of HIV in the overall population, is called the “base rate”. So doctors know better than to neglect the base right, right? Well, maybe not as much as you’d hope. In study after study, everyone from undergraduates with no statistical training, to medical students, to trained and experienced medical doctors seem prone to base rate neglect. For example, in one study of 160 gynecologists, only 21% got the right answer on a similar problem (breast cancer instead of HIV, mammograms instead of blood tests). But there is some good news. After a bit of training, 87% of those same gynecologists solved these problems correctly. Curiously, part of that training was one weird trick: translating the problem into natural frequencies. Instead of “the chance of a North American having HIV is a tenth of a percent”, try “one in every thousand North Americans has HIV”. Rather than frame things in terms of percentages or probabilities, frame them in terms of sets of individuals—a thousand North Americans, or a hundred taxicabs. That makes it much easier for humans to find the right answer, these psychologists say. You may have noticed this kind of natural frequency language in medical pamphlets, like at your doctors office. Because of this research on human reasoning, it’s increasingly common to explain the prevalence of some condition, or how to interpret a certain diagnosis, in natural frequency terms. And, you might have noticed, I used it too when I explained how to get the right answers! "],
["bayes-theorem.html", "3.3 Bayes’ Theorem", " 3.3 Bayes’ Theorem There’s also a formula you can use, and it’s such an important formula in so many fields that we should get to know it now. The formula tells you how to compute the probability of the hypothesis you’re interested in, given the evidence you’ve received. So let’s use \\(H\\) to stand for our hypothesis, and \\(E\\) for the evidence. Bayes’ Theorem \\[ Pr(H \\given E) = Pr(H)\\frac{Pr(E \\given H)}{Pr(E)} \\] We saw earlier that the direction matters with conditional probabilities. 100% of Torontonians live in Canada, but only some Canada-dwellers live in Toronto. So \\(Pr(H \\given E)\\) isn’t the same thing as \\(Pr(E \\given H)\\). One thing Bayes’ Theorem tells us, though, is that there is a relationship. You can get from \\(Pr(E \\given H)\\) to \\(Pr(H \\given E)\\), if you have some additional information. Specifically, you need to have: \\(Pr(H)\\), the probability of the hypothesis before the evidence \\(E\\) is taken into account. \\(Pr(E)\\), the unconditional probability of the evidence, the chance it would be true if you didn’t yet know yet whether \\(H\\) is true or not. And, it turns out, we have all that information in the kinds of problems we’ve been discussing in this chapter. Let’s apply Bayes’ Theorem to the taxicab example to illustrate. We want to know \\(Pr(B \\given W)\\), the probability the cab is blue (\\(B\\)) given the witness said it was (\\(W\\)). We know the reverse probability: \\(Pr(W \\given B) = 80/100\\), because the court found witness to be 80% accurate. So we just need… \\(Pr(B) = 15/100\\), because only 15% of cabs are blue, and \\(Pr(W) = 29/100\\), because we calculated the witness will identify 29 out of every 100 cabs as “blue” (17 of the green ones, 12 of the blue ones). So we can just plug all those numbers into Bayes’ theorem and get: \\[ \\begin{aligned} Pr(B \\given W) &amp;= Pr(B)\\frac{Pr(W \\given B)}{Pr(B)}\\\\ &amp;= 15/100 \\frac{80/100}{29/100}\\\\ &amp;\\approx .41 \\end{aligned} \\] This is actually the same calculation we used to solve this problem before. But now we know a general formula for doing the same calculation in any problem that has the same structure! That’s nice for a few reasons. One is that pictures and diagrams won’t always be manageable. Sometimes the numbers are just too large or too small. But also, once you have the formula you can program a computer to do the calculation for you! "],
["heuristics-biases.html", "3.4 Heuristics &amp; Biases", " 3.4 Heuristics &amp; Biases Daniel Kahneman and Amos Tversky are the two psychologists who first tried out the taxicab problem on experimental subjects in 1972. Ands they eventually won the Nobel Prize in economics for their work on human reasoning. They revolutionized the field, discovering numerous ways human reasoning violates the laws of probability theory and statistics, beyond just neglecting base rates. The research program they launched came to be known as the “heuristics and biases” program. The idea being that humans use heuristics—quick and dirty shortcuts—to make reasoning easier. But, although these shortcuts work pretty well, they don’t always. They result in certain biases, predictable patterns of error, like base rate neglect. Here are two more examples from their research. 3.4.1 The Bank Teller Fallacy This question is from a paper Kahneman and Tversky published in 1983: Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? Linda is a bank teller. Linda is a bank teller and is active in the feminist movement. Almost 85% of their subjects chose (2). But that can’t be right, and you don’t need to know anything about Linda to prove it. To be a feminist and a bank teller, you need to be a bank teller. So whenever (2) is false, so is (1). But not vice versa. Imagine throwing a dart randomly at a Venn diagram: You can’t hit the feminist bank tellers without hitting the bank tellers. So why do people choose (2)? Kahneman and Tversky suggested that people are relying on a “representativeness” heuristic. They’re gauging how much the description of Linda is representative of the class of bank tellers (not very) vs. the class of feminist bank tellers (more so). Ordinarily, representativeness is a decent guide to probability. But not always. 3.4.2 R is for Terror Where is the letter R more likely to occur in an English word, as the first letter or as the third letter Most people say first letter. But (of course) third position is actually more likely. (That’s not something you can just figure out by logic, mind you. You have to run a search on a dictionary and count.) So why do people tend to think first position is more likely? Kahneman and Tversky hypothesized another heuristic: “availability”. It’s easier to think of words that begin with R—they’re more readily available in memory. And the easier it is to imagine or recall an example of something, the more frequent it seems. "],
["social-distortions.html", "3.5 Social Distortions", " 3.5 Social Distortions Before moving let’s pause to reflect on the social and political implications of all this. If psychologists like Kahneman and Tversky are right, and our reasoning is distorted by heuristics like availability, then are those distortions reflected in the shape of our society? You could write whole books on this question, and people do. So I’ll just note one possible (and provocative) answer, represented in the following remark from psychologist Joshua D. Foster: the 2009 budget for homeland security (the folks that protect us from terrorists) will likely be about $50 billion. Don’t get us wrong, we like the fact that people are trying to prevent terrorism, but even at its absolute worst, terrorists killed about 3,000 Americans in a single year. And less than 100 Americans are killed by terrorists in most years. By contrast, the budget for the National Highway Traffic Safety Administration (the folks who protect us on the road) is about $1 billion, even though more than 40,000 people will die this year on the nation’s roads. That’s hardly the final word on this question. But I think it’s a decent place to start thinking about such matters. "],
["simpsons-paradox.html", "4 Simpson’s Paradox", " 4 Simpson’s Paradox In 1973, the University of California at Berkeley got worried about being sued for discriminating against women. In their graduate admissions that year, only 35% of female applicants had been admitted, compared to 44% of male applicants. A professor in the statistics department was recruited to look into it. He found that men and women actually had similar acceptance rates department by department. In fact, women had a higher acceptance rate across most of the larger departments. So how did women end up with a lower acceptance rate overall? Because, it turned out, women had applied more to the more selective departments. Many more women than men had applied to the English department, for example, which admitted only a small fraction of its applicants. That doesn’t mean sexism played no role here necessarily. You might wonder why women and men differed so much in their choices of where to apply, for example. And just because one group has a higher admissions rate doesn’t mean they aren’t being discriminated against. Harvard once put an admissions cap on Jews because they were being admitted in such large numbers! But it does mean the initial appearance of discrimination in Berkeley’s 1973 admissions was misleading. And it’s a great illustration of Simpson’s paradox, where a trend appears at one level of analysis yet reverses at a more fine-grained level. In fact, it’s even possible for one group to have higher admissions across the board, in every department, and still have a lower admission rate overall. Imagine a fictional case with just two departments. And let’s get all the information into a single chart by using width to display the number of applicants: The blue bars are taller than their partner red bars, and yet there’s much less blue area overall. So what’s the moral? There’s a kind of “part-whole” fallacy to watch out for with probabilities. What’s true of all the parts isn’t necessarily true of the whole. Just because a trend is present in every slice of a population doesn’t mean it’ll be present in the population overall. "],
["the-law-of-total-probability.html", "4.1 The Law of Total Probability", " 4.1 The Law of Total Probability There is a right way to do part-whole reasoning with probabilities, though! And it introduces us to a powerful companion principle to Bayes’ theorem from the last chapter. Consider this question: if 20% of all applicants to Chemistry are admitted, and 30% percent of applicants to Philosophy, what’s the overall admittance rate across these two departments? Well, it has to be somewhere between 20% and 30%. But what percentage exactly? It won’t necessarily be smack in the middle at 25%. Imagine 100 people apply to Chemistry and only 10 to philosophy. Then 23 people will be admitted out of 110. So the overall rate will be about 21%. In the reverse situation where 100 people apply to Philosophy and 10 to Chemistry, 32 people will be admitted out of 110, or about 29%. So the overall rate is closer to the departmental rate of the larger department. And if they’re equal sizes, then it will be smack in the middle at 25%. The Law of Total Probability describes this general pattern. A person’s chance of being admitted, \\(Pr(A)\\), is somewhere between their chances of being admitted to Chemistry if they apply there, \\(Pr(A|C)\\), and their chance of being admitted to Philosophy if they apply there, \\(Pr(A|P)\\): \\[ Pr(A|C) &lt; Pr(A) &lt; Pr(P). \\] (I’m assuming they applied to just one department, for simplicity.) But where exactly will \\(Pr(A)\\) fall in that interval? Well, the more likely it is they applied to Chemistry, the closer it will be to the left end. And the more likely it is they applied to Philosophy, the closer it’ll be to the right. Here’s another way to think about the same principle. Suppose you want to know whether \\(A\\) is true—whether you’ll get an A in your next philosophy class, for example. One way to get a handle on that is to break the question down into cases. Maybe the class will be boring, maybe not. We’ll use \\(B\\) for boring and \\(\\neg B\\) for not boring (the tilde symbol \\(\\neg\\) being a common way to symbolize negation). If the class is boring your chances of getting an A aren’t so good, let’s suppose. Wheres you’re likely to do well if it’s not boring. So the more likely it is the class is boring, the lower your chances of getting an A will be. That kind of reasoning is captured and made precise by: The Law of Total Probability \\[Pr(A) = Pr(A \\given B) Pr(B) + Pr(A \\given \\neg B) Pr(\\neg B).\\] In terms of our tree-diagramming technique, the LTP says to add up the final quantities from the \\(A\\)-and-\\(B\\) and \\(A\\)-and-\\(\\neg B\\) branches to get \\(Pr(A)\\). "],
["bayes-ltp-awesome.html", "4.2 Bayes + LTP = Awesome", " 4.2 Bayes + LTP = Awesome A big part of why the LTP is interesting is that it makes for a powerful combination with Bayes’ theorem. In fact, you often need the LTP to get to the point where you can apply Bayes’ theorem. Remember the problem of base rate neglect? In one example, we imagined a fictional blood test for HIV that always comes up positive when the virus is present. But real medical tests aren’t so dependable. They can generate false negatives as well as false positives. And the LTP helps us get the right answer in these more realistic cases. Here’s an example, still fictional, but more realistic: About 1 in every 100 people have Weisberg’s syndrome (an annoying but mostly harmless disease). There’s a blood test that’s 90% accurate: in 90% of cases where the disease is present, the test comes up positive, and in 90% of cases where it’s absent the test comes up negative. Suppose a randomly selected person tests positive. What are the chances they have the disease? Bayes’ theorem says that for any hypothesis \\(H\\) and piece of evidence \\(E\\): Bayes’ Theorem \\[ Pr(H \\given E) = Pr(H)\\frac{Pr(E \\given H)}{Pr(E)}. \\] We want to calculate \\(Pr(D \\given P)\\), the probability of having Weisberg’s disease given a positive blood test. So Bayes’ theorem says: \\[ Pr(D \\given P) = Pr(D) \\frac{Pr(P \\given D)}{Pr(P)}.\\] So we need the three numbers on the right hand side: \\(Pr(D) = 1/100\\), that’s the base rate given in the problem. \\(Pr(P \\given D) = 90/100\\), that’s the test-accuracy, also given. \\(Pr(P) = \\ldots\\) uh-oh, this number isn’t given in the description of the problem! You guessed it, LTP to the rescue! We can calculate the chance of a positive blood test, \\(Pr(P)\\), by breaking it into two cases. What’s the chance of a positive test if you really have the disease, and what’s the chance if you don’t. Then weigh each case according to the chance it’s true. In terms of the LTP, that means we calculate: \\[ \\begin{aligned} Pr(P) &amp;= Pr(P \\given D)Pr(D) + Pr(P \\given \\neg D)Pr(\\neg D)\\\\ &amp;= (90/100)(1/100) + (10/100)(99/100)\\\\ &amp;= 1,080/10,000\\\\ &amp;= 27/250. \\end{aligned} \\] Where did I get the 99/100 term from, you might be wondering? Well if 1 in 100 people have the disease then 99/100 people don’t. So the chance of a positive blood test is 27/250, which is about 11%. And now we have all three terms we need to apply Bayes’ theorem: \\[ \\begin{aligned} Pr(D \\given P) &amp;= 1/100 \\frac{90/100}{27/250}\\\\ &amp;= 0.08333\\ldots \\end{aligned} \\] Once again, the answer is pretty counterintuitive. Despite a test with 90% accuracy, there’s still a less than 10% chance you have the disease if you get a positive test. Base rates, don’t neglect ’em. But the new lesson is: Bayes’ theorem and the LTP together can solve some problems we wouldn’t be able to solve otherwise. In fact, you’ll often see Bayes’ theorem written with the LTP already built in to the denominator: \\[ Pr(H \\given E) = \\frac{Pr(E \\given H)Pr(H)}{ Pr(E \\given H) Pr(H) + Pr(E \\given \\neg H)Pr(\\neg H)}.\\] That’s ugly, but really useful if you’re doing these calculations on the regular. We’re more interested in the philosophical significance of Bayes’ theorem, though. And this messy version obscures a lot of philosophically interesting stuff, as we’ll see in the next few chapters. So we’ll stick to the simple version. "],
["the-raven-paradox.html", "5 The Raven Paradox", " 5 The Raven Paradox Generalization is an essential part of science. Without general principles, we couldn’t predict how things will turn out. Will this vaccine immunize you or make you sick? Does the rocket have enough fuel to make it into orbit? General principles of medicine and physics give us the answers here, if we can discover them. Generalities also help us understand the world. Why does your bicycle stay upright as long as the wheels are turning, then tip over when you stop? Why do rainbows appear after a rainstorm. General principles about momentum and electromagnetism illuminate these phenomena. But how does science verify a generalization? How do we establish that a principle is generally true? The obvious and classic answer: by verifying its instances. To establish that all electrons have negative charge, examine lots of electrons. To test whether all humans mortal, go and examine lots of humans. These reflections lead to the principle of scientific reasoning known as Nicod’s Criterion: Nicod’s Criterion A generalization is confirmed by each observed instance of it (unless you’ve already found a counterexample, of course). It’s a simple and plausible idea. So of course it leads to paradox. "],
["the-raven-paradox-1.html", "5.1 The Raven Paradox", " 5.1 The Raven Paradox In the mid-20th Century, one group of philosophers sought to discover the fundamental principles of scientific reasoning. They wanted a “logic of confirmation”, as Carl Hempel called it. Would Nicod’s Criterion be part of that logic? Maybe not: Suppose you’re an ornithologist testing the hypothesis that all ravens are black. You could go out looking for ravens, but you’re short on research funds. So instead you adopt the following, perverse method. You wander around campus looking for non-black things and verifying that they are not ravens: red shoes, brown walls, yellow pencils, green statues, etc. Eventually you bump into your department chair, who wants to know why you aren’t doing your research. “I am!”, you reply, “I’m just making clever use of Nicod’s Criterion to make the job easier.” “How in the world is that?!”, your chair asks in disbelief. Your cheery retort: “With each observation of a red shoe or a yellow pencil, I confirm the hypothesis that all non-black things are non-ravens. And that’s the same as proving the hypothesis that all ravens are black! Because the two hypothesis are logically equivalent.” Your chair is a logic nerd, it turns out, and so she recognizes you’re right about one thing here. The following two hypotheses are indeed logically equivalent: All ravens are black. All non-black things are non-ravens. Imagine collecting all the ravens in the world and all the black objects into a room, and placing them into designated circles: If all ravens are black, the only part of the raven circle with anything in it will be the part that overlaps with the black circle. The crescent region on the left will be empty: But that’s just what the second hypothesis says, too. If all the non-black things are non-ravens, then all the things outside the black circle must also be outside the raven circle. So again, the left crescent will be empty. And yet, your department chair flatly denies your request for more funding. She doesn’t even miss a beat; it’s a hard pass. “What gives?”, you protest. “You agreed these are equivalent hypotheses. Are you saying my observation only confirm one of them?” “No”, says your chair, “I agree with Carl Hempel’s famous Equivalence Condition: whatever confirms one of these hypotheses must confirm the other as well.” “So you’re questioning Nicod’s Criterion??” “No”, she replies, “not right now, anyway.” “So what’s the problem?” “Well, you are confirming that all ravens are black. But only very, very slowly. So slowly that it’s not worth doing. Each observation you make of a red shoe, or a yellow pencil, confirms the hypothesis. But by so little that it might as well be no support for the hypothesis at all.” What in the world is she talking about? "],
["the-hosiasson-lindenbaum-solution.html", "5.2 The Hosiasson-Lindenbaum Solution", " 5.2 The Hosiasson-Lindenbaum Solution It turns out your chair did a minor in philosophy, where she read about a little-known Polish logician named Janina Hosiasson-Lindenbaum, who was shot dead by the Nazis in 1942. But two years before that, she published an influential analysis of precisely the issue you and your chair are debating. She concluded that observing a red shoe does support the hypothesis that all ravens are black, but by a tiny, negligible amount. And, it turns out, philosophers nowadays generally agree that she was right. How could a red shoe have any bearing at all on ravens and their colouring? Well, any object you observed could, in principle, disprove your hypothesis. Each turn of your head could reveal (say) a white raven. When you see something else instead, like a red shoe, the hypothesis passes a sort of test. But it’s only a very weak sort of test. Most things you encounter aren’t black, and aren’t ravens. So you’ll probably turn your head to see something non-black, in which case it just has to be a non-raven for the hypothesis to avoid disproof. Since ravens aren’t a frequent sight, the hypothesis will probably pass this test. But if you go looking for ravens, then the hypothesis faces a more serious test. It says each raven you find will be black. Assuming you haven’t yet established anything about ravens’ colouring, the hypothesis could well turn out to be wrong. Some of the ravens you find could well be white. When you find they’re not, your hypothesis’ bona fides get a real, substantial boost. "],
["hats-grasshoppers.html", "5.3 Hats &amp; Grasshoppers", " 5.3 Hats &amp; Grasshoppers So is Nicod’s Criterion legit then? Your chair hinted at reservations, and for good reasons. Nicod’s Criterion seems plausible, and it applies just fine to lots of cases. But not to all cases. And that means it can’t be a fundamental scientific principle—part of the “logic of confirmation”. Here’s one example where it fails: Three philosophers go to dinner and check their hats at the door. After dinner the waiter returns their hats, and philosopher A notices she has B’s hat. Now consider this general statement: each philosopher has been given the wrong hat. Will this be confirmed or disconfirmed if B turns out to have A’s hat? The answer: it will be disconfirmed, even though this would be a positive instance of the general claim. If B finds that he has A’s hat, then that’s a case of getting the wrong hat. But it also means C will definitely get the right hat, since hers is the only hat left unaccounted for. I like that example because it’s tidy. It’s easy to keep track of just three hats in your head, and the consequences are perfectly certain. C must get her own hat if A and B have been given each other’s. But the tidiness here comes at the price of realism, as it so often does. So here’s a less tidy, and slightly more realistic, example: An entomologist friend of yours tells you about a remote island in the Atlantic Ocean, Pitcairn Island, which has no grasshoppers. In other words: all the grasshoppers in the world live outside of Pitcairn Island. A few years later, you and your friend take a trip to a neighbouring island, where you see many grasshoppers. You express some doubts about your friends prior assurances. But he insists that these are positive instances of his hypothesis: they are examples of grasshoppers living outside of Pitcairn Island. But, to settle the matter, he agrees to take the ferry with you over to Pitcairn Island. At the docks, the ferry arrives from Pitcairn and a horde of grasshoppers disembarks, swarming past you. “More confirmation of my hypothesis!”, your friend declares. “A whole swarm of grasshoppers outside of Pitcairn Island!” I hope you will agree that your friend’s scientific credentials might now be in doubt. "],
["the-grue-paradox.html", "6 The Grue Paradox", " 6 The Grue Paradox A lot of science works by extrapolating from observed patterns. Pollsters survey a sample of voters to gauge the leaning of the electorate as a whole, for example. If 54% of respondents prefer Candidate X, they conclude that 54% of all voters (give or take) prefer X. Climate scientists extrapolate from historical trends to estimate future trends. The planet has been steadily warming for the last 100 years. That’s one reason to expect it will continue to warm in the coming years. A fundamental principle of scientific inquiry seems to be something like: expect the unobserved to resemble the observed. Philosophers call this the Principle of Induction. "],
["a-gruesome-concept.html", "6.1 A Gruesome Concept", " 6.1 A Gruesome Concept But one philosopher, Nelson Goodman, identified a difficult puzzle about the principle of induction. It revolves around a bizarre concept of his invention, which he dubbed “grue”. There are two ways for an object to be grue. Some green things are grue, but only some. It depends on when we first encounter them. If our first observation of a green object happens before the year 2020, then it’s grue. So the Statue of Liberty is grue: it’s a green object that was first observed before the year 2020 (long before). But if our first encounter with a green object is in the year 2020 or later, then it’s not grue. Likewise if we never observe it—because it’s on the far side of the universe, for example, or buried deep underground. I said there are two ways for an object to be grue: some blue objects are grue, too. Not the ones observed before 2020, though. Instead it’s the ones that aren’t observed before 2020. If a blue object is observed for the first time after 2019, or it’s never observed at all, then it’s grue. So blue sapphires that won’t be mined before the year 2020 are grue. As usual, it helps to have a diagram: We can also define ‘grue’ in explicit, verbal terms: Grue An object is grue if EITHER (a) it is green and first observed before the year 2020, OR (b) it’s blue and not observed before 2020. To test your understanding, see if you can explain why the following are examples of grue things: the $20 bill in my pocket, Kermit the Frog, the first (Canadian) $5 bill to be printed in 2020, the first sapphire to be mined in 2020, and blue planets on the far side of the universe. Then see if you can explain why these things aren’t grue: fire engines, the Star of India, and the first $20 bill to be printed in 2020. Once you’ve got all those down, try this question: do grue objects change colour in the year 2020? It’s a common confusion to say they do. But no, grue objects don’t change colour. The Statue of Liberty is green and (let’s assume) it always will be. So it’s grue, and always will be, because it’s a green thing that was first observed before the year 2020. Part (a) of the definition of ‘grue’ guarantees that. The only way time comes into it is in determining which green things are grue (and which blue things). If a green thing is first observed before 2020, then it’s grue, ever and always. Likewise, if a blue thing is not first observed before 2020, then it’s grue, and always has been! "],
["the-paradox.html", "6.2 The Paradox", " 6.2 The Paradox So what’s the big deal about grue? Well ask yourself whether you’ve ever seen a grue emerald. I have. In fact, every emerald I’ve ever seen has been grue. And the same goes for every emerald you’ve ever seen. Every emerald anyone has ever seen has been grue. Why? Because they’re all green. And they’ve all been observed before the year 2020 (it’s 2017 as I write this). So they’re all grue the first way—they all satisfy part (a) of the definition. (Notice it’s an EITHER/OR definition, so you only have to satisfy one of the two parts to be grue.) So all the emeralds we’ve ever seen have been grue. Let’s apply the Principle of Induction then: All observed emeralds have been grue. Therefore all emeralds are grue. But if all emeralds are grue, then the first emeralds to be mined in 2020 will be grue. And that means they’ll be blue! Because they won’t have been observed before 2020, so the only way for them to be grue is to be blue. So there are blue emeralds out there, just waiting to be pulled out of the earth! Uh-oh, something has definitely gone off the rails here. But what? Another way put the challenge: we have two “patterns” in our observed data. The emeralds we’ve seen are uniformly green, but they’re also uniformly grue. We can’t project both these patterns into the future, though. They contradict each other starting in 2020. Now, the green pattern is the real one, obviously. The grue “pattern” is bogus, and no one but a philosopher would even bother thinking about it. So why is it bogus? What’s so special about green? Apparently the Principle of Induction has a huge hole in it! It says to extrapolate from observed patterns. But which patterns? Patterns are cheap, as any data scientist will tell you. Given a bunch of data points on an xy-plane, there are lots of ways to connect the dots. Even if they all lie on a straight line, you could draw an oscillating curve that passes through each point, or even a much wilder (and sillier) curve. Fun fact: deciding which patterns to use and which to ignore is a big part of what machine learning experts do. And it’s one reason humans are still essential to designing artificial intelligence. Thanks to our experience, and our genetic inheritance, we have lots of information about which patterns are likely to continue, and which are bogus “patterns” like grue. But how do we pass all that wisdom on to the machines, so that they can take it from here? How do we tell them the difference between green and grue? "],
["disjunctivitis.html", "6.3 Disjunctivitis", " 6.3 Disjunctivitis Here’s one very natural reply. The problem with grue is it’s a disjunctive concept: it’s defined using EITHER/OR. It suffers from “disjunctivitis”. But the beauty of Goodman’s puzzle is the neat way it exposes the flaw here. It allows us to make ‘green’ the disjunctive concept instead! How? Start by building grue a friend, a concept to fill in the missing spaces in our original diagram. We’ll call it “bleen”: Here’s how you’d define green in terms of grue and bleen then: Green An object is green if EITHER (a) it’s grue and first observed before the year 2020, OR (b) it’s bleen and not observed before 2020. Now maybe you’re thinking: you could define green that way, but that’s not how it’s actually defined. In reality, we already understand the concept of green, and we have to learn the concept of grue from its disjunctive definition. The problem is, that’s just a fact about us humans, not about grue vs. green. That’s just the way we homo sapiens happen to be built (or maybe socialized, or both). Some bizarre species of alien could grow up thinking in grue/bleen terms instead. And when they landed on Earth, we’d have to explain our green/blue language to them using an EITHER/OR definition. Then they would be looking at us thinking: you guys have a very weird, disjunctive way of thinking! What could we say to them to establish the superiority of our way of thinking? It’s been more than 70 years since Goodman first posed this challenge. Yet no answer has emerged as the clear and decisively correct one. "],
["time-dependence.html", "6.4 Time Dependence", " 6.4 Time Dependence Another common reply to Goodman’s challenge is to say that ‘grue’ is defective because it’s time-dependent. It means different things depending on the time an object is first observed. But the same reversal of fortunes that toppled the “disjunctivitis” diagnosis happens here. We can define green in terms of grue and bleen. And when we do, it’s green that’s the time-dependent concept, not grue. So we’re left in the same spot. We need some way of showing that the “true” order of definition is the one we’re used to. By what criterion can we say that green is more fundamental, more basic, than grue? "],
["the-moral.html", "6.5 The Moral", " 6.5 The Moral Though it may seem just cute or merely a curiosity, Goodman’s puzzle is actually profound. I see it as one of the deepest and most troubling problems we’ll encounter in this book, in fact. The reason why will become clearer soon, especially in the next chapter. But to give you the gist of what we’re dealing with, here is the lesson I (and many others) take from the grue paradox. Whatever the logic of science is, it cannot be written down or summarized in a few, simple principles. The Principle of Induction is barely even a sketch waiting to be filled in. And it can’t be filled in, at least not with the tools we presently have, like logic and probability. Which observed patterns should we expect to apply in general? Which ones should we tell the machines to extrapolate from, and which to ignore? We know intuitively how to answer this question in many cases. We know that green is legit while grue is bogus. But we have no way of making this tacit knowledge explicit or general, so that it can be programmed into a machine. "],
["bertrands-paradox.html", "7 Bertrand’s Paradox", " 7 Bertrand’s Paradox Wait, wait, wait: what are statistics textbooks filled with then, if not principles of good scientific reasoning? Aren’t there well established laws of probability? Can’t scientists just follow those and not worry about all this gruesome nonsense? You would think so, but no. "],
["the-cube-factory.html", "7.1 The Cube Factory", " 7.1 The Cube Factory Consider this example from the influential philosopher of science Bas van Fraassen: A factory makes cubes whose sides are always between 1 and 3 feet long. What is the probability the next cube they make will have sides between 1 and 2 feet long? The obvious answer is 1/2. The 1-to-2 range is half of the 1-to-3 range, so half the time you’d expect cubes whose sides are between 1 and 2 feet long, as opposed to between 2 and 3 feet long. But now consider this question: what’s the probability the volume of the cube will be between 1 and 8 cubic feet? The answer is a bit less obvious now, but with just a little arithmetic we get 7/26. How? Well, the range of possible volumes in cubic feet is \\(1^3\\) to \\(3^3\\), which is \\(1\\) to \\(27\\). And we want the chance the actual volume will be between \\(1\\) and \\(8\\). So that’s 7/26 of the total range: \\((8-1)/(27-1) = 7/26\\). But here’s the kicker: these two questions are actually one and the same, yet we got two different answers. How are they the same? Because a cube having sides between 1 and 2 feet long is equivalent to having volume between 1 and 8 cubic feet. If the sides are 1 foot long, the volume is 1 cubic foot. If the sides are 2 feet long, the volume is 8 cubic feet. So sides 1-to-2 feet = volume 1-to-8 cubic feet. So we have two answers to the same question. What’s the chance the next cube will be between 1 and 2 feet on a side? Put another way: that it will have volume between 1 and 8 cubic feet? We got 1/2 on our first go, but only 7/26 on the second. Which answer is right? "],
["the-principle-of-indifference.html", "7.2 The Principle of Indifference", " 7.2 The Principle of Indifference If you’re thinking we must have screwed up our arithmetic somewhere, you can go back and check. But I promise you, that’s not the cause of the problem. It’s something else, a famous principle we relied on without mentioning explicitly: The Principle of Indifference If all you know is there are \\(n\\) possibilities, each possibility has the same probability: \\(1/n\\). If all you know is that a coin has two sides, each has probability 1/2. If all you know is a die has six sides, each has probability 1/6. When the range of possibilities lies on a continuum—like if the length of a cube’s sides has to be somewhere between 1 and 3—there’s an extension of this basic idea. The Principle of Indifference II If the range of possible outcomes is an interval of size \\(x\\), and you have no information about where in that interval the truth lies, the probability of any subinterval of length \\(y\\) is \\(y/x\\). The cube factory example demonstrates a problem with this principle: it gives contradictory results depending how we frame the question. If we think about things in terms of the lengths of the sides, then \\(x = 2\\) and \\(y = 1\\), so the probability is \\(1/2\\). But if we think in terms of volume instead, then \\(x = 26\\) and \\(y = y\\), so the probability is \\(7/26\\). The problem isn’t in the arithmetic. It comes from a curious—but uncontroversial!—mathematical fact. The same set of possibilities can be represented on different scales (side-length vs volume), and its relative size on each scale can be different. The volume scale is the cube of the side-length scale, and cubing numbers results in larger increases for larger numbers. So cubing the 1-to-2 range increases its length, but cubing the 2-to-3 range increases it even more. The Principle of Indifference says to use the size of a range of possibilities to determine its probabilities. But size on which scale? That’s the problem. "],
["how-widespread-is-the-problem.html", "7.3 How Widespread is the Problem?", " 7.3 How Widespread is the Problem? The cube factory example isn’t just some rarified edge case, sadly. This problem shows up everywhere. There’s always more than one way to frame these questions. Take a train trip, for example. Maybe all you know is that the trip is 60 miles long, and the train always arrives somewhere between 11:50 and 12:10. If you apply the Principle of Indifference to that 20 minute interval, you’ll think it’s fifty-fifty whether the train will arrive by noon. But if instead you apply the principle to the train’s average speed, you’ll get a different answer. Even simple seeming “finite” situations aren’t immune to the problem. Suppose I tell you my friend is buying a car today, either a Honda or a Toyota. So it’s 1/2 probability for each, right? Except that he’s considering two models of Honda: Civic and Accord. Ok, so then it’s 1/3 each. Or is it 1/4 for each model of Honda, and 1/2 for Toyota? And what about all the different colours he might buy? And the prices he might pay? And what time will the purchase be completed? We can keep dividing up the space of possibilities finer and finer, without limit. A finite number of possible outcomes can always be infinitely divided. So, at some point, we’ll need to apply the Principle of Indifference to a continuous range of possibilities, to an interval of some length. And what length? Well, that will depend, once again, on how we frame the situation. "],
["the-laws-of-probability.html", "7.4 The Laws of Probability", " 7.4 The Laws of Probability The Principle of Indifference goes back hundreds of years. The greats who created our modern theory of probability, like Thomas Bayes and Pierre-Simon Laplace, relied on it heavily. You won’t find it in a modern math or stats textbook, though. Problems like the cube factory were well known by the late 1800’s, thanks to the mathematician Pierre Bertrand (though he used different examples). So by the time the laws of probability were codified in the 1930’s, the Principle of Indifference was on the outs. What will you find in a modern textbook then? Just three, simple rules. First, probabilities are always numbers between 0 and 1. This is barely even a rule, really more agreeing on a scale—like labeling a volume dial from 0 to 11. Except probabilities only go up to 1. (But check this out.) Second, if something must happen, then its probability is 1. If that sounds obvious, that’s kind of the point. This second law just establishes 1 as the top of the scale. (You could actually do things upside down and make 0 the “top” of the probability scale! But it’d be really counterintuitive to work that way.) Third and finally, if two possibilities are mutually exclusive—if they can’t both happen—then the chance one or the other will happen is the probability of the first plus the probability of the second. The chance a fair die will land either three or five is 1/6 + 1/6 = 2/6, for example. Amazingly, you can derive all kinds of interesting and sophisticated things about probability from just these three, simple rules. In fact, you can basically derive the whole mathematical theory! (You have to soup up the third law a bit for some of the more advanced results. It has to be extended to cases with infinite possibilities instead of just two. But it’s really just the same idea applied to a larger number of things.) But just as important is what these laws don’t say. For example, they don’t say heads and tails are equally likely when you have no information about a coin flip. They say the chance of heads is some number between 0 and 1, just as the chance of tails is. They also say the chances of heads and tails have to add up to 1. But that doesn’t mean they have to be equal. They could be 1/3 and 2/3, or 1/10 and 9/10, or even 0 and 1! Without the Principle of Indifference, the laws of probability are kind of empty. They tell you that if the chance of heads is 1/2, then the chance of tails has to be 1/2. But they don’t tell you the chance of heads is 1/2. It could be anywhere from 0 to 1 for all the textbooks say. That’s the price of abandoning the Principle of Indifference. "],
["the-problem-of-priors.html", "7.5 The Problem of Priors", " 7.5 The Problem of Priors So what probabilities should we start with when doing a scientific study or statistical analysis? This is known as the problem of priors. Recall Bayes’ theorem: \\[ Pr(H \\given E) = Pr(H) \\frac{Pr(E \\given H)}{Pr(E)}. \\] The quantity \\(Pr(H)\\) is called the prior probability of \\(H\\), because it’s the probability before we get information \\(E\\). Whereas \\(Pr(H \\given E)\\) is called the posterior probability, the probability of \\(H\\) after we get information \\(E\\). To calculate \\(Pr(H \\given E)\\) using Bayes’ theorem, you need prior probabilities like \\(Pr(H)\\) (and also \\(Pr(E)\\) and \\(Pr(E \\given H)\\)). There was a time when mathematicians would have happily relied on the Principle of Indifference to settle those prior probabilities. But thanks to Bertrand’s Paradox, the Principle of Indifference lost its respectability. And now there’s no accepted recipe for settling prior probabilities. "],
["grue-all-over-again.html", "7.6 Grue All Over Again", " 7.6 Grue All Over Again Bertrand’s paradox and the grue paradox from the previous chapter are suspiciously similar. For one thing, they are both problems of “language dependence”. If you grew up speaking and thinking in terms of grue and bleen, you’d apply the Principle of Induction very differently from the way you actually do. You’d think all emeralds are grue. And you’d expect the first emeralds mined in the New Year to be a different colour (specifically, blue). Similarly, the probabilities we get from the Principle of Indifference depend on the language in which we frame a problem like the cube factory. If we think about the size of the next cube in terms of length, we get a probability of 1/2. But if we think in terms of volume we get 7/26. But also, both paradoxes are manifestations of the problem of priors. They both challenge us to say, in a principled way, how likely each possibility is to start with. In the case of the grue paradox, we need some principled reason for saying it’s intrinsically more likely that all emeralds are green than that they’re all grue. If we can’t say why “all green” is inherently more plausible than “all grue”, then it doesn’t matter how many green emeralds we’ve observed up to now. Both hypothesis fit our observations perfectly: they both say all emeralds observed before 2018 will be green. To break the standoff, we need some criterion for favouring the green language over the grue one. "],
["where-this-leaves-us.html", "7.7 Where This Leaves Us", " 7.7 Where This Leaves Us Where does all this leave us? Not in a great place. The probabilities we start with determine the fate of science. If we start with sensible probabilities, we get sensible conclusions—like that all emeralds are green. Otherwise we get nonsense: all emeralds are grue. But Bertrand’s paradox overturned what we thought was the sensible way of establishing prior probabilities. The Principle of Indifference was supposed to tell us what priors to start with, until it ran smack into contradictions like the cube factory example. So the problem of priors is nasty indeed. And, I’m sorry to tell you, it has no accepted solution. There is a hacky sort of workaround, which is widely used across the sciences. And though it’s taught to every aspiring young scientist as the accepted method, the fact that it’s a hacky workaround is not usually mentioned. Probably because a lot of people don’t see it that way. So let’s see what the method is, and you can decide for yourself. "],
["lindleys-paradox.html", "8 Lindley’s Paradox", " 8 Lindley’s Paradox So how do scientists test their theories? How do they decide when an experiment supports a hypothesis, and when it refutes it? The short answer: they ask whether the results would be too much of a coincidence if the hypothesis were true. "],
["coincidence.html", "8.1 Coincidence", " 8.1 Coincidence Suppose the government starts minting a new, three dollar coin. We take one and flip it ten times. It lands heads every time. That would be a big coincidence if the coin were fair. Too much of a coincidence, in fact. The chances of getting ten out of ten heads with a fair coin are less than one in a thousand. So the hypothesis that the coin is fair has been tested, and it has failed that test. From our little experiment, we conclude that the coin is not fair, but biased. Another example: in a test of a new cancer drug, 90% of patients who take the drug are cured, compared to only 10% in the placebo group. That, again, would be a big coincidence if the drug were ineffective. Too much of a coincidence. So the hypothesis that the drug is ineffective has been tested, and it has failed the test. We conclude instead that the drug has beneficial effects. "],
["the-general-formula.html", "8.2 The General Formula", " 8.2 The General Formula Here’s the general formula for this method: State the hypothesis you want to test—that the coin is fair, that the drug is ineffective, etc. This is called the null hypothesis. (Because, traditionally, the default assumption is that the data are random, with no pattern.) Gather your data—flip the coin ten times, administer the drug to a group of patients, etc. Imagine the hypothesis is true, and consider how likely the data you’ve observed would be if it were true. If the answer is “very unlikely”, then reject the hypothesis. How unlikely is “very unlikely”, exactly? How much of a coincidence is too much of a coincidence? Traditionally the cutoff is 5% or less. (We’ll come back to the reason behind this tradition soon.) When the results fall in the range below the cutoff, they’re called statistically significant. And the method we’ve just described is called significance testing. "],
["what-about-bayes.html", "8.3 What About Bayes?", " 8.3 What About Bayes? You might be wondering what happened to Bayes’ theorem and Bertrand’s paradox here. Did we somehow just magically solve the problem of priors? No. We didn’t calculate \\(Pr(H \\given E)\\), instead we’ve been focusing on \\(Pr(E \\given H)\\). (Well, sort of. There’s a wrinkle here we’ll come back to later.) When \\(H\\) is a definite statistical hypothesis, you can calculate the probability of each outcome under the assumption that \\(H\\) is true. If our hypothesis is that a coin is fair, you can calculate the probability of getting ten out of ten tails in a row. Similarly, if our hypothesis is that each patient has only a 10% chance of being cured, we can calculate the probability that 90% of the patients in our test group will be cured. So we can calculate \\(Pr(E \\given H)\\) when \\(H\\) is an explicit and exact statistical hypothesis. The idea behind significance testing is to decide whether to reject \\(H\\) just on the basis of \\(Pr(E \\given H)\\). (Well, again, sort of: wrinkle.) If it’s too low for \\(H\\) to be believable, then we reject \\(H\\). "],
["normal-approximations.html", "8.4 Normal Approximations", " 8.4 Normal Approximations But how exactly do we calculate these probabilities? How do we determine when the results fall below the 5% cutoff? Computers do a lot of this work nowadays, especially in complicated cases. But there’s a shortcut for doing it by hand, at least in simple cases like coin tosses. And it gives you a much deeper understanding of significance testing, so let’s see how it goes. Suppose I flip a coin 100 times and it lands heads 75 of those times. Is that suspicious? Should we wonder whether it’s really fair? We need to figure out how unlikely it is to get 75 or more heads when you flip a fair coin 100 times. This would be tedious to do with perfect precision, but there’s a trick for getting a good approximation. If you flip a fair coin 100 times, the most likely outcome is 50 heads and 50 tails. A bit less likely is 51 heads, or 49 heads. A bit less likely still is 52 heads, or 48 heads. And so on. The overall pattern looks like this: Notice how much it resembles the famous “bell curve” shape: In fact they line up almost perfectly: So we can use a bell curve to approximate the probabilities we’re after. But how does that help? Because bell curves have some handy mathematical properties. "],
["characterizing-a-bell-curve.html", "8.5 Characterizing a Bell Curve", " 8.5 Characterizing a Bell Curve Fun fact: a bell curve is completely characterized by just two numbers. First is the center of the bell, called the mean, or \\(\\mu\\) for short. In our example \\(\\mu = 50\\). That’s the most likely outcome for \\(100\\) flips of a fair coin. The general formula is: \\[\\mu = np.\\] Here \\(n\\) is the number of tosses, \\(100\\) in our example. And \\(p\\) is the probability of heads on each toss, \\(1/2\\) in our example. So \\(\\mu = np = 100 \\times 1/2 = 50\\). Second is the width of the bell, called the standard deviation, or \\(\\sigma\\) for short. The general formula for \\(\\sigma\\) is a bit mysterious: \\[\\sigma = \\sqrt{np(1-p)}.\\] There’s a good-but-complicated mathematical reason behind this formula, so let’s just take it for granted. In our example \\(\\sigma = \\sqrt{np(1-p)} = \\sqrt{100 \\times 1/2 \\times 1/2} = 5.\\) Now that we know where the bell is centered (\\(\\mu = 50\\)), and how wide it is (\\(\\sigma = 5\\)), we can estimate how probable various outcomes are. How? Well, i’s a handy mathematical fact that, 95% of the time, the result of our coin flipping experiment will be within two “standard deviations” of the mean. In other words, we should expect the number of heads to be in the \\(\\mu \\pm 2 \\sigma\\) range 95% of the time, if the coin really is fair. In our example, \\(\\mu \\pm 2 \\sigma\\) is \\(50 \\pm 10\\), so the “95% of the the time” range is from 40 to 60 heads. But we got 75 heads. So that’s a pretty big coincidence if the coin really is fair, the kind of thing that would happen less than 5% of the time. So our result is statistically significant! The hypothesis that the coin is fair should be rejected, according to the method of significance testing. "],
["the-68-95-99-rule.html", "8.6 The 68-95-99 Rule", " 8.6 The 68-95-99 Rule In general, the math of bell curves works like this. If we go 5 heads out from 50, the probability is less than 68%. That’s one “standard deviation”, or \\(1 \\times \\sigma\\). If we go 10 heads out, which is two standard deviations (\\(2 \\times \\sigma\\)), the probability is less than 95%. And if we go 15 heads out, which is three standard deviations or \\(3 \\times \\sigma\\), the probability is less than 99%. So, in general, we can get a good sense of how surprising a result is by the following rule. If it falls within \\(\\sigma\\) of the mean \\(\\mu\\), it’s not surprising; that’s what you’d expect most of the time, about 68%. If it falls outside that range, it’s a little surprising; you’d expect that to happen sometimes, about a third of the time (100% – 68% = 32%). But if it falls outside that range, then it’s quite surprising. You’d expect that to happen rarely, only about 5% of the time (100% – 95% = 5%). And if it falls outside even that range, then it’s very surprising. You’d expect that to happen very rarely, less than 1% of the time. "],
["another-example.html", "8.7 Another Example", " 8.7 Another Example It takes practice to get the hang of all this, so let’s do another example. This one is from a book by Ian Hacking: VisioPerfect has a run of 2,400 light bulbs. According to the company’s publicity, 96% of the bulbs manufactured by its production process are long life… The monthly magazine Consumers’ Advocate asserts that it tested a run of 2,400 bulbs—400 six packs—from VisioPerfect. It found 133 short-life bulbs. (Hacking, p. 205) Is this a statistically significant result? Should we reject the claims of VisioPerfect’s publicists? To find out, we start by calculating \\(\\mu\\) and \\(\\sigma\\). And for that we need \\(n\\) and \\(p\\). In this example \\(n = 2,400\\): that’s the size of our sample of bulbs. And \\(p = .04\\): that’s the chance of getting a short-life bulb according to the hypothesis we’re testing. So using the formula from earlier, \\(\\mu = np = 2,400 \\times .04 = 96\\). And \\(\\sigma = \\sqrt{np(1-p)} = \\sqrt{2,400 \\times .04 \\times .96} = 9.6\\). Now, we observed 133 short-life bulbs. And according to the 68-95-99 rule, that’s a statistically significant result if 133 is more than two standard deviations away from the mean. In other words, if it’s outside the \\(\\mu \\pm 2 \\sigma\\) range. And in this case \\(\\mu + 2\\sigma = 115.2 &lt; 133\\), so the result is significant. According to the method of significance testing then, we should reject VisioPerfect’s advertised claim. "],
["warning-danger-ahead.html", "8.8 Warning: Danger Ahead", " 8.8 Warning: Danger Ahead Significance testing is confusing stuff. Trained scientists, and even statisticians, often misunderstand it. And they misapply it in real studies as a result. In fact, the American Statistical Association recently released a statement to clarify the idea and prevent its misuse. So what does it really mean for a result to be “statistically significant”? It means that if the null hypothesis is true, then a result this unusual was less than 5% likely to occur. But that doesn’t mean the chance the null hypothesis is true is now less than 5%! Making that leap would confuse \\(Pr(E \\given H)\\) with \\(Pr(H \\given E)\\). It also doesn’t mean the true hypothesis is significantly different from the null hypothesis. Imagine a coin that’s just slightly biased towards heads—maybe it comes up heads 51% of the time. With enough flips, you could establish that the coin isn’t fair, with statistical significance. But that doesn’t mean it’s significantly biased. It’s barely biased at all: 51% rather than 50%. The “significance” terminology isn’t great—it may even be worse than “null hypothesis”. But let’s try not to get hung up on preferred nomenclature. It’s the underlying logic of significance testing that concerns us here. "],
["significance-subjectivity.html", "8.9 Significance &amp; Subjectivity", " 8.9 Significance &amp; Subjectivity Let’s tackle a big question we’ve been avoiding: why is the cutoff 5%? Nothing magical or special happens when a result crosses that line. It’s just easy to calculate using the bell curve trick we just learned. In fact, although 5% is the cutoff commonly used in social sciences like psychology and sociology, different areas of research have different customs. A cutoff of 1%, or even 0.1%, is common in some physical sciences. Come to think of it, does the idea of a fixed cut off make sense at all? Sometimes a result is really far out there—the kind of thing you’d happen to expect less than one time in a thousand—and yet the null hypothesis shouldn’t be rejected. If you’re playing poker at a major casino and you’re dealt a straight, that doesn’t mean the deck is stacked in your favour. You know casinos are tightly regulated; they use carefully calibrated automatic shufflers; and they want you to lose, not win! Some statisticians, called “Bayesians”, see the whole significance testing method as misguided. When we calculate significance levels, we’re basically calculating \\(Pr(E \\given H)\\): the probability of the result if the hypothesis is true. But Bayes’ theorem tells us this is just part of the information we need to calculate what we really want, namely \\(Pr(H \\given E)\\). We want to know how plausible the hypothesis is given our results. But for that, we need \\(Pr(H)\\) and \\(Pr(E)\\): \\[ Pr(H \\given E) = Pr(H) \\frac{Pr(E \\given H)}{Pr(E)}. \\] Take the casino example. The null hypothesis \\(H\\) is that the cards are being dealt at random. In this kind of case, \\(Pr(H)\\) is very high: major casinos use standard decks and automated shuffling machines, and they’re very tightly regulated. That’s why, even though \\(Pr(E \\given H)\\) is low—it’s very unlikely to get a straight when cards are being dealt at random—\\(Pr(H \\given E)\\) is still high. If you’re basically certain the dealing is fair, you’ll still be confident it’s fair even if you’re dealt a straight on your first hand. So Bayesian statisticians see significance testing as a crude way of approximating Bayes’ theorem. Significance testing only looks objective on the surface, they say, because we’re pretending \\(Pr(E \\given H)\\) is the only term in Bayes’ theorem that matters. But other terms like \\(Pr(H)\\) do matter, as the casino example shows. So the choice of cutoff is subjective, based on a personal judgment about \\(Pr(H)\\). This criticism is sharpened by a famous problem known as Lindley’s paradox. "],
["lindleys-paradox-1.html", "8.10 Lindley’s Paradox", " 8.10 Lindley’s Paradox Imagine you run a flower store, and you receive a large shipment of tulip bulbs. The supplier only sends two kinds of shipments: Type A: 25% of the bulbs grow red tulips, 75% yellow. Type B: 50% of the bulbs grow red tulips, 50% yellow. But the label on this shipment has been scratched off. So, to determine which type it is, you select 48 random bulbs and plant them to see what colour they grow. 36 grow red, 12 yellow. If it were me, I’d think this clearly shows the shipment is Type B, with 50% red bulbs. Even though your sample grew more than 50% red tulips, this would be even less likely if the shipment were Type A, with only 25% red bulbs. So the results of your experiment fit the Type B hypothesis much better than the Type A hypothesis. What happens if we apply significance testing, though? We’ll end up rejecting both hypotheses! Because the results don’t fit either hypothesis particularly well, even though they fit the Type B hypothesis better than Type A. Let’s test the Type B hypothesis to see how this works out. You sampled 48 bulbs, so \\(n = 48\\). And according to the Type B hypothesis, each bulb has a 50% chance of growing red, so \\(p = .5\\). Applying our formulas for \\(\\mu\\) and \\(\\sigma\\) from earlier, we get: \\[ \\begin{aligned} \\mu &amp;= 25,\\\\ \\sigma &amp;\\approx 3.5. \\end{aligned} \\] So, according to the Type B hypothesis, 95% of the time we’d expect the number of red tulips in a sample of 48 to be between 18 and 32 (that’s \\(25 \\pm 2 \\times 3.5\\)). Yet our actual result was 36, which is outside that range. (In fact it’s even outside the 99% range.) If you do the calculations for the Type A hypothesis, you’ll find a parallel result. The observed 36 red tulips is statistically significant, so we should reject that hypothesis too. But that means we would reject both hypotheses. Yet we know they’re the only two possibilities! Something seems to have gone wrong. "],
["a-bayesian-analysis.html", "8.11 A Bayesian Analysis", " 8.11 A Bayesian Analysis Bayesian critics say the problem is that we ignored the prior probabilities. If we apply Bayes’ theorem, which takes account of all the information—including the fact that the only two possibilities are Type A and B—we get the sensible result. We’ll conclude that the Type B hypothesis is true. For example, if we treat the Type A and Type B possibilities as equally plausible, we can assign \\(Pr(H) = 1/2\\). Bayes’ theorem then tells us that \\(Pr(H \\given E) = .999999998\\) for the Type B hypothesis, compared to \\(.000000002\\) for Type A. (The details of this calculation are pretty hairy, so I had my computer do it.) Now, we know from previous chapters on Bertrand’s paradox and the grue paradox that there are problems lurking here. By assuming \\(Pr(H) = 1/2\\) in this calculation, we’re essentially relying on the Principle of Indifference. And that way lies paradox. But, the Bayesians say, Bayes’ theorem will give similar results even if we set \\(Pr(H) = 1/3\\) for the Type B hypothesis, or \\(Pr(H) = 1/4\\). Bayes’ theorem will always say the probability of this hypothesis goes up given the results of the experiment, because the results fit the Type B hypothesis better. So, the say, we don’t have to worry too much about what the correct value of \\(Pr(H)\\) is. "],
["the-standoff.html", "8.12 The Standoff", " 8.12 The Standoff Trouble is, we need to know how much the Type B hypothesis goes up. Does it go up to a number that’s basically 1, like .999999998? Or does it go up from a number that’s basically 0 to a slightly larger number that’s still close to 0, like .000000001? What we should believe about \\(H\\) depends on the value of \\(Pr(H \\given E)\\). Is it close to 1, close to 0, or somewhere in between? And that depends on what number we start with for \\(Pr(H)\\). So there just doesn’t seem to be any getting around the problem of priors. On the other hand, significance testing seems to just be a way of pretending the problem doesn’t exist. It may work well if it’s used carefully and judiciously. But, critics say, it often isn’t used that way. And even when it is used that way, that’s just Bayes’ theorem in disguise—making a rough guess about what \\(Pr(H)\\) should be, yet pretending you’ve done no such thing. "],
["decisions-decisions.html", "9 Decisions, Decisions", " 9 Decisions, Decisions We’ve been talking a lot about what to believe, not so much what to do. But “probability is the very guide of life”, as Bishop Butler famously wrote in 1736. What career should you pursue? Which people should you befriend and which should you avoid? Which foods should you eat? Such questions depend on the likely outcomes of your choice. But depend how? Is there a simple formula for making decisions? There is. It was discovered over 350 years ago, by a French mathematician and philosopher named Blaise Pascal, in order to help his friend with a gambling problem. "],
["goods-and-odds.html", "9.1 Goods and Odds", " 9.1 Goods and Odds Imagine you’re at a casino and you’ve been given a voucher for one free game of dice. The game is simple. The house will roll a six-sided die once, and you must choose one of the following bets: Bet A: if the die lands on an even number you win $2. Bet B: if the die lands on a high number (5 or 6) you win $3. Which bet should you choose? The first has better odds, a one-half chance of winning compared to one-in-three. But the second has better rewards: $3 rather than $2. How to weigh these considerations against one another? That’s the question decision theory tries to answer. A choice can be better or worse in two ways. It can have better odds, or it can have better “goods”—better payouts in our casino example. Our goal is to find a formula that combines odds and goods to reach a decision. "],
["the-long-run.html", "9.2 The Long Run", " 9.2 The Long Run Well, consider what would happen if you were to play all night, choosing the same bet over and over again. If you chose Bet A, you’d win $2 half the time. If you played a thousand games, you’d expect to win about 500 of them. At $2 a pop, that’s $1,000 over 1,000 games, an average of $1 per game. This is called the expected monetary value of the bet. It’s not the amount you’d expect to win playing just once, notice. You’ll either win $2 or $0. But in the long run, you’d expect to win $1 on average. In decision theory we write \\(EMV(A) = \\$1\\): the expected monetary value of Bet A is $1. What about Bet B? If you chose it over and over again, you’d win $3 a third of the time. If you played 900 games, you’d expect to win about 300 of them. At $3 a pop, that’s $900 over 900 games, an average of $1 per game again. So \\(EMV(B) = \\$1\\), the same as Bet A! If EMV is our guide to the best choice, then these two bets are equally good. Either one would be rational choice. But what’s the underlying formula here? Let’s make it explicit. "],
["the-emv-formula.html", "9.3 The EMV Formula", " 9.3 The EMV Formula Each choice has multiple possible outcomes. If you take Bet A for example, you could win $2 or you could win nothing. What we did in thinking about the long run was essentially to multiply the probability of each outcome against the payout. Half the time you’ll win $2 in the long run, so we multiply \\(1/2 \\times \\$2\\). And half the time you’ll win $0, so we multiply \\(1/2 \\times \\$0\\). Then we combine those products by adding them together: \\[ \\begin{aligned} EMV(A) &amp;= 1/2 \\times \\$2 + 1/2 \\times \\$0\\\\ &amp;= \\$1. \\end{aligned} \\] Likewise for Bet B: a third of the time you’ll win $3, so we multiply \\(1/3 \\times \\$3\\). And two thirds of the time you’ll win nothing so we multiply \\(2/3 \\times \\$0\\). Then we add up: \\[ \\begin{aligned} EMV(B) &amp;= 1/3 \\times \\$3 + 2/3 \\times \\$0\\\\ &amp;= \\$1. \\end{aligned} \\] There won’t always be just two possible outcomes, of course. Sometimes a choice will have three, or four, or any number of possible outcomes \\(n\\). The general formula labels the possible outcomes \\(O_1, O_2, \\ldots, O_n\\), and calculates the EMV of a choice C thus: \\[ EMV(C) = Pr(O_1) \\times \\$O_1 + Pr(O_2) \\times \\$O_2 + \\ldots + Pr(O_n) \\times \\$O_n. \\] "],
["utility.html", "9.4 Utility", " 9.4 Utility Now, a lot of people would choose Bet A over Bet B, I suspect. I know I would. Because I don’t much care whether I win $2 or $3, I care more about the thrill of winning anything at all. (Why else would I bet at a casino? Not to make money, that’s for sure!) So money isn’t everything, there’s also the fun of winning a game. And of course there are more needs too, like food, sleep, love, friendship, and (at least for me) Wi-Fi. And then there are the less fundamental goods we value, like creature comforts, cushy vacations, gadgets and conveniences, and so on. To extend our decision formula to cover all these variegated goods, we need to place them all on a single, numerical scale. This scale is called utility. It’s a numerical measure of how valuable or desirable an outcome is to the person making the decision. In one way, utility is extremely subjective. Tastes differ, so what has high utility and what has low utility differs from person to person. A trip to New York City has high utility for many people, but not for me (too crowded!). In another way though, utility is an objective matter. It is an objective fact about me that a trip to New York City has low utility for me, indeed, it’s an objective fact you can easily know. You can take my word for it, for example. But even if you were still skeptical, you could offer me a trip to New York and see how much I’d be willing to pay for it. (Answer: you’d have to pay me. A lot.) "],
["quantifying-utility.html", "9.5 Quantifying Utility", " 9.5 Quantifying Utility Still, you’re probably wondering how we can quantify people’s personal priorities and tastes. How can we pin a number on the degree to which someone likes or dislikes a trip to New York? We need actual numbers if we’re going to replace the dollar amounts in the EMV formula with utilities! The trick to conjuring up these numbers was first discovered by the philosopher Frank Ramsey in the 1920’s. Sadly, Ramsey died unexpectedly at the age of 26, and the idea was lost until the 1940’s when it was rediscovered independently. The polymath John von Neumann (also a godfather of computer science) and economist Oskar Morgenstern published a classic book in 1944, Theory of Games and Economic Behavior, which still defines the fields of economics and decision theory to this day. So what’s the big idea? The core of it is just this: the more utility an outcome has for you, the less risk you’ll be willing to accept in trading it for a shot at something better. Imagine your top choice for a vacation spot is Barcelona, and your bottom choice is Kiev. New York is somewhere in between. Now imagine further that you’re holding a ticket to New York, and I hold one ticket to Barcelona and one to Kiev. I offer to gamble in exchange for your New York ticket: we’ll roll a six-sided die, and if it lands either 4, 5, or 6, you get my Barcelona ticket. Otherwise you get the Kiev ticket. Would you take this deal? What if you had to roll a 5 or higher to get the Barcelona ticket? What if you had to roll a 6?? Suppose you’re only willing to trade your New York ticket for this gamble if a 4 or better wins the Barcelona ticket. Then you must place a good amount of value on that New York ticket. You’re not willing to part with it unless you have a decent shot at Barcelona over Kiev. But if you’re even willing to make the trade when a roll of 6 is required to win the Barcelona ticket, then you must not like New York that much. You’re willing to part with it in exchange for a long shot at Barcelona, even though you’ll probably end up going to Kiev. In that case, a trip to New York can’t be all that much better than a trip to Kiev in your eyes. Using this basic idea, we can actually place a precise number on the utility a trip to New York has for you. Since Barcelona is your top choice, we’ll say it has utility 1. (Just like with probability, we use 1 as the top of the scale for simplicity. We could use a different number without changing anything of importance.) And since Kiev is your bottom choice, we’ll stipulate it has utility 0. On this 0-to-1 utility scale, where does New York place? If you aren’t willing to trade your New York ticket for the Barcelona/Kiev gamble unless the chance of winning is at least 1/2, then it’s half way up that scale. A trip to New York has utility 1/2 for you. But if you’d be willing to make the trade when the chances are as low as 1/3, then it’s only a third of the way up that scale. The utility of a trip to New York is 1/3 for you then. The general recipe is this. To locate outcome \\(O\\) on a scale from the best option \\(B\\) to the worst option \\(W\\), we set \\(U(B) = 1\\) and \\(U(W) = 0\\). Then we offer to trade \\(O\\) for a gamble with probability \\(p\\) of outcome \\(W\\), and probability \\(1-p\\) of outcome \\(B\\). The lowest value \\(p\\) can take, with you still willing to make the trade, gives us is the utility of \\(O\\): \\(U(O)\\). "],
["expected-utility.html", "9.6 Expected Utility", " 9.6 Expected Utility With utilities precisely quantified, we can extend our decision formula to non-monetary decisions. Recall, the EMV of a choice \\(C\\) was: \\[ EMV(C) = Pr(O_1) \\times \\$O_1 + Pr(O_2) \\times \\$O_2 + \\ldots + Pr(O_n) \\times \\$O_n. \\] So we just replace dollars with “utiles” to define expected utility, or \\(EU\\): \\[ EU(C) = Pr(O_1) \\times U(O_1) + Pr(O_2) \\times U(O_2) + \\ldots + Pr(O_n) \\times U(O_n). \\] The fundamental rule of decision theory is to choose the option with the highest expected utility. (If multiple options have maximal expected utility, then any one of them is a reasonable choice.) But why this formula? Why should we choose options that maximize expected utility, instead of some other formula? It’s a good question. Following this rule looks pretty sensible in a wide variety of circumstances, and so it’s by far the most widely endorsed rule. But sometimes it gives strange advice, and so some people think it should be revised. That’s part of what we’ll be looking at in the next few chapters. "],
["the-st-petersburg-paradox.html", "10 The St. Petersburg Paradox", " 10 The St. Petersburg Paradox In 1713 a mathematician named Nicolaus Bernoulli contemplated the following game: I’m going to flip a fair coin as many times as needed until it lands heads. When it does land heads, I’m going to stop and pay you a dollar amount according to the following scheme: If the heads happens on the first toss, you get $2. If the heads happens on the second toss, you get $4. If the heads happens on the third toss, you get $8. Etc. The general pattern here is that you get \\(\\$(2 \\times 2 \\times \\ldots) = \\$2^n\\), where \\(n\\) is the number of tosses. If it takes 3 tosses to get a heads, then you get \\(\\$2^3 = \\$8\\). If it takes 10 tosses, you get \\(\\$2^{10} = \\$1,024\\). And so on. This might seem uncharacteristically generous of me, and in truth it is. I’m actually going to charge you a fee up front to get in on this game. The question is, how much are you willing to pay? Probably not very much. After all, you’ll probably only win a few bucks. The probability of winning at most $8 is 7/8, for example, or 87.5%. And the probability is about 98% that you’ll win less than $100. What’s the EMV though? Well there’s a 1/2 chance of winning $2, a 1/4 chance of winning $4, a 1/8 chance of winning $8… so: \\[ \\begin{aligned} EMV &amp;= 1/2 \\times \\$2 + 1/4 \\times \\$4 + \\ldots + 1/2^n \\times \\$2^n + \\ldots\\\\ &amp;= \\$1 + \\$1 + \\ldots + \\$1 + \\ldots\\\\ &amp;= \\infty. \\end{aligned} \\] Now here’s the paradox. A game with infinite expected value is one you should be willing to pay any price to pay. Because infinity minus any finite number is still infinity. Whether I demand $10, $100, or $1,000,000 to play, the game still has infinite expected value. And yet, intuitively, the game isn’t worth more than a few dollars. "],
["bernoullis-solution.html", "10.1 Bernoulli’s Solution", " 10.1 Bernoulli’s Solution Nicholas Bernoulli devised this puzzle to challenge the idea that we should use expected value to determine the best decision. But the puzzle gets its name from his cousin Daniel Bernoulli, who published a famous response in the St. Petersburg Academy Proceedings in 1738. Daniel’s answer to his cousin’s puzzle was based on an insight of yet another mathematician, Gabriel Cramer. The value of a dollar, Cramer realized, depends on how many dollars you heavy already. To someone destitute, a dollar can make the difference between eating lunch and going hungry. But to a billionaire, one more dollar makes no noticeable difference. Or consider how a million dollars might change your life, compared to (say) Mark Zuckerberg’s. With a million dollars you might quit your job, or change careers entirely, or move from a small rented apartment to a large house. But for Mark Zuckerberg, well… I suspect there isn’t much he could do with that extra million that he can’t do already. So, Bernoulli concluded, while the dollar amounts may double and double in the St. Petersburg game, the utilities do not. Because winning $4 is not twice as valuable as winning $2; winning $8 is not twice as good as winning $2; and so on. As the dollar amounts double, the utility to be had does increase, but by less and less each time. Exactly how much better is $4 than $2 then? And $8 over $4? Bernoulli argued that as dollar rewards go up, utility increases “logarithmically”. In visual terms: More precisely, Bernoulli’s idea was that the utility of \\(\\$x\\) is given by the logarithm function, \\(\\ln\\). So \\(U(\\$x) = \\ln(\\$x)\\). Bernoulli then demonstrated that the expected utility of the St. Petersburg game—as opposed to its expected monetary value—ends up being finite. More exactly, it ends up being \\(\\ln(4)\\), the equivalent in utiles of about $4. Which lines up pretty well with what most people are willing pay to play! "],
["st-petersburgs-revenge.html", "10.2 St. Petersburg’s Revenge", " 10.2 St. Petersburg’s Revenge Sadly, Bernoulli’s solution only offers short-lived relief. The paradox bounces right back. We just have to change the rewards so that they double in utiles instead of dollars. Imagine the game now works as follows: If the heads happens on the first toss, you get 2 utiles. If the heads happens on the second toss, you get 4 utiles. If the heads happens on the third toss, you get 8 utiles. Etc. The exact same calculation we did with money now shows that this game has infinite expected utility. So you should be willing to sacrifice anything to play. Hmmm… can we always double the utility of the rewards though? Is there no limit on how much good the game can deliver? Not if we take Bernoulli’s solution seriously. There is no upper bound on the logarithm function, it just keeps increasing forever and ever. The increases do get smaller and smaller as the number of dollars increases, as the graph suggests. But they are still limitless. If you go far enough out to the right, you will eventually double the amount of utility you started from. There’s even a simple equation for figuring out how many more dollars you need to double the utility. (For the mathematically inclined, the dollar payouts go like this: \\(\\$e^2, \\$e^4, \\$e^6\\), etc.) On the other hand, there’s only so much money in the world. Even if the government agreed to create as much money as needed to cover the game, there would only be so much you could do with it. At some point, there just won’t be any more material goods to buy. And there’s only so much you can pay other people to do to make your life—or the world as a whole—better. "],
["god-comes-to-st-petersburg.html", "10.3 God Comes to St. Petersburg", " 10.3 God Comes to St. Petersburg Still, these limitations don’t seem like the right kind. Our world may be imperfect. And our potential may be bounded by our physical and social natures. But shouldn’t the laws of reason transcend those contingencies? Shouldn’t the rational decision still be rational even if our lives had infinite potential? Imagine God appears to you and offers to play the St. Petersburg game. The rewards now are days in paradise. And with God’s limitless powers, there is no limit to the amount of good you can enjoy there: you will never get bored of heaven’s rewards. And no more of this logarithm business! Each day in heaven adds as much good to your existence as the last. God will see to that. Now the St. Petersburg game really does have infinite expected utility. But God also demands a price to play, to be exacted in days in hell. To play the divine St. Petersburg game, you must first spend a thousand years with the damned. Each day in hell is an agony, but a finite one (we may suppose). So God’s price is finite. But it is incredibly steep. And most likely that millennium of suffering will only be rewarded with handful of days in heaven. Your chances of winning more than 64 days in heaven, for example, are less than 2%. Will you play God’s St. Petersburg game? Even if I were convinced it was the rational choice, I don’t think I could bring myself to do it. "],
["the-human-angle.html", "11 The Human Angle", " 11 The Human Angle Three hundred years after Pascal and &amp; co., another French mathematician devised a famous challenge for the idea of expected utility. In the years following World War II, the idea that human decisions could be boiled down to a single equation was taking hold in the United States. I mentioned the classic 1944 book by John von Neumann and Oskar Morgenstern back in Chapter 9. Well, ten years later another classic was born in The Foundations of Statistics, written by von Neumann’s former assistant, Leonard Savage. Both books derived the expected utility rule from first principles, establishing it as the gold standard in decision making. But some French mathematicians disliked this approach. As Ian Hacking writes, they “thought that there was something mechanical, unthinking, and terribly ‘American’ about having a blind rule to compute your free choices.” And in 1953, Maurice Allais developed a famous puzzle to demonstrate. "],
["the-allais-paradox.html", "11.1 The Allais Paradox", " 11.1 The Allais Paradox Suppose I offer you a choice: you can have a million dollars with no strings attached, or you can gamble. The gamble has an 89% chance of delivering a million dollars, a 10% chance of delivering five million, and a 1% chance of delivering nothing. Which will you choose, the guaranteed million or the gamble with a shot at five? Most people favour the sure million. A million dollars would reshape their whole life. And even though five million dollars would make an even bigger difference, the difference between one million and five million isn’t enough for them to risk walking away from this opportunity empty handed. Even though the risk would only be a 1% chance, they’d rather just take the safe million. Now imagine a different choice. This time both options are gambles, with very similar odds. The main difference is in the potential payoffs. The first gamble has an 89% chance of paying nothing and an 11% chance of winning a million. The second gamble has a 90% chance of paying nothing and a 10% chance of paying five million. Which will you choose, the slightly safer shot at a million or the slightly riskier shot at five million? Most people favour the second option, the slightly riskier shot at five million. There is no sure thing now, in fact whichever way you go you’ll probably walk away empty handed. And if you’re just willing to accept a small, 1% increase in the chance of leaving empty handed, you can have a shot at five million instead of one million. For most people, that exchange seems worth it. So what? Well, in fact most people have both these preferences: they’d rather have the safe million in the first choice, but take the risk at five million in the second. But this violates the law of expected utility. "],
["allais-eu-and-you.html", "11.2 Allais, EU, and You", " 11.2 Allais, EU, and You How can I say people are violating the expected utility rule here if I don’t know their personal utilities? Isn’t it possible that, for these people, the difference in value between getting nothing and getting a million dollars makes sense of these preferences? Nope. And that’s part of the beauty and ingenuity of Allais’ paradox. No matter what a person’s utilities are for $0, $1M, and $5M, the popular choices violate the expected utility rule. In fact it only takes a few lines of arithmetic to see why. First let’s labels the options here: 1A 1B 2A 2B $1M 89%: $1M 10%: $5M 1%: $0 89%: $0 11%: $1M 90%: $0 10%: $5M Most people choose 1A over 1B and 2B over 2A, so we’re considering the possibility that \\(EU(1A) &gt; EU(1B)\\) and \\(EU(2A) &gt; EU(2B)\\). Let’s write out the expected utility formula for each option: \\[ \\begin{aligned} EU(1A) &amp;= U(\\$1M),\\\\ EU(1B) &amp;= .89 \\cdot U(\\$1M) + .1 \\cdot U(\\$5M) + .01 \\cdot U(\\$0M),\\\\ EU(2A) &amp;= .89 \\cdot U(\\$0M) + .11 \\cdot U(\\$1M),\\\\ EU(2B) &amp;= .9 \\cdot U(\\$0M) + .1 \\cdot U(\\$5M). \\end{aligned} \\] A little arithmetic will show that \\(EU(1A) - EU(1B) = EU(2A) - EU(2B)\\): \\[ \\begin{aligned} EU(1A) - EU(1B) &amp;= -.01 \\cdot U(\\$0M) + .11 \\cdot U(\\$1M) - .1 \\cdot U(\\$5M),\\\\ EU(2A) - EU(2B) &amp;= -.01 \\cdot U(\\$0M) + .11 \\cdot U(\\$1M) - .1 \\cdot U(\\$5M).\\\\ \\end{aligned} \\] So if 1A has higher expected utility than 1B, the same has to be true for 2A over 2B! And this holds no matter what we plug in for the utility values. Because we left those untouched, as placeholders. So however you value money, there’s just no way to have \\(EU(1A) &gt; EU(1B)\\) but \\(EU(2A) &lt; EU(2B)\\). "],
["savages-response.html", "11.3 Savage’s Response", " 11.3 Savage’s Response In a way, this actually makes sense. After all, both choices involve the same tradeoff. Imagine you’re holding a million dollars, and contemplating trading it for gamble 1B. What you’re contemplating is taking on a 1% risk of nothing in exchange for a 10% shot at five million dollars. And that’s the same tradeoff as in the choice between 2A and 2B: are you willing to take on an extra 1% risk of nothing in exchange for a 10% chance at five million? The only difference between the two choices is the context in which this tradeoff is contemplated. In the first context there is a safe option, you can keep your million and walk away. But in the second context, there is no safe option. You must face an 89% chance of leaving empty handed. The only question is whether you’re willing to accept an added 1% risk in exchange for the shot at five million. Should this difference in context make a difference to whether you accept the tradeoff? Leonard Savage, the champion of expected utility, famously felt the allure of context here. As he wrote in his landmark 1954 book: When the two situations were first presented, I immediately expressed preference for Gamble [1A] as opposed to Gamble [1B] and for Gamble [2B] as opposed to Gamble [2A], and I still feel an intuitive attraction to these preferences. But I have accepted the following way of looking at things… Savage’s way of looking at things was to imagine the gambles being determined by a lottery with a hundred tickets labelled #1 to #100: #1 #2–11 #12–100 1A $1M $1M $1M 1B $0 $5M $1M 2A $1M $1M $0 2B $0 $5M $0 Now, if you’re choosing between 1A and 1B, you wouldn’t care if you knew the ticket would be one of #12 to #100. Either way you get $1M. So all that matters is which option you would prefer if you know the ticket would be one of #1 through #11. And if you would choose 1A then, you should make the same choice between 2A and 2B. Because it’s the exact same choice when the ticket drawn will be one of #1 through #11. And it doesn’t matter which you choose if it’s one of the others. Savage encoded this idea in a famous principle he dubbed “the Sure-thing Principle”: Sure-Thing Principle If you would choose A over B if you knew \\(X\\) was true, and also if you knew \\(X\\) was false, then you should choose A over B when you don’t know whether \\(X\\) is true or not. Allais’ paradox shows that people don’t always follow this principle. Although it may sound obvious, it has implications that aren’t so obvious. It enforces a kind of “context free” decision making that humans don’t always find intuitive or comfortable. For many of us, it matters whether there’s a safe option—whether we could have the million dollars regardless, whether the ticket drawn will be one of #1–#11 or not. "],
["dr-savages-prescription.html", "11.4 Dr. Savage’s Prescription", " 11.4 Dr. Savage’s Prescription Savage’s answer to Allais illustrates a core conceptual distinction, one that arises again and again in philosophy. In Savage’s view, the expected utility rule is prescriptive rule, not descriptive. It says what people should, not what they will do. Just as people don’t always take their medicine as prescribed, people don’t always choose the option with the highest expected utility. We’ve already seen many ways in which humans are error-prone when it comes to reasoning with probabilities: base-rate neglect, the gambler’s fallacy, etc. Savage viewed Allais’ paradox similarly. It’s just another way humans fail to follow core principles of good reasoning, like the Sure-thing Principle. In fact Savage made the Sure-thing Principle a foundational axiom of his theory. It’s one of the eponymous Foundations of Statistics his book examines, and one central achievement of the book was to derive the law of expected utility from a small list of such assumptions. More than sixty years later, the debate over the Sure-thing Principle still hasn’t been entirely settled. "],
["the-ellsberg-paradox.html", "11.5 The Ellsberg Paradox", " 11.5 The Ellsberg Paradox When Chelsea Manning and Edward Snowden leaked classified material to the public, they found an advocate in Daniel Ellsberg. Ellsberg had leaked the famous Pentagon Papers to newspapers in 1971, revealing the U.S. government lies to the American public about the Vietnam war. Ellsberg was charged with espionage and conspiracy. Luckily for him, the government bungled the case. At Ellsberg’s trial all sorts of shenanigans emerged. White House officials had even broken into his psychiatrist’s office to find embarrassing information. The judge dismissed the charges. And the same White House burglars later became notorious for the most famous break-in in American politics, the Watergate scandal. But before lighting the fuse that ultimately destroyed Richard Nixon’s presidency, Ellsberg was known for his work in decision theory. In 1963 he challenged Savage’s Sure-thing Principle with puzzles like this one. Imagine an urn with 90 balls. 30 are red, and 60 are either black or white, but in unknown proportion. There might be anywhere from 0 to 60 black balls. A ball will be drawn at random, and you must choose between the following: 1A: win $100 if the ball is red, 1B: win $100 if the ball is black. You also face a second choice: 2A: win $100 if the ball is either red or white, 2B: win $100 if the ball is either black or white. Most people choose 1A over 1B, since you know what you’re getting with 1A: a 1/3 chance at the $100. Whereas 1B might give worse odds, maybe even no chance at all if there are no black balls. At the same time, most people choose 2B over 2A, and for a similar reason. With 2B, you know you’re getting a 2/3 chance at the $100. While 2A might give much worse odds, maybe even as low as 1/3 if there are no white balls in the urn. Like in the Allais paradox, this popular combination of choices violates the expected utility rule. The calculation that shows this is pretty similar to the one we did with Allais though, so let’s not rehearse it here. Instead let’s think about what Ellsberg is showing us here. "],
["ellsberg-allais.html", "11.6 Ellsberg &amp; Allais", " 11.6 Ellsberg &amp; Allais Ellsberg’s paradox is strongly reminiscent of Allais’. More than the two-choice structure they share, both also exploit a human preference for the known. In the Allais paradox we prefer the sure million, and in the Ellsberg paradox we prefer to know our chances. The kind of risk at play in each paradox has a different character, notice. In Allais’ paradox all the probabilities are known, and in one case we can even know the outcome. If you choose the safe million, you know what your fate will be. But in the Ellsberg paradox, you never know the outcome. The most you can know is the chance of each outcome. And yet, our preference for the known still takes hold. We still prefer to go with what we know, even if all we can know is the chance of each outcome. Is this preference for known risks rational, or irrational? Well, it violates Savage’s Sure-thing Principle. Consider Ellsberg’s dilemma as a decision table: Red Black White 1A $100 $0 $0 1B $0 $100 $0 2A $100 $0 $100 2B $0 $100 $100 If you knew a white ball was going to be drawn, you wouldn’t care which option you chose. And if you knew a white ball wouldn’t be drawn, then options 1A and 2A would be equivalent. So consistency would seem to demand selecting 2A if you selected 1A. Many decision theorists find this reasoning compelling. But more than a few turn it on its head and say so much the worse for the Sure-thing Principle. "],
["conclusion.html", "11.7 Conclusion", " 11.7 Conclusion Psychologists have tested the Allais and Ellsberg paradoxes on real human subjects many times, with pretty consistent results. These results have provided much insight into the psychology of human choice. They’ve even shaped a Nobel prize winning theory about how human decisions differ from the expected utility rule. But in philosophy, the study of decision theory tends to focus on how we should reason, not how we actually do reason. And as a prescriptive theory, the expected utility rule still looms large. It has its rivals, and is by no means the only game in town. But 350 years after Pascal and Fermat first dreamt it up, it remains the most popular game in town, by a wide margin. "],
["the-self-torturer.html", "12 The Self-Torturer", " 12 The Self-Torturer Stub: fill me in. "]
]
