[
["the-taxicab-problem.html", "3 The Taxicab Problem", " 3 The Taxicab Problem In 1972, two psychologists who would go on to win a Nobel prize for their research into human reasoning, asked participants in one study the following question: A cab was involved in a hit and run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data: 85% of the cabs in the city are Green and 15% are Blue. A witness identified the cab as Blue. The court tested the reliability of the witness under the same circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. What is the probability that the cab involved in the accident was Blue rather than Green? It’s really tempting to say 80%, and most people answer something like that. But the right answer is much lower, almost by half: 41%. How could it be so low, when the witness gets each colour right 80% of the time? The short answer is: because there are so many more green cabs than blue. But that needs some explaining. True, the witness only mistakenly says “blue” 20% of the time when the cab is really green. But almost all the cabs are green, so his “blue” mistakes end up being fairly common. With so many green cabs rolling by, even mistaking only 20% of them for blue makes for a lot of false “blue” reports. Especially compared to the small number of blue cabs actually on the road. So a lot of the time when the witness says “blue”, the cab is really green. In fact, they’re wrong more often than they’re right when it comes to “blue” cabs. As usual, it’s way easier to see what’s going on with a diagram. So imagine there are just 100 cabs in town, 85 blue and 15 green. We’ll draw a square for each cab: The dashed blue line indicates the cabs the witness will identify as “blue”. It encompasses 80% of the blue squares and only 20% of the green squares. But with so many more green squares than blue ones, it ends up including more green squares than blue ones. We can also see where the 41% figure comes from now. There are 17 green squares in the dashed blue area; because that’s 20% of 85. And there are 12 blue squares in the dashed blue region; that’s 80% of 15. So all in all then there are 29 cabs the witness calls “blue” (17 + 12 = 29). But only 12 of those really are blue, and 12/29 is about 41%. "],
["conditional-probabilities.html", "3.1 Conditional Probabilities", " 3.1 Conditional Probabilities What makes the taxicab problem so confusing? Well for one, it’s easy to get these two things muddled: The chance the witness will say “blue” when a cab is blue: 80%. The chance a cab is blue when the witness says “blue”: 41%. Schematically, this is the difference between: blue \\(\\overset{\\small 80\\%}{\\Longrightarrow}\\) “blue” “blue” \\(\\overset{\\small 41\\%}{\\Longrightarrow}\\) blue. We’re being asked to calculate the second number in the taxicab problem, whereas it’s the first number that’s given to us. The problem says: The court […] concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. Which means they correctly identify blue cabs as “blue” 80% of the time, and likewise for green. But it’s not exactly super clear from the wording, so it’s easy to read it as saying “blue” \\(\\overset{\\small 80\\%}{\\Longrightarrow}\\) blue. But even once that’s sorted out, it’s still tricky. You might have assumed the order doesn’t matter: if it’s 80% in one direction, then it’s 80% in the other direction too, no? No. The numbers in each direction are actually independent. If you live in Toronto there’s a 100% chance you live in Canada, but not the other way around. Very few Canada-dwellers live in Toronto. Likewise, most humans are “pentadactyl”—they have five digits on each limb. But very few pentadactyl creatures are people. Just think of all the chimps, gorillas, cats, cows, and even bats! They’re all pentadactyl too. (Evolution is weird.) Once you’ve seen a few examples, a diagram helps illustrate the general point. Given two cases \\(A\\) and \\(B\\), they can overlap in all kinds of ways. We can have most of the As be Bs, but not vice versa: Or we can have most As be Bs and vice versa: In probability-speak, we’re learning crucial lessons about conditional probability: the chance of something under a certain condition. The probability the witness will say “blue” given that the cab really is blue is 80%. That’s usually written \\(Pr(W \\given B) = 80\\%\\): the probability of \\(W\\) (the witness saying “blue”) given \\(B\\) (the cab really being blue). Put another way, suppose for the moment the cab really is blue. How likely is it then the witness will say “blue”? 80%. The general format is: \\[ Pr(Q \\given P) = x \\mbox{ means the probability of $Q$, given $P$, is $x$.}\\] The \\(P\\) to the right of the \\(\\given\\) is the condition, the thing we are supposing for the moment is true. The \\(Q\\) to the left of the \\(\\given\\) is the thing we are considering the probability of, given \\(P\\) as an assumption. So in the taxicab problem we’re told \\(Pr(W \\given B)\\), the probability the witness will say “blue” given the cab really is blue. What we’re asked to figure out is \\(Pr(B \\given W)\\), the probability the cab really is blue given that the witness said “blue”. And using our diagram we found that was about 12/29, or about 41%. "],
["doctors-without-base-rates.html", "3.2 Doctors Without Base Rates", " 3.2 Doctors Without Base Rates So now you’re all trained up and immunized, right? You’ll never be one of those eighty-percent suckers again, right?? Turns out even people who should know better, people with extensive scientific and statistical training, fall prey to this fallacy. And that includes medical doctors, which you might find… concerning. Consider a relatively rare virus, like HIV. Very few North Americans have HIV and don’t know it, fewer than one in every thousand. Now imagine a highly accurate (and highly fictional) blood test. It always detects the presence of the virus, and only gives a false positive in a tenth of a percent of cases. If you take the blood test and you get a positive result, how worried should you be? The test is so accurate, it’s hard to see how there could be much room for optimism. But, of course, if it were that simple we wouldn’t be talking about it. Imagine taking a thousand random North Americans and giving them the blood test. The one person with HIV will test positive. But so will one of the 999 people who don’t have HIV. Because a tenth of a percent of 999 is basically one (0.999 if you want to be exact). So even if you test positive, there’s still a fifty-fifty shot you don’t have HIV. Out of every two positive tests, there’s one true and one false—so yours might be the false positive. One way of thinking about what’s going on here is that you have two pieces of relevant information. One the one hand, the test is highly accurate, which points to you having the virus. But on the other, very few people have the virus, which points the opposite way. So you have to work through the numbers to see how those two, conflicting pieces of information balance out. The tendency to focus on just the first piece of information, the accuracy of the test, is called “base rate neglect”. Because, well, the other piece of information, the rate of HIV in the overall population, is called the “base rate”. So doctors know better than to neglect the base right, right? Well, maybe not as much as you’d hope. In study after study, everyone from undergraduates with no statistical training, to medical students, to trained and experienced medical doctors seem prone to base rate neglect. For example, in one study of 160 gynecologists, only 21% got the right answer on a similar problem (breast cancer instead of HIV, mammograms instead of blood tests). But there is some good news. After a bit of training, 87% of those same gynecologists solved these problems correctly. Curiously, part of that training was one weird trick: translating the problem into natural frequencies. Instead of “the chance of a North American having HIV is a tenth of a percent”, try “one in every thousand North Americans has HIV”. Rather than frame things in terms of percentages or probabilities, frame them in terms of sets of individuals—a thousand North Americans, or a hundred taxicabs. That makes it much easier for humans to find the right answer, these psychologists say. You may have noticed this kind of natural frequency language in medical pamphlets, like at your doctors office. Because of this research on human reasoning, it’s increasingly common to explain the prevalence of some condition, or how to interpret a certain diagnosis, in natural frequency terms. And, you might have noticed, I used it too when I explained how to get the right answers! "],
["bayes-theorem.html", "3.3 Bayes’ Theorem", " 3.3 Bayes’ Theorem There’s also a formula you can use, and it’s such an important formula in so many fields that we should get to know it now. The formula tells you how to compute the probability of the hypothesis you’re interested in, given the evidence you’ve received. So let’s use \\(H\\) to stand for our hypothesis, and \\(E\\) for the evidence. Bayes’ Theorem \\[ Pr(H \\given E) = Pr(H)\\frac{Pr(E \\given H)}{Pr(E)} \\] We saw earlier that the direction matters with conditional probabilities. 100% of Torontonians live in Canada, but only some Canada-dwellers live in Toronto. So \\(Pr(H \\given E)\\) isn’t the same thing as \\(Pr(E \\given H)\\). One thing Bayes’ Theorem tells us, though, is that there is a relationship. You can get from \\(Pr(E \\given H)\\) to \\(Pr(H \\given E)\\), if you have some additional information. Specifically, you need to have: \\(Pr(H)\\), the probability of the hypothesis before the evidence \\(E\\) is taken into account. \\(Pr(E)\\), the unconditional probability of the evidence, the chance it would be true if you didn’t yet know yet whether \\(H\\) is true or not. And, it turns out, we have all that information in the kinds of problems we’ve been discussing in this chapter. Let’s apply Bayes’ Theorem to the taxicab example to illustrate. We want to know \\(Pr(B \\given W)\\), the probability the cab is blue (\\(B\\)) given the witness said it was (\\(W\\)). We know the reverse probability: \\(Pr(W \\given B) = 80/100\\), because the court found witness to be 80% accurate. So we just need… \\(Pr(B) = 15/100\\), because only 15% of cabs are blue, and \\(Pr(W) = 29/100\\), because we calculated the witness will identify 29 out of every 100 cabs as “blue” (17 of the green ones, 12 of the blue ones). So we can just plug all those numbers into Bayes’ theorem and get: \\[ \\begin{aligned} Pr(B \\given W) &amp;= Pr(B)\\frac{Pr(W \\given B)}{Pr(B)}\\\\ &amp;= 15/100 \\frac{80/100}{29/100}\\\\ &amp;\\approx .41 \\end{aligned} \\] This is actually the same calculation we used to solve this problem before. But now we know a general formula for doing the same calculation in any problem that has the same structure! That’s nice for a few reasons. One is that pictures and diagrams won’t always be manageable. Sometimes the numbers are just too large or too small. But also, once you have the formula you can program a computer to do the calculation for you! "],
["heuristics-biases.html", "3.4 Heuristics &amp; Biases", " 3.4 Heuristics &amp; Biases Daniel Kahneman and Amos Tversky are the two psychologists who first tried out the taxicab problem on experimental subjects in 1972. Ands they eventually won the Nobel Prize in economics for their work on human reasoning. They revolutionized the field, discovering numerous ways human reasoning violates the laws of probability theory and statistics, beyond just neglecting base rates. The research program they launched came to be known as the “heuristics and biases” program. The idea being that humans use heuristics—quick and dirty shortcuts—to make reasoning easier. But, although these shortcuts work pretty well, they don’t always. They result in certain biases, predictable patterns of error, like base rate neglect. Here are two more examples from their research. 3.4.1 The Bank Teller Fallacy This question is from a paper Kahneman and Tversky published in 1983: Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? Linda is a bank teller. Linda is a bank teller and is active in the feminist movement. Almost 85% of their subjects chose (2). But that can’t be right, and you don’t need to know anything about Linda to prove it. To be a feminist and a bank teller, you need to be a bank teller. So whenever (2) is false, so is (1). But not vice versa. Imagine throwing a dart randomly at a Venn diagram: You can’t hit the feminist bank tellers without hitting the bank tellers. So why do people choose (2)? Kahneman and Tversky suggested that people are relying on a “representativeness” heuristic. They’re gauging how much the description of Linda is representative of the class of bank tellers (not very) vs. the class of feminist bank tellers (more so). Ordinarily, representativeness is a decent guide to probability. But not always. 3.4.2 R is for Terror Where is the letter R more likely to occur in an English word, as the first letter or as the third letter Most people say first letter. But (of course) third position is actually more likely. (That’s not something you can just figure out by logic, mind you. You have to run a search on a dictionary and count.) So why do people tend to think first position is more likely? Kahneman and Tversky hypothesized another heuristic: “availability”. It’s easier to think of words that begin with R—they’re more readily available in memory. And the easier it is to imagine or recall an example of something, the more frequent it seems. "],
["social-distortions.html", "3.5 Social Distortions", " 3.5 Social Distortions I’d like to close this chapter by briefly reflecting on the social and political implications of all this. If psychologists like Kahneman and Tversky are right, and our reasoning is distorted by heuristics like availability, then how do those distortions affect the shape of our society? You could write whole books on this question, and people do. So I’ll just note one possible (and provocative) answer, represented in the following remark from psychologist Joshua D. Foster: the 2009 budget for homeland security (the folks that protect us from terrorists) will likely be about $50 billion. Don’t get us wrong, we like the fact that people are trying to prevent terrorism, but even at its absolute worst, terrorists killed about 3,000 Americans in a single year. And less than 100 Americans are killed by terrorists in most years. By contrast, the budget for the National Highway Traffic Safety Administration (the folks who protect us on the road) is about $1 billion, even though more than 40,000 people will die this year on the nation’s roads. That’s hardly the final word on this question. But I think it’s a decent place to start thinking about such matters. "]
]
