[
["index.html", "Probability Through Paradox Preface", " Probability Through Paradox Jonathan Weisberg Preface This book introduces probability by way of its many paradoxes. When I first had the idea to write a book like this, I sat down to list as many puzzles and paradoxes of probability as I could think of off the top of my head. There were over twenty-five, right off the bat. After a bit more thought and discussion with friends on facebook, the number grew to well over thirty. I had always known there were plenty of probabilistic puzzles around. But I was still surprised to see them all listed in one place. And then I was struck by how much of what I know about probability is bound up with the items on that least. So I decided the book was a good idea. You can learn a lot about probability just by getting to know its paradoxes. It’s also a good way to learn about probability because puzzles and paradoxes are a common frame of reference for experts who talk and write about probability. They even serve as a shibboleth sometimes; fluency with these paradoxes is the secret-handshake by which initiates identify their peers. These considerations are especially true of philosophical approaches to probability, but they also apply to its mathematical and applied facets. So, although our approach will be primarily philosophical, I hope that you will find plenty of interest in this book whatever your orientation. "],
["the-monty-hall-problem.html", "1 The Monty Hall Problem", " 1 The Monty Hall Problem Let’s start with a puzzle so famous it’s appeared in Hollywood movies: the Monty Hall problem. "],
["ask-marilyn.html", "1.1 Ask Marilyn", " 1.1 Ask Marilyn Marilyn vos Savant had the highest IQ ever recorded, until the Guinness Book of Records dropped that category in 1990 because of concerns about IQ tests. In her “Ask Marilyn” column for Parade magazine, vos Savant answers hard questions from readers: puzzles, brain teasers, philosophical quandaries… you name it. When she published her answer to our first puzzle, a bunch of people with fancy credentials wrote in to complain she was wrong. Here was the question: Suppose you’re on a game show, and you’re given the choice of three doors. Behind one door is a car, behind the others, goats. You pick a door, say #1, and the host, who knows what’s behind the doors, opens another door, say #3, which has a goat. He says to you, “Do you want to pick door #2?” Is it to your advantage to switch your choice of doors? —Craig F. Whitaker, Columbia, Maryland If you’re like me, your gut says it doesn’t matter. There are two doors left; one has the car behind it; so they both have the same one-in-two chance of being a winner. Flip a coin, stick or move. It doesn’t matter. But Marilyn said you should switch! And you know what? She’s right. Why? Because the host knows where the car is, and he’s not going to just show it to you. That means he effectively just told you it’s behind behind door #2, unless you got lucky with your first pick of door #1. Because unless you got lucky on your first pick, his choice of which door to open was forced. He had to open door #3 to avoid showing you the car behind door #2. Again, that’s assuming you were unlucky on your first pick of door #1. But that’s a healthy two-in-three chance. So there’s a two-in-three chance door #2 has the car. Still don’t believe me? It’s okay, that’s normal. Here’s how Marilyn convinced her readers to switch. Imagine the same game, but with 100 doors. You pick door #1, and the host opens every other door except, let’s say, door #42. What do you think now, is it just a fifty-fifty shot? No way! Unless you got very lucky on your first pick and the car is behind door #1, you’ve just learned that the lucky door is #42. Still don’t believe me? That’s okay, that’s normal too! One of the most famous mathematicians of the 20th century wasn’t convinced either. Paul Erdős published over 1,500 papers in mathematics, more than any other mathematician ever. A lot of his work was even advanced research in probability theory! But when his friend, Andrew Vázsonyi, told him about this puzzle, he wasn’t having it. He kept asking Vázsonyi for a better explanation why he should switch. Vázsonyi even showed him a computer simulation he’d made of the game, where switching doors won the car two-thirds of the time. But Erdős still wanted a pure, mathematical explanation. Fun facts about Erdős by the way: he basically lived for math. He would travel the world, visiting friends at their houses, often uninvited. He would drink coffee (and, eventually, take amphetamines) and do math with his host, until they solved whatever problem the host had been working on before Erdős showed up. Then he’d move on to the next house, and the next problem. He was such a character, and so ubiquitous, that mathematicians make a game of tracing their connection to him. Every mathematician has an “Erdős number”. It’s kind of like six degrees of Kevin Bacon. Erdős is number zero. People who coauthored with Erdős have number 1. People who coauthored with them have number 2. Their coauthors have number 3, and so on. "],
["three-prisoners.html", "1.2 Three Prisoners", " 1.2 Three Prisoners In my last year as an undergraduate I took a class that changed my life. It literally defined my career as a philosopher. To this day, my career is devoted to the concepts, tools, and problems I met there for the first time. One of those problems, which my professor assigned as homework, goes like this: Three prisoners, A, B, and C, are condemned to die in the morning. But the king decides in the night to pardon one of them. He makes his choice at random and communicates it to the guard, who is sworn to secrecy. She can only tell the prisoners that one of them will be released at dawn. Prisoner A welcomes the news, as he now has a 1/3 chance of survival. Hoping to go even further, he says to the guard, “I know you can’t tell me whether I am condemned or pardoned. But at least one other prisoner must still be condemned, so can you just name one who is?”. The guard replies (truthfully) that B is still condemned. “Ok”, says A, “then it’s either me or C who was pardoned. So my chance of survival has gone up to 1/2”. Unfortunately for A, he is mistaken. But how? This homework problem got me so turned inside out, I ended up writing a computer program to run simulations of it! Just like Vázsonyi. (Fools and wise men think alike, I guess.) Well, my professor laughed. He called my computer simulation an “empirical” approach, and he didn’t mean it as a compliment. Like Erdős, he wanted me to use reason, not a brute-force, computerized experiment. Well, here’s one reasoned argument that prisoner A is mistaken. By A’s logic, their chances would also go down to 1/2 if the guard identified C instead of B. Because then there would be only one more prisoner to be released, with two candidates remaining: A and B. But if A’s chances would be the same no matter what the guard said, then the guard’s response couldn’t possibly be informative. If your chances of having a certain disease are the same whether a certain medical test comes up positive or not, then the test must be useless, right? This argument is right as far as it goes. But it doesn’t really explain where the prisoner went wrong. It just tells us she must’ve gone wrong somewhere. But where? I think the correct explanation involves a famous precept known as The Total Evidence Requirement. It says pretty much what it sounds like: to get the true probability of something, you have to take account of all the evidence you have. And the guard isn’t just telling A that B is still condemned. She’s also telling B, implicitly, that she chose to identify B as one of the condemned prisoners. It’s really counterintuitive that this extra information makes a difference. But it turns out to make all the difference in the world. Here’s one way to see how. A knows B can’t be the pardoned prisoner, because the guard said so. So there are two possibilities remaining: A is the pardoned prisoner. C is the pardoned prisoner. In the first scenario, where A has been pardoned, the guard had a choice between identifying B or C in her response. She chose to identify B, but she could have identified C instead. But in the second scenario, the guard had no choice. She had to name B, since she couldn’t tell A that she’s still condemned. So when the guard says “B is still condemned”, her report is exactly what you’d expect in the second scenario, where C was pardoned. Whereas in the first scenario, where A was pardoned, the guard’s report was half as likely. She could just as easily have named C instead. So the guard’s report fits twice as well with the second scenario. And that’s why A’s chance of survival is still 1/3. This is essentially the same solution we used for the Monty Hall problem. And we can use a diagram to visualize it. The king chooses one of the three prisoners at random, so each has the same 1/3 chance: Then we consider the guard’s options. If A is pardoned, she can choose to name either B or C. But if B is pardoned, she has to name C—otherwise she’ll give away A’s fate. And likewise, she has to name C if B is pardoned. So there are two possible outcomes where the guard names B: And look! One of them is half as likely as the other, namely the one where A was pardoned. So when the guard names B, A’s chance of having been pardoned is half that of still being condemned. In other words, her chance of survival is 1/3, versus a 2/3 chance of still being condemned. So A’s chances really do stay fixed. The guard’s report is no reprieve. It makes no difference to A’s chances, as seems right. "],
["monty-hall-again.html", "1.3 Monty Hall Again", " 1.3 Monty Hall Again Erdős left Vázsonyi’s house unsatisfied and unconvinced. But a few days later he called: someone had finally explained to his satisfaction why you should switch doors in the Monty Hall problem. “He proceeded to tell me the reasoning”, said Vázsonyi, “but I couldn’t fathom his explanation.” I guess we’ll never know what persuaded Erdős. But luckily we don’t need to, because the same technique we used on the three-prisoner puzzle works on Monty Hall. We can even use the same tree diagram, we just have to change the labels. The first branching-point is now the placement of the car, and each door has the same 1/3 chance. The second branching-point is Monty’s decision about which door to open. And just like the guard, his hand is forced in all but one case: Your initial choice was door #1, so if the car is behind door #2 Monty has to open door #3. Otherwise he’ll be showing you that you have the wrong door. Likewise, he has to open #2 if the car is behind #3. Only if the car is behind #1 does Monty have a choice. So there are two possible situations where Monty opens door #3. But one of them is half as likely as the other, namely the one where the car is behind door #1. So, when you see Monty open door #3, the car is probably behind door #2. Switch! The Total Evidence Requirement is clearly something to take seriously. To get the right answer, you have to take account of all the information you have. Monty isn’t just showing you that door #3 is a bust, he’s also likely giving something away with his choice of door to open. So it’s as important to consider how you get your information as it is to consider what information you got. These days there’s a lot of talk about the dangers filter bubbles. But the news has always been a selective window onto the world, because only some things sell. “Man Bites Dog” is news, as journalists say, while “Dog Bites Man” is not. Or, to take a more political and realistic example from the recent news, “Seven Die in Terrorist Attack on London Bridge” is news, “Thousands Die Quietly of Cancer” is not. "],
["the-gamblers-fallacy.html", "2 The Gambler’s Fallacy", " 2 The Gambler’s Fallacy My wife’s family keeps having girls. She’s one of three sisters (no brothers), and each sister has two daughters (no sons). That’s nine girls in a row! They’ve gone two generations with no boys yet. So family gatherings often turn to the obvious question: are we due for a boy next? Here are three different answers, each with a perfectly sensible looking rationale to back it up. Answer #1. Yes, the next baby is more likely to be a boy than a girl. Ten girls in a row is really unlikely. (Less than a tenth of a percent chance, if you want an exact number.) So the tenth baby will most likely be a boy. Answer #2. No, it’s even odds on boy vs. girl. A baby’s sex is determined by an isolated, random event. So it’s like a coin flip, a fifty-fifty shot every time. Answer #3. No, a girl is actually more likely! Girls run in the family, clearly. So although it could be a boy, in this family girls have the edge statistically speaking. Discussions like this tend to ignore the fact that a lot of people are born intersex. And ignoring intersex births has been the source of enough suffering in the world already, so let’s try to avoid that mistake. We’ll put our question this way: how does a string of nine girls in a row affect the chance of another girl? Do the odds go up? Down? Or do the stay the same? "],
["independence.html", "2.1 Independence", " 2.1 Independence It all hangs on whether the sex of each baby is independent of the others. Two events are independent when the outcome of one doesn’t change the probabilities of the other. A stock example of independence is sampling with replacement. Imagine an urn with 50 black marbles and 50 white ones. You draw a ball at random, put it back and give the urn a shake, and then draw again. Even if you draw white balls ten times in a row here, the odds of black-vs-white on the eleventh draw are still fifty-fifty. The draws are independent because you always put the marble from the previous draw back. That way every time you reach in the urn there just as many black balls as white ones. Now imagine the same exercise without replacement. Every time you draw a ball, you set it aside instead of putting it back. Now the draws are dependent. If you draw ten white balls in a row, the next draw is less likely to come up white. Now there are fifty black balls to just forty white ones. "],
["bias.html", "2.2 Bias", " 2.2 Bias Flips of an ordinary coin (like a Canadian Loony) also illustrate independence. Even if you get ten heads in a row, the eleventh toss is still fifty-fifty. That’s because ordinary coins—not the kind you get at a magic shop—are symmetric and evenly balanced. So each toss is… well, a tossup. If it’s really an ordinary coin, the initial ten heads in a row is just a coincidence. Coin flips aren’t just independent though, they’re also unbiased: heads and tails are equally likely. A process is biased if some outcomes are more likely than others. Like a loaded coin that comes up heads three quarters of the time is biased. "],
["fairness.html", "2.3 Fairness", " 2.3 Fairness So coin flips are both unbiased and independent, which makes them fair: fair = no bias + no dependence. Fair processes are important because nobody can get an edge. In a casino, the dice are fair, the roulette wheels are fair, the decks of cards are fair… And that means anybody who walks up to the roulette wheel or the craps table or the blackjack table can start gambling without any disadvantage. (How do casinos make money then? Well, in roulette for example, the payouts don’t match the odds of winning. The wheel is fair but the prices aren’t!) "],
["the-gamblers-fallacy-1.html", "2.4 The Gambler’s Fallacy", " 2.4 The Gambler’s Fallacy People sometimes forget that fair processes are independent, a mistake so tempting and common it has its own name: the gambler’s fallacy. If a roulette wheel comes up black five times in a row, some gamblers figure it’s “due” for red. If they get a bunch of bad hands in a row at poker, they figure they’re due for a good one. With a fair process, it’s unlikely for the same thing to keep happening over and over for a long time. If you flip a coin ten times in a row, you expect to get a mix of heads and tails. So when the same thing does happen many times in a row, people figure it has to change soon. But this way of thinking neglects independence! A fair process is also an independent process, by definition. It was really unlikely that you’d get a streak of ten heads in a row. But once the streak has happened, the eleventh toss is a fresh start—another fifty-fifty tossup. "],
["fallacies-vs-misfortunes.html", "2.5 Fallacies vs. Misfortunes", " 2.5 Fallacies vs. Misfortunes But wait: imagine you flip a coin a thousand times and it lands heads every time. Every. Damn. Time. Shouldn’t you at least be suspicious? It sure looks like something weird is going on, something that makes this coin land heads repeatedly. So shouldn’t you expect heads on the next flip? How could that be a fallacy? It’s not a fallacy! It’s perfectly good reasoning. It’s only a fallacy when you know the process is fair (or assume it is). And here, you doubt the process is fair, with good reason. The lesson: there’s a difference between a fallacy and misfortune. A fallacy is a logical error, a failure to correctly use the information you have. When you know a process is fair but neglect the independence that entails, that’s an error of logic. You should know better. But sometimes you just get bad information. If a fair coin really did land heads a thousand times in a row, you’d be forgiven for thinking it’s not actually fair. You’d be in the unfortunate position of having some misleading information—really misleading information in this example. (But don’t worry too much. Information this misleading is also really unlikely.) "],
["the-hot-hand.html", "2.6 The Hot Hand", " 2.6 The Hot Hand When a basketball player hits several shots in a row, they’re said to be on fire, which many people take seriously. They think the rest of the team should feed the ball to the player with the hot-hand because they’re more likely to make a shot. But a famous study published in 1985 found that a player’s shots are actually independent. Most people don’t know about that study, though. And certainly nobody knew what the result of the study would be before it was conducted! So a lot of believers in the hot hand were in the unfortunate position of just not knowing a player’s shots are independent. So the hot hand isn’t the same as the gambler’s fallacy. That doesn’t mean believers in the hot hand are off the logical hook, though. The same study also analyzed the reasoning that leads people to think a player’s shots are dependent. Their conclusion: people tend to see patterns in sequences of misses and hits even when they’re random. So there may be another fallacy at work. There might even be more than one. If players and fans want to think that “the zone” is a real place one can get into, then maybe they’re guilty of wishful thinking, too. Oh, one other thing. Some recent studies from Stanford and Harvard found that the hot-hand may actually be real after all! How can that be, what did the earlier studies miss? It’s still being looked into, but one possibility is: defense. When a basketball player gets in the zone, the other team ups their game. The hot player has to take harder shots. So the Harvard study added a correction to account for increased difficulty, and the Stanford study looked at baseball instead. Then they found evidence of streaks. “Applied statistics is hard,” as influential statistician Andrew Gelman once said. "],
["girls-revisited.html", "2.7 Girls Revisited", " 2.7 Girls Revisited So what about my wife’s family and their nine girls in a row? As best I can tell, the available evidence says girls running in the family isn’t a thing (or boys either). So the odds of a girl next are unchanged. Like the hot-hand though, most people don’t know about the research on this question. And that includes my in-laws. So even if they figure we’re due for a boy, they’re not guilty of the gambler’s fallacy. Are they guilty of a different fallacy though? Like the basketball fans who see non-existent patterns in random sequences of missed and hit shots? I’m going leave this one up to you, the reader. My in-laws might end up reading this book some day! "],
["the-taxicab-problem.html", "3 The Taxicab Problem", " 3 The Taxicab Problem In 1972, two psychologists who would go on to win a Nobel prize for their research into human reasoning, asked participants in one study the following question: A cab was involved in a hit and run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data: 85% of the cabs in the city are Green and 15% are Blue. A witness identified the cab as Blue. The court tested the reliability of the witness under the same circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. What is the probability that the cab involved in the accident was Blue rather than Green? It’s really tempting to say 80%, and most people answer something like that. But the right answer is much lower, almost by half: 41%. How could it be so low, when the witness gets each colour right 80% of the time? The short answer is: because there are so many more green cabs than blue. But that needs some explaining. True, the witness only mistakenly says “blue” 20% of the time when the cab is really green. But almost all the cabs are green, so his “blue” mistakes end up being fairly common. With so many green cabs rolling by, even mistaking only 20% of them for blue makes for a lot of false “blue” reports. Especially compared to the small number of blue cabs actually on the road. So a lot of the time when the witness says “blue”, the cab is really green. In fact, they’re wrong more often than they’re right when it comes to “blue” cabs. As usual, it’s way easier to see what’s going on with a diagram. So imagine there are just 100 cabs in town, 85 blue and 15 green. We’ll draw a square for each cab: The dashed blue line indicates the cabs the witness will identify as “blue”. It encompasses 80% of the blue squares and only 20% of the green squares. But with so many more green squares than blue ones, it ends up including more green squares than blue ones. We can also see where the 41% figure comes from now. There are 17 green squares in the dashed blue area; because that’s 20% of 85. And there are 12 blue squares in the dashed blue region; that’s 80% of 15. So all in all then there are 29 cabs the witness calls “blue” (17 + 12 = 29). But only 12 of those really are blue, and 12/29 is about 41%. "],
["conditional-probabilities.html", "3.1 Conditional Probabilities", " 3.1 Conditional Probabilities What makes the taxicab problem so confusing? Well for one, it’s easy to get these two things muddled: The chance the witness will say “blue” when a cab is blue: 80%. The chance a cab is blue when the witness says “blue”: 41%. Schematically, this is the difference between: blue \\(\\overset{\\small 80\\%}{\\Longrightarrow}\\) “blue” “blue” \\(\\overset{\\small 41\\%}{\\Longrightarrow}\\) blue. We’re being asked to calculate the second number in the taxicab problem, whereas it’s the first number that’s given to us. The problem says: The court […] concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. Which means they correctly identify blue cabs as “blue” 80% of the time, and likewise for green. But it’s not exactly super clear from the wording, so it’s easy to read it as saying “blue” \\(\\overset{\\small 80\\%}{\\Longrightarrow}\\) blue. But even once that’s sorted out, it’s still tricky. You might have assumed the order doesn’t matter: if it’s 80% in one direction, then it’s 80% in the other direction too, no? No. The numbers in each direction are actually independent. If you live in Toronto there’s a 100% chance you live in Canada, but not the other way around. Very few Canada-dwellers live in Toronto. Likewise, most humans are “pentadactyl”—they have five digits on each limb. But very few pentadactyl creatures are people. Just think of all the chimps, gorillas, cats, cows, and even bats! They’re all pentadactyl too. (Evolution is weird.) Once you’ve seen a few examples, a diagram helps illustrate the general point. Given two cases \\(A\\) and \\(B\\), they can overlap in all kinds of ways. We can have most of the As be Bs, but not vice versa: Or we can have most As be Bs and vice versa: In probability-speak, we’re learning crucial lessons about conditional probability: the chance of something under a certain condition. The probability the witness will say “blue” given that the cab really is blue is 80%. That’s usually written \\(Pr(W \\given B) = 80\\%\\): the probability of \\(W\\) (the witness saying “blue”) given \\(B\\) (the cab really being blue). Put another way, suppose for the moment the cab really is blue. How likely is it then the witness will say “blue”? 80%. The general format is: \\[ Pr(Q \\given P) = x \\mbox{ means the probability of $Q$, given $P$, is $x$.}\\] The \\(P\\) to the right of the \\(\\given\\) is the condition, the thing we are supposing for the moment is true. The \\(Q\\) to the left of the \\(\\given\\) is the thing we are considering the probability of, given \\(P\\) as an assumption. So in the taxicab problem we’re told \\(Pr(W \\given B)\\), the probability the witness will say “blue” given the cab really is blue. What we’re asked to figure out is \\(Pr(B \\given W)\\), the probability the cab really is blue given that the witness said “blue”. And using our diagram we found that was about 12/29, or about 41%. "],
["doctors-without-base-rates.html", "3.2 Doctors Without Base Rates", " 3.2 Doctors Without Base Rates So now you’re all trained up and immunized, right? You’ll never be one of those eighty-percent suckers again, right?? Turns out even people who should know better, people with extensive scientific and statistical training, fall prey to this fallacy. And that includes medical doctors, which you might find… concerning. Consider a relatively rare virus, like HIV. Very few North Americans have HIV and don’t know it, fewer than one in every thousand. Now imagine a highly accurate (and highly fictional) blood test. It always detects the presence of the virus, and only gives a false positive in a tenth of a percent of cases. If you take the blood test and you get a positive result, how worried should you be? The test is so accurate, it’s hard to see how there could be much room for optimism. But, of course, if it were that simple we wouldn’t be talking about it. Imagine taking a thousand random North Americans and giving them the blood test. The one person with HIV will test positive. But so will one of the 999 people who don’t have HIV. Because a tenth of a percent of 999 is basically one (0.999 if you want to be exact). So even if you test positive, there’s still a fifty-fifty shot you don’t have HIV. Out of every two positive tests, there’s one true and one false—so yours might be the false positive. One way of thinking about what’s going on here is that you have two pieces of relevant information. One the one hand, the test is highly accurate, which points to you having the virus. But on the other, very few people have the virus, which points the opposite way. So you have to work through the numbers to see how those two, conflicting pieces of information balance out. The tendency to focus on just the first piece of information, the accuracy of the test, is called “base rate neglect”. Because, well, the other piece of information, the rate of HIV in the overall population, is called the “base rate”. So doctors know better than to neglect the base right, right? Well, maybe not as much as you’d hope. In study after study, everyone from undergraduates with no statistical training, to medical students, to trained and experienced medical doctors seem prone to base rate neglect. For example, in one study of 160 gynecologists, only 21% got the right answer on a similar problem (breast cancer instead of HIV, mammograms instead of blood tests). But there is some good news. After a bit of training, 87% of those same gynecologists solved these problems correctly. Curiously, part of that training was one weird trick: translating the problem into natural frequencies. Instead of “the chance of a North American having HIV is a tenth of a percent”, try “one in every thousand North Americans has HIV”. Rather than frame things in terms of percentages or probabilities, frame them in terms of sets of individuals—a thousand North Americans, or a hundred taxicabs. That makes it much easier for humans to find the right answer, these psychologists say. You may have noticed this kind of natural frequency language in medical pamphlets, like at your doctors office. Because of this research on human reasoning, it’s increasingly common to explain the prevalence of some condition, or how to interpret a certain diagnosis, in natural frequency terms. And, you might have noticed, I used it too when I explained how to get the right answers! "],
["bayes-theorem.html", "3.3 Bayes’ Theorem", " 3.3 Bayes’ Theorem There’s also a formula you can use, and it’s such an important formula in so many fields that we should get to know it now. The formula tells you how to compute the probability of the hypothesis you’re interested in, given the evidence you’ve received. So let’s use \\(H\\) to stand for our hypothesis, and \\(E\\) for the evidence. Bayes’ Theorem \\[ Pr(H \\given E) = Pr(H)\\frac{Pr(E \\given H)}{Pr(E)} \\] We saw earlier that the direction matters with conditional probabilities. 100% of Torontonians live in Canada, but only some Canada-dwellers live in Toronto. So \\(Pr(H \\given E)\\) isn’t the same thing as \\(Pr(E \\given H)\\). One thing Bayes’ Theorem tells us, though, is that there is a relationship. You can get from \\(Pr(E \\given H)\\) to \\(Pr(H \\given E)\\), if you have some additional information. Specifically, you need to have: \\(Pr(H)\\), the probability of the hypothesis before the evidence \\(E\\) is taken into account. \\(Pr(E)\\), the unconditional probability of the evidence, the chance it would be true if you didn’t yet know yet whether \\(H\\) is true or not. And, it turns out, we have all that information in the kinds of problems we’ve been discussing in this chapter. Let’s apply Bayes’ Theorem to the taxicab example to illustrate. We want to know \\(Pr(B \\given W)\\), the probability the cab is blue (\\(B\\)) given the witness said it was (\\(W\\)). We know the reverse probability: \\(Pr(W \\given B) = 80/100\\), because the court found witness to be 80% accurate. So we just need… \\(Pr(B) = 15/100\\), because only 15% of cabs are blue, and \\(Pr(W) = 29/100\\), because we calculated the witness will identify 29 out of every 100 cabs as “blue” (17 of the green ones, 12 of the blue ones). So we can just plug all those numbers into Bayes’ theorem and get: \\[ \\begin{aligned} Pr(B \\given W) &amp;= Pr(B)\\frac{Pr(W \\given B)}{Pr(B)}\\\\ &amp;= 15/100 \\frac{80/100}{29/100}\\\\ &amp;\\approx .41 \\end{aligned} \\] This is actually the same calculation we used to solve this problem before. But now we know a general formula for doing the same calculation in any problem that has the same structure! That’s nice for a few reasons. One is that pictures and diagrams won’t always be manageable. Sometimes the numbers are just too large or too small. But also, once you have the formula you can program a computer to do the calculation for you! "],
["heuristics-biases.html", "3.4 Heuristics &amp; Biases", " 3.4 Heuristics &amp; Biases Daniel Kahneman and Amos Tversky are the two psychologists who first tried out the taxicab problem on experimental subjects in 1972. Ands they eventually won the Nobel Prize in economics for their work on human reasoning. They revolutionized the field, discovering numerous ways human reasoning violates the laws of probability theory and statistics, beyond just neglecting base rates. The research program they launched came to be known as the “heuristics and biases” program. The idea being that humans use heuristics—quick and dirty shortcuts—to make reasoning easier. But, although these shortcuts work pretty well, they don’t always. They result in certain biases, predictable patterns of error, like base rate neglect. Here are two more examples from their research. 3.4.1 The Bank Teller Fallacy This question is from a paper Kahneman and Tversky published in 1983: Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? Linda is a bank teller. Linda is a bank teller and is active in the feminist movement. Almost 85% of their subjects chose (2). But that can’t be right, and you don’t need to know anything about Linda to prove it. To be a feminist and a bank teller, you need to be a bank teller. So whenever (2) is false, so is (1). But not vice versa. Imagine throwing a dart randomly at a Venn diagram: You can’t hit the feminist bank tellers without hitting the bank tellers. So why do people choose (2)? Kahneman and Tversky suggested that people are relying on a “representativeness” heuristic. They’re gauging how much the description of Linda is representative of the class of bank tellers (not very) vs. the class of feminist bank tellers (more so). Ordinarily, representativeness is a decent guide to probability. But not always. 3.4.2 R is for Terror Where is the letter R more likely to occur in an English word, as the first letter or as the third letter Most people say first letter. But (of course) third position is actually more likely. (That’s not something you can just figure out by logic, mind you. You have to run a search on a dictionary and count.) So why do people tend to think first position is more likely? Kahneman and Tversky hypothesized another heuristic: “availability”. It’s easier to think of words that begin with R—they’re more readily available in memory. And the easier it is to imagine or recall an example of something, the more frequent it seems. "],
["social-distortions.html", "3.5 Social Distortions", " 3.5 Social Distortions Before moving let’s pause to reflect on the social and political implications of all this. If psychologists like Kahneman and Tversky are right, and our reasoning is distorted by heuristics like availability, then are those distortions reflected in the shape of our society? You could write whole books on this question, and people do. So I’ll just note one possible (and provocative) answer, represented in the following remark from psychologist Joshua D. Foster: the 2009 budget for homeland security (the folks that protect us from terrorists) will likely be about $50 billion. Don’t get us wrong, we like the fact that people are trying to prevent terrorism, but even at its absolute worst, terrorists killed about 3,000 Americans in a single year. And less than 100 Americans are killed by terrorists in most years. By contrast, the budget for the National Highway Traffic Safety Administration (the folks who protect us on the road) is about $1 billion, even though more than 40,000 people will die this year on the nation’s roads. That’s hardly the final word on this question. But I think it’s a decent place to start thinking about such matters. "],
["simpsons-paradox.html", "4 Simpson’s Paradox", " 4 Simpson’s Paradox In 1973, the University of California at Berkeley got worried about being sued for discriminating against women. In their graduate admissions that year, only 35% of female applicants had been admitted, compared to 44% of male applicants. A professor in the statistics department was recruited to look into it. He found that men and women actually had similar acceptance rates department by department. In fact, women had a higher acceptance rate across most of the larger departments. So how did women end up with a lower acceptance rate overall? Because, it turned out, women had applied more to the more selective departments. Many more women than men had applied to the English department, for example, which admitted only a small fraction of its applicants. That doesn’t mean sexism played no role here necessarily. You might wonder why women and men differed so much in their choices of where to apply, for example. And just because one group has a higher admissions rate doesn’t mean they aren’t being discriminated against. Harvard once put an admissions cap on Jews because they were being admitted in such large numbers! But it does mean the initial appearance of discrimination in Berkeley’s 1973 admissions was misleading. And it’s a great illustration of Simpson’s paradox, where a trend appears at one level of analysis yet reverses at a more fine-grained level. In fact, it’s even possible for one group to have higher admissions across the board, in every department, and still have a lower admission rate overall. Imagine a fictional case with just two departments. And let’s get all the information into a single chart by using width to display the number of applicants: The blue bars are taller than their partner red bars, and yet there’s much less blue area overall. So what’s the moral? There’s a kind of “part-whole” fallacy to watch out for with probabilities. What’s true of all the parts isn’t necessarily true of the whole. Just because a trend is present in every slice of a population doesn’t mean it’ll be present in the population overall. "],
["the-law-of-total-probability.html", "4.1 The Law of Total Probability", " 4.1 The Law of Total Probability There is a right way to do part-whole reasoning with probabilities, though! And it introduces us to a powerful companion principle to Bayes’ theorem from the last chapter. Consider this question: if 20% of all applicants to Chemistry are admitted, and 30% percent of applicants to Philosophy, what’s the overall admittance rate across these two departments? Well, it has to be somewhere between 20% and 30%. But what percentage exactly? It won’t necessarily be smack in the middle at 25%. Imagine 100 people apply to Chemistry and only 10 to philosophy. Then 23 people will be admitted out of 110. So the overall rate will be about 21%. In the reverse situation where 100 people apply to Philosophy and 10 to Chemistry, 32 people will be admitted out of 110, or about 29%. So the overall rate is closer to the departmental rate of the larger department. And if they’re equal sizes, then it will be smack in the middle at 25%. The Law of Total Probability describes this general pattern. A person’s chance of being admitted, \\(Pr(A)\\), is somewhere between their chances of being admitted to Chemistry if they apply there, \\(Pr(A|C)\\), and their chance of being admitted to Philosophy if they apply there, \\(Pr(A|P)\\): \\[ Pr(A|C) &lt; Pr(A) &lt; Pr(P). \\] (I’m assuming they applied to just one department, for simplicity.) But where exactly will \\(Pr(A)\\) fall in that interval? Well, the more likely it is they applied to Chemistry, the closer it will be to the left end. And the more likely it is they applied to Philosophy, the closer it’ll be to the right. Here’s another way to think about the same principle. Suppose you want to know whether \\(A\\) is true—whether you’ll get an A in your next philosophy class, for example. One way to get a handle on that is to break the question down into cases. Maybe the class will be boring, maybe not. We’ll use \\(B\\) for boring and \\(\\neg B\\) for not boring (the tilde symbol \\(\\neg\\) being a common way to symbolize negation). If the class is boring your chances of getting an A aren’t so good, let’s suppose. Wheres you’re likely to do well if it’s not boring. So the more likely it is the class is boring, the lower your chances of getting an A will be. That kind of reasoning is captured and made precise by: The Law of Total Probability \\[Pr(A) = Pr(A \\given B) Pr(B) + Pr(A \\given \\neg B) Pr(\\neg B).\\] In terms of our tree-diagramming technique, the LTP says to add up the final quantities from the \\(A\\)-and-\\(B\\) and \\(A\\)-and-\\(\\neg B\\) branches to get \\(Pr(A)\\). "],
["bayes-ltp-awesome.html", "4.2 Bayes + LTP = Awesome", " 4.2 Bayes + LTP = Awesome A big part of why the LTP is interesting is that it makes for a powerful combination with Bayes’ theorem. In fact, you often need the LTP to get to the point where you can apply Bayes’ theorem. Remember the problem of base rate neglect? In one example, we imagined a fictional blood test for HIV that always comes up positive when the virus is present. But real medical tests aren’t so dependable. They can generate false negatives as well as false positives. And the LTP helps us get the right answer in these more realistic cases. Here’s an example, still fictional, but more realistic: About 1 in every 100 people have Weisberg’s syndrome (an annoying but mostly harmless disease). There’s a blood test that’s 90% accurate: in 90% of cases where the disease is present, the test comes up positive, and in 90% of cases where it’s absent the test comes up negative. Suppose a randomly selected person tests positive. What are the chances they have the disease? Bayes’ theorem says that for any hypothesis \\(H\\) and piece of evidence \\(E\\): Bayes’ Theorem \\[ Pr(H \\given E) = Pr(H)\\frac{Pr(E \\given H)}{Pr(E)}. \\] We want to calculate \\(Pr(D \\given P)\\), the probability of having Weisberg’s disease given a positive blood test. So Bayes’ theorem says: \\[ Pr(D \\given P) = Pr(D) \\frac{Pr(P \\given D)}{Pr(P)}.\\] So we need the three numbers on the right hand side: \\(Pr(D) = 1/100\\), that’s the base rate given in the problem. \\(Pr(P \\given D) = 90/100\\), that’s the test-accuracy, also given. \\(Pr(P) = \\ldots\\) uh-oh, this number isn’t given in the description of the problem! You guessed it, LTP to the rescue! We can calculate the chance of a positive blood test, \\(Pr(P)\\), by breaking it into two cases. What’s the chance of a positive test if you really have the disease, and what’s the chance if you don’t. Then weigh each case according to the chance it’s true. In terms of the LTP, that means we calculate: \\[ \\begin{aligned} Pr(P) &amp;= Pr(P \\given D)Pr(D) + Pr(P \\given \\neg D)Pr(\\neg D)\\\\ &amp;= (90/100)(1/100) + (10/100)(99/100)\\\\ &amp;= 1,080/10,000\\\\ &amp;= 27/250. \\end{aligned} \\] Where did I get the 99/100 term from, you might be wondering? Well if 1 in 100 people have the disease then 99/100 people don’t. So the chance of a positive blood test is 27/250, which is about 11%. And now we have all three terms we need to apply Bayes’ theorem: \\[ \\begin{aligned} Pr(D \\given P) &amp;= 1/100 \\frac{90/100}{27/250}\\\\ &amp;= 0.08333\\ldots \\end{aligned} \\] Once again, the answer is pretty counterintuitive. Despite a test with 90% accuracy, there’s still a less than 10% chance you have the disease if you get a positive test. Base rates, don’t neglect ’em. But the new lesson is: Bayes’ theorem and the LTP together can solve some problems we wouldn’t be able to solve otherwise. In fact, you’ll often see Bayes’ theorem written with the LTP already built in to the denominator: \\[ Pr(H \\given E) = \\frac{Pr(E \\given H)Pr(H)}{ Pr(E \\given H) Pr(H) + Pr(E \\given \\neg H)Pr(\\neg H)}.\\] That’s ugly, but really useful if you’re doing these calculations on the regular. We’re more interested in the philosophical significance of Bayes’ theorem, though. And this messy version obscures a lot of philosophically interesting stuff, as we’ll see in the next few chapters. So we’ll stick to the simple version. "],
["the-raven-paradox.html", "5 The Raven Paradox", " 5 The Raven Paradox Generalization is an essential part of science. Without general principles, we couldn’t predict how things will turn out. Will this vaccine immunize you or make you sick? Does the rocket have enough fuel to make it into orbit? General principles of medicine and physics give us the answers here, if we can discover them. Generalities also help us understand the world. Why does your bicycle stay upright as long as the wheels are turning, then tip over when you stop? Why do rainbows appear after a rainstorm. General principles about momentum and electromagnetism illuminate these phenomena. But how does science verify a generalization? How do we establish that a principle is generally true? The obvious and classic answer: by verifying its instances. To establish that all electrons have negative charge, examine lots of electrons. To test whether all humans mortal, go and examine lots of humans. These reflections lead to the principle of scientific reasoning known as Nicod’s Criterion: Nicod’s Criterion A generalization is confirmed by each observed instance of it (unless you’ve already found a counterexample, of course). It’s a simple and plausible idea. So of course it leads to paradox. "],
["the-raven-paradox-1.html", "5.1 The Raven Paradox", " 5.1 The Raven Paradox In the mid-20th Century, one group of philosophers sought to discover the fundamental principles of scientific reasoning. They wanted a “logic of confirmation”, as Carl Hempel called it. Would Nicod’s Criterion be part of that logic? Maybe not: Suppose you’re an ornithologist testing the hypothesis that all ravens are black. You could go out looking for ravens, but you’re short on research funds. So instead you adopt the following, perverse method. You wander around campus looking for non-black things and verifying that they are not ravens: red shoes, brown walls, yellow pencils, green statues, etc. Eventually you bump into your department chair, who wants to know why you aren’t doing your research. “I am!”, you reply, “I’m just making clever use of Nicod’s Criterion to make the job easier.” “How in the world is that?!”, your chair asks in disbelief. Your cheery retort: “With each observation of a red shoe or a yellow pencil, I confirm the hypothesis that all non-black things are non-ravens. And that’s the same as proving the hypothesis that all ravens are black! Because the two hypothesis are logically equivalent.” Your chair is a logic nerd, it turns out, and so she recognizes you’re right about one thing here. The following two hypotheses are indeed logically equivalent: All ravens are black. All non-black things are non-ravens. Imagine collecting all the ravens in the world and all the black objects into a room, and placing them into designated circles: If all ravens are black, the only part of the raven circle with anything in it will be the part that overlaps with the black circle. The crescent region on the left will be empty: But that’s just what the second hypothesis says, too. If all the non-black things are non-ravens, then all the things outside the black circle must also be outside the raven circle. So again, the left crescent will be empty. And yet, your department chair flatly denies your request for more funding. She doesn’t even miss a beat; it’s a hard pass. “What gives?”, you protest. “You agreed these are equivalent hypotheses. Are you saying my observation only confirm one of them?” “No”, says your chair, “I agree with Carl Hempel’s famous Equivalence Condition: whatever confirms one of these hypotheses must confirm the other as well.” “So you’re questioning Nicod’s Criterion??” “No”, she replies, “not right now, anyway.” “So what’s the problem?” “Well, you are confirming that all ravens are black. But only very, very slowly. So slowly that it’s not worth doing. Each observation you make of a red shoe, or a yellow pencil, confirms the hypothesis. But by so little that it might as well be no support for the hypothesis at all.” What in the world is she talking about? "],
["the-hosiasson-lindenbaum-solution.html", "5.2 The Hosiasson-Lindenbaum Solution", " 5.2 The Hosiasson-Lindenbaum Solution It turns out your chair did a minor in philosophy, where she read about a little-known Polish logician named Janina Hosiasson-Lindenbaum, who was shot dead by the Nazis in 1942. But two years before that, she published an influential analysis of precisely the issue you and your chair are debating. She concluded that observing a red shoe does support the hypothesis that all ravens are black, but by a tiny, negligible amount. And, it turns out, philosophers nowadays generally agree that she was right. How could a red shoe have any bearing at all on ravens and their colouring? Well, any object you observed could, in principle, disprove your hypothesis. Each turn of your head could reveal (say) a white raven. When you see something else instead, like a red shoe, the hypothesis passes a sort of test. But it’s only a very weak sort of test. Most things you encounter aren’t black, and aren’t ravens. So you’ll probably turn your head to see something non-black, in which case it just has to be a non-raven for the hypothesis to avoid disproof. Since ravens aren’t a frequent sight, the hypothesis will probably pass this test. But if you go looking for ravens, then the hypothesis faces a more serious test. It says each raven you find will be black. Assuming you haven’t yet established anything about ravens’ colouring, the hypothesis could well turn out to be wrong. Some of the ravens you find could well be white. When you find they’re not, your hypothesis’ bona fides get a real, substantial boost. "],
["hats-grasshoppers.html", "5.3 Hats &amp; Grasshoppers", " 5.3 Hats &amp; Grasshoppers So is Nicod’s Criterion legit then? Your chair hinted at reservations, and for good reasons. Nicod’s Criterion seems plausible, and it applies just fine to lots of cases. But not to all cases. And that means it can’t be a fundamental scientific principle—part of the “logic of confirmation”. Here’s one example where it fails: Three philosophers go to dinner and check their hats at the door. After dinner the waiter returns their hats, and philosopher A notices she has B’s hat. Now consider this general statement: each philosopher has been given the wrong hat. Will this be confirmed or disconfirmed if B turns out to have A’s hat? The answer: it will be disconfirmed, even though this would be a positive instance of the general claim. If B finds that he has A’s hat, then that’s a case of getting the wrong hat. But it also means C will definitely get the right hat, since hers is the only hat left unaccounted for. I like that example because it’s tidy. It’s easy to keep track of just three hats in your head, and the consequences are perfectly certain. C must get her own hat if A and B have been given each other’s. But the tidiness here comes at the price of realism, as it so often does. So here’s a less tidy, and slightly more realistic, example: An entomologist friend of yours tells you about a remote island in the Atlantic Ocean, Pitcairn Island, which has no grasshoppers. In other words: all the grasshoppers in the world live outside of Pitcairn Island. A few years later, you and your friend take a trip to a neighbouring island, where you see many grasshoppers. You express some doubts about your friends prior assurances. But he insists that these are positive instances of his hypothesis: they are examples of grasshoppers living outside of Pitcairn Island. But, to settle the matter, he agrees to take the ferry with you over to Pitcairn Island. At the docks, the ferry arrives from Pitcairn and a horde of grasshoppers disembarks, swarming past you. “More confirmation of my hypothesis!”, your friend declares. “A whole swarm of grasshoppers outside of Pitcairn Island!” I hope you will agree that your friend’s scientific credentials might now be in doubt. "],
["the-grue-paradox.html", "6 The Grue Paradox", " 6 The Grue Paradox A lot of science works by extrapolating from observed patterns. Pollsters, for example, survey a random sample of voters to gauge the leaning of the electorate as a whole. If 54% of respondents prefer Candidate X, they conclude that 54% of all voters prefer X, give or take. Similarly, climate scientists extrapolate from historical trends to estimate future trends. The planet has been steadily warming for the last 100 years, which is one reason to expect it will continue to warm for the next few years, at least. A fundamental principle of scientific inquiry thus seems to be something like: expect the unobserved to resemble the observed. Philosophers call this the Principle of Induction. "],
["a-gruesome-concept.html", "6.1 A Gruesome Concept", " 6.1 A Gruesome Concept But one philosopher, Nelson Goodman, identified a difficult puzzle about the principle of induction. It revolves around a bizarre concept of his invention, which he dubbed “grue”. There are two ways for an object to be grue. Some green things are grue, but only some. It depends on when we first encounter them. If our first observation of a green object happens before the year 2020, then it’s grue. So the Statue of Liberty is grue: it’s a green object that was first observed before the year 2020 (long before). But if our first encounter with a green object is in the year 2020 or later, then it’s not grue. Likewise if we never observe it—because it’s on the far side of the universe, for example, or buried deep underground. I said there are two ways for an object to be grue: some blue objects are grue, too. Not the ones observed before 2020, though. Instead it’s the ones that aren’t observed before 2020. If a blue object is observed for the first time after 2019, or it’s never observed at all, then it’s grue. So blue sapphires that won’t be mined before the year 2020 are grue. As usual, it helps to have a diagram: We can also define ‘grue’ in explicit, verbal terms: Grue An object is grue if EITHER (a) it is green and first observed before the year 2020, OR (b) it’s blue and not observed before 2020. To test your understanding, see if you can explain why the following are examples of grue things: the $20 bill in my pocket, Kermit the Frog, the first (Canadian) $5 bill to be printed in 2020, the first sapphire to be mined in 2020, and blue planets on the far side of the universe. Then see if you can explain why these things aren’t grue: fire engines, the Star of India, and the first $20 bill to be printed in 2020. Once you’ve got all those down, try this question: do grue objects change colour in the year 2020? It’s a common confusion to say they do. But no, grue objects don’t change colour. The Statue of Liberty is green and (let’s assume) it always will be. So it’s grue, and always will be, because it’s a green thing that was first observed before the year 2020. Part (a) of the definition of ‘grue’ guarantees that. The only way time comes into it is in determining which green things are grue (and which blue things). If a green thing is first observed before 2020, then it’s grue, ever and always. Likewise, if a blue thing is not first observed before 2020, then it’s grue, and always has been! "],
["the-paradox.html", "6.2 The Paradox", " 6.2 The Paradox So what’s the big deal about grue? Well ask yourself whether you’ve ever seen a grue emerald. I have. In fact, every emerald I’ve ever seen has been grue. And the same goes for every emerald you’ve ever seen. Every emerald anyone has ever seen has been grue. Why? Because they’re all green. And they’ve all been observed before the year 2020 (it’s 2017 as I write this). So they’re all grue the first way—they all satisfy part (a) of the definition. (Notice it’s an EITHER/OR definition, so you only have to satisfy one of the two parts to be grue.) So all the emeralds we’ve ever seen have been grue. Let’s apply the Principle of Induction then: All observed emeralds have been grue. Therefore all emeralds are grue. But if all emeralds are grue, then the first emeralds to be mined in 2020 will be grue. And that means they’ll be blue! Because they won’t have been observed before 2020, so the only way for them to be grue is to be blue. So there are blue emeralds out there, just waiting to be pulled out of the earth! Uh-oh, something has definitely gone off the rails here. But what? Another way put the challenge: we have two “patterns” in our observed data. The emeralds we’ve seen are uniformly green, but they’re also uniformly grue. We can’t project both these patterns into the future, though. They contradict each other starting in 2020. Now, the green pattern is the real one, obviously. The grue “pattern” is bogus, and no one but a philosopher would even bother thinking about it. So why is it bogus? What’s so special about green? Apparently the Principle of Induction has a huge hole in it! It says to extrapolate from observed patterns. But which patterns? Patterns are cheap, as any data scientist will tell you. Given a bunch of data points on an xy-plane, there are lots of ways to connect the dots. Even if they all lie on a straight line, you could draw an oscillating curve that passes through each point, or even a much wilder (and sillier) curve. Fun fact: deciding which patterns to use and which to ignore is a big part of what machine learning experts do. And it’s one reason humans are still essential to designing artificial intelligence. Thanks to our experience, and our genetic inheritance, we have lots of information about which patterns are likely to continue, and which are bogus “patterns” like grue. But how do we pass all that wisdom on to the machines, so that they can take it from here? How do we tell them the difference between green and grue? "],
["disjunctivitis.html", "6.3 Disjunctivitis", " 6.3 Disjunctivitis Here’s one very natural reply. The problem with grue is it’s a disjunctive concept: it’s defined using EITHER/OR. It suffers from “disjunctivitis”. But the beauty of Goodman’s puzzle is the neat way it exposes the flaw here. It allows us to make ‘green’ the disjunctive concept instead! How? Start by building grue a friend, a concept to fill in the missing spaces in our original diagram. We’ll call it “bleen”: Here’s how you’d define green in terms of grue and bleen then: Green An object is green if EITHER (a) it’s grue and first observed before the year 2020, OR (b) it’s bleen and not observed before 2020. Now maybe you’re thinking: you could define green that way, but that’s not how it’s actually defined. In reality, we already understand the concept of green, and we have to learn the concept of grue from its disjunctive definition. The problem is, that’s just a fact about us humans, not about grue vs. green. That’s just the way we homo sapiens happen to be built (or maybe socialized, or both). Some bizarre species of alien could grow up thinking in grue/bleen terms instead. And when they landed on Earth, we’d have to explain our green/blue language to them using an EITHER/OR definition. Then they would be looking at us thinking: you guys have a very weird, disjunctive way of thinking! What could we say to them to establish the superiority of our way of thinking? It’s been more than 70 years since Goodman first posed this challenge. Yet no answer has emerged as the clear and decisively correct one. "],
["time-dependence.html", "6.4 Time Dependence", " 6.4 Time Dependence Another common reply to Goodman’s challenge is to say that ‘grue’ is defective because it’s time-dependent. It means different things depending on the time an object is first observed. But the same reversal of fortunes that toppled the “disjunctivitis” diagnosis happens here. We can define green in terms of grue and bleen. And when we do, it’s green that’s the time-dependent concept, not grue. So we’re left in the same spot. We need some way of showing that the “true” order of definition is the one we’re used to. By what criterion can we say that green is more fundamental, more basic, than grue? "],
["the-moral.html", "6.5 The Moral", " 6.5 The Moral Though it might seem just cute, or a mere curiosity, Goodman’s puzzle is actually profound. In fact, I see it as one of the deepest and most troubling problems we’ll encounter in this book. The reason why will become clearer soon, especially when we discuss Bertrand’s paradox in the next chapter. But to give you the gist of what we’re dealing with, here is the lesson I (and many others) take from the grue paradox. Whatever the logic of science is, it cannot be written down or summarized in a few, simple principles. The Principle of Induction is barely even a schema, needing much filling in. And the real substance can’t be filled in, at least not in any simple way with the tools we have like logic and probability. Which observed patterns should we expect to apply in general? Which ones should we tell the machines to extrapolate from, and which to ignore? We know intuitively how to answer this question in many cases. We know that green is legit, and grue is bogus. But we have no way of making this tacit knowledge explicit in general, so that we can program it all into a machine. "]
]
